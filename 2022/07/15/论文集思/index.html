

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="idea：多任务 zero-shot 多语言 prompt  ？脉冲神经网络&#x2F;强化学习&#x2F;lifelong learning？ 社会性 性别问题 stories makes us human 《How Can We Know What Language Models Know》(LPAQA) 在写指令式的prompt的时候可以通过实验找到最适合不同的下游任务的句式，多样化的指令式">
<meta property="og:type" content="article">
<meta property="og:title" content="论文集思">
<meta property="og:url" content="http://example.com/2022/07/15/%E8%AE%BA%E6%96%87%E9%9B%86%E6%80%9D/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="idea：多任务 zero-shot 多语言 prompt  ？脉冲神经网络&#x2F;强化学习&#x2F;lifelong learning？ 社会性 性别问题 stories makes us human 《How Can We Know What Language Models Know》(LPAQA) 在写指令式的prompt的时候可以通过实验找到最适合不同的下游任务的句式，多样化的指令式">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-07-15T09:30:27.000Z">
<meta property="article:modified_time" content="2022-08-17T04:03:14.412Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Paper Reading">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>论文集思 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>YFHuang&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/test.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="论文集思"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-15 17:30" pubdate>
          2022年7月15日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          84 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">论文集思</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="idea："><a href="#idea：" class="headerlink" title="idea："></a>idea：</h1><p><strong>多任务 zero-shot 多语言 prompt  ？脉冲神经网络&#x2F;强化学习&#x2F;lifelong learning？</strong></p>
<p>社会性 性别问题 stories makes us human</p>
<p>《How Can We Know What Language Models Know》(LPAQA)</p>
<p><strong>在写指令式的prompt的时候可以通过实验找到最适合不同的下游任务的句式，多样化的指令式的prompt也可以引入学习权重，而不是简单的进行集合。</strong></p>
<p>《GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models》用无梯度的搜索编辑算法（对指令进行删除 增加 交换 转义）</p>
<p><strong>第七个实验对我很有启发：</strong>how instructions are internally utilize by models remains largely unknown and merits further study. <strong>指令是如何被模型内部所利用的</strong> 非常值得去探讨，很多混乱的或者是不相关的指令往往有时候也会与人类所认为的好的指令起到一样的提升模型性能的效果。其实一直对我来说，在做给数据集增加离散的指令的事情，但这个领域一直像一个黑盒一样，我并不知道为什么添加了指令可以给模型带来性能的提高，或者说指令对模型来说是否真的像对人类一样是帮助理解任务的，在内部是如何被利用的，非常值得去揭秘</p>
<p>《RLPrompt:Optimizing Discrete Text Prompts With Reinforcement Learning》(RLPrompt)</p>
<p>这篇文章中的结论也是优化后的提示语虽然诱发了很强的任务表现，但往往是没有明确的人类可理解含义的杂乱的文本，这与之前的研究相呼应，即利用提示语的LM不一定遵循人类语言模式。.Our results show that good optimized prompts for the downstream task indeed are often not linguistically coherent, but instead tend to be gibberish；The observation suggests that frozen LMs also make use of prompts differently from humans, in line with previous discoveries in prompt-based model fine-tuning (Webson and Pavlick, 2021).冷冻的LMs对提示的利用也与人类不同 <strong>语言模型对prompt的利用与人类不同？</strong></p>
<p>实验的3.2  <strong>从较小的模型中学到的提示语可以更好地转移到较大的模型中</strong>（例如，distilGPT-2到GPT-2 xlarge），实现与使用较小模型本身相似的性能。<strong>这为未来的研究开辟了一个有希望的、令人兴奋的方向–通过跨LM的转移性，我们可以从较小的模型中廉价地学习一个提示，并将其应用于更大、更昂贵的模型进行推理。</strong></p>
<p>《Large Language Models are Zero-Shot Reasoners》 <strong>Let’s think step by step</strong></p>
<p>encourages more research into uncovering high-level and <strong>multi-task zero-shot capabilities hidden inside those models</strong>.  在对于所有类似推理的任务都可以加上“Let’s think step by step”，说明模型是可以像人类一样思考的，那其他任务会不会也有这种最好的引导模型的方式，并且得到一个重要结论，在few-shot小样本学习的过程中模型学的不是例子中的内容，而是答案的格式</p>
<p>《Training language models to follow instructions with human feedback》</p>
<p>模型仍然会产生不良的或有偏见的输出、编造事实，并在没有明确提示的情况下产生性和暴力相关的内容。<strong>并且由于训练数据的缘故，InstructGPT也因此更偏向于英语圈的文化价值观。</strong>此外，这种遵循用户指令训练还有一个副作用：模型更容易被命令去生成某些不良的输出，从而造成滥用。<strong>为了解决这个问题，就需要模型能够自己学会拒绝某些指令，如何能让模型学会拒绝一些指令？目前还暂时无解。</strong></p>
<p> 《Cross-Lingual Ability of Multilingual Masked Language Models：A Study of<br>  Language Structure》</p>
<p>关于三个成分的消融实验，constituent order（就是一些主谓宾成分的顺序）不是跨语言转换的关键，而composition（固定结构搭配 也就是语义 比如说small viliage）是所有语言之间最重要的共同点。</p>
<p>尝试去解决探索的问题：</p>
<p><strong>1.Multilingual</strong></p>
<ul>
<li>“Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models” <strong>提出了更严格的多任务框架来预测多语模型 Zero-shot cross-lingual transfer 的表现</strong>，不需要在目标 low-resource language 评估，甚至在 low-resource language 完全没有标注数据作为测试集时，即可预测模型的zero-shot跨语言迁移效果。</li>
<li><strong>关于预训练多语模型为什么表现好，仍是一个没有定论的问题</strong>。：“Cross-Lingual Ability of Multilingual Masked Language Models：A Study of Language Structure”再次指出词表 overlap（anchor）并不是 pretrained multilingual model 跨语言能力的原因，Constituent 的顺序也不是，而语义的组合才是。而之前有研究关于 word anchor 有类似或者相反的结论。</li>
<li><strong>关于如何预训练更好的多语模型和覆盖更多样的语言，除了在同一语系（如 indo-European language）中使用 subword 或者 character 可以有多共享的词汇作为“anchor”</strong>（比如“Canine：Pre-training an Efficient Tokenization-Free Encoder for Language Representation”）。</li>
<li><strong>对于差异很大的语言，比如中文和英文，我认为还应该设法让模型学到不同语言间共享的语法结构</strong>，比如类似 Universal Dependency（UD）的结构，在我们之前的工作（“Frustratingly Simple but Surprisingly Strong：Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing”）中 UD 被证明对 zero-shot cross-lingual semantic parsing 帮助巨大。</li>
<li>“<strong>Language Diversity: from Low-Resource to Endangered Languages</strong>”，在 rising star talk上Sebastian Ruder 做了 “Scaling NLP Systems to the Next 1000 Languages”的演讲</li>
</ul>
<p><strong>2.大模型无法完成的</strong></p>
<ul>
<li><p><strong>Ambiguity： ** Y</strong>ejin Choi 提到 Ambiguity 是自然语言的内在性质，自然语言理解不是严格的分类问题<strong>（“language understanding is not categorization”），我们应该接受无处不在的 ambiguity，NLP 最基本的任务 POS Tagging 中 POS 的定义在随时间而变化；给定不同的场景（context），两句话的 NLI 关系可能由蕴含变为相斥（“Partial-input baselines show that NLI models can ignore context, but they don’t.”）；</strong>情感分类由最初的只有 postive negtive 标签，到引入了 neutral 的标签；由于标注者的个体不同，人的标注不可避免会有 ambiguity 和 bias<strong>（“Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection”）；</strong>自动问答中也有 AmbigQA、SituatedQA 这样的数据集（Eunsol Choi 在 rising star talk 中再次强调了同一个问题的答案可能随时间temporal、地点 geographical 等背景的变化而变化<strong>）；</strong>nonmonotonic reasoning 中，引入新的知识后，原有的推论和逻辑会被推翻<strong>。最近 temporal modeling 本身也成为比较火的领域（如 TKGC，时序 &#x2F;event 数据的建模等）；另外</strong>模型如何理解 ambiguous 的数据，以及利用 ambiguous 的数据提升模型也有很多有趣的工作**，Swabha Swayamdipta 在 rising star talk 中着重介绍了用 training dynamics 发现 ambiguous，并生成 ambiguous 数据来帮助提升模型（OOD）泛化能力的工作（“WANLI：Worker and AI Collaboration for Natural Language Inference Dataset Creation”）。</p>
</li>
<li><p><strong>Reasoning &#x2F; Logic &#x2F; Structure</strong>：在“the next big ideas” talk 中，逻辑&#x2F;推理&#x2F;结构这些大模型本质的缺陷再次被着重强调。Heng Ji 强调了结构在 multilingual transfer（例如我们之前的“Frustratingly Simple but Surprisingly Strong: Using Language-Independent Features for Zero-shot Cross-lingual Semantic Parsing”文章）, 长文本理解，多模态泛化中都应发挥更关键的作用；Dan Roth 提到知识的解构（decompose），重组（compose）和规划（plan）的决策过程是实现推理（如temporal&#x2F;numerical reasoning）的关键， 如何利用各种各样的 Incidental supervision signal（比如 Comparable texts&#x2F;Language-world mapping）是学习这个决策（decision）过程的途径。感觉有点类似 Zhiting Hu 的 Panoramic Learning—training AI agents with ALL types of experiences 了哈哈哈；“符号主义真的还需要吗？”的争论仍在继续，一方面 Hang Li 等仍在强调逻辑的重要性（用类似 MoE 的方式组合 Neural Prediction 和 Symbolic Prediction）；一方面 Yejin Choi 在 keynote 中 Continuum 部分所说，<strong>随着大模型的成功，“语言（language），知识（knowledge），推理（reasoning）”应该在大模型时代融为一体，而我们之前过分强调了形式和逻辑的作用（“Reasoning is intuitive inference where logic plays a marginal role”），用形式语言和逻辑 cover 掉所有自然语言中的 variation 是永远不可能的</strong>。</p>
</li>
<li><p><strong>Out-of-distribution （OOD）Generalization &amp; Robustness</strong>：大模型在 out-of-distribution data 上泛化的能力仍是模型实际应用中最应该关心的问题之一；<strong>在语言中 Compositionality 关注度明显提升</strong>，在和 Luke 的聊天中他提到他们最近的 code pretrained model 规模增大的情况下 compositional generalization 也会有明显的提升，在和 Jacob Andreas 的交流中他还是强调了数据在compositionality 的作用（包括数据扩增，利用大模型生成数据等），Sash（Alexander Rush）貌似最近也对 compositionality 极感兴趣，可惜没找到机会和他聊天；此外，利用大模型逐步 prompting 是最近比较火热的提升 compositionality 方式；关于 Robustness，利用 out-of-distribution&#x2F;perturbed data 来 attack 模型来检验或者提升模型仍然持续有文章出现（比如我们的“TableFormer：Robust Transformer Modeling for Table-Text Encoding”）。</p>
</li>
<li><p><strong>Long Document Understanding &#x2F; Generation：</strong>这里包括 Corpus &#x2F; Discourse &#x2F; Story &#x2F; Screenplay &#x2F; long dialogue&#x2F; Movie &#x2F; TV series 等的理解和生成；<strong>大模型对长文本的理解和生成仍是最大的问题之一。</strong>一种解决方案是提升模型允许编码的序列长度和改进 self-attention 效率，一种是先 retrieve 出来重要的短文本再编码，另外一种就通过结构进行多层级编码或解码。在“the next big ideas”演讲中，Heng Ji 重新强调了 corpus-level IE 的重要性，Mirella Lapata 强调了故事的重要性。</p>
</li>
<li><p><strong>Knowledge</strong>：关于在大模型时代的知识图谱（KG），Heng Ji 基本提到了可能的用法：1）To pretrained LM 2）GNN 3）Structural constraints during inference 4）Structure alignment via weak supervision and self-supervised learning；<strong>大模型本身也可以当作知识库（生成知识）或者帮助 KG 的构建</strong>，比如 Yejin Choi 也有一系列 commonsense KG 构建和使用的工作；Semi-parametric 的方法也成了主流之一，retrieval-augmented 的方法已经被广泛应用于理解和生成任务，这方面依然不断有有趣的工作出现，如“Training Language Models with Memory Augmentation”；另外，“Semiparametric Methods in NLP: Decoupling Logic from Knowledge” workshop 也是我最喜欢的 workshop 之一，除了 cover 到大部分相关方向，Deepmind 提到的用 retrievel 的方式做蛋白质结构预测的工作，让许久不做 biology 的我着实眼前一亮。</p>
</li>
<li><p><strong>Problem Definition &#x2F; Dataset Creation &#x2F; Evaluation：</strong>Edaurd Hovy 在 big ideas 演讲里提到了应该从问题本身思考，找出有什么 wrong&#x2F;worst case&#x2F;never seen cases，明白”why things go wrong”，再寻找解决方案。这也是我一直以来认为在研究和工程中应该遵循的方式，<strong>好好做 error analysis，发现问题，再对症下药；</strong> <strong>另一方面，做 NLP 最重要的不应该是模型本身，人（human）应该调动主管能动性去更好地定义问题，构建数据集，进行更好的evaluation（evaluation仍然是generation中老大难的问题）</strong>。</p>
</li>
</ul>
<hr>
<p><strong>3.Large LM的目的：更好地为人类所用(help people instead of replacing people)</strong></p>
<ul>
<li><p><strong>Interactive Learning &#x2F; Human-in-the-loop &#x2F; Human-AI Collaboration：</strong>Eduard Hovy 在 big ideas 的演讲中提到了除了相对客观的在 LM 或者 web 中的知识（Commonsense knowledge about Schema mined from web&#x2F;LM）<strong>人以及社会的知识也极为重要</strong>（Commonsense knowledge about people and people in groups：roles）；<strong>并且人应该去指导模型达成想要的目标。</strong>我想这也是interactive learning，<strong>human-in-the-loop learning 作为热门研究话题要达到的一部分目的。比如有趣的工作有 Ensol Choi的“Simulating Bandit Learning from User Feedback for Extractive Question Answering”，以及 Yejin 提到的“Reframing human-ai collaboration for generating free-text explanations”</strong>。</p>
</li>
<li><p><strong>SocialNLP：</strong>我老板 Diyi Yang 给的 rising star talk 详细讲述了人和社会因素应该在 NLP 中发挥更大的作用（很高兴见证终身成就奖老板 Bonnie 主持 Rising Star 老板的 talk）。另外 Diyi 的 outstanding paper “Inducing Positive Perspectives with Text Reframing”<strong>定义了“积极转述”这个很有社会影响的问题</strong>，很开心对这个工作有过微小的贡献。</p>
</li>
<li><p><strong>Complex Tasks：</strong>随着大模型能力越来越强，<strong>可能可以做一些人类非常关心的，更复杂的，使我们成为人的任务，比如 Mirella Lapta 提到的 story understanding 和 story telling，我非常喜欢她提到的类似“stories make us human”的观点</strong>。</p>
</li>
<li><p><strong>安全性&#x2F;隐私：</strong> <strong>大模型的安全性问题仍然是重点</strong>，federated learning 在这次 ACL 中有一个workshop“Federated Learning for Natural Language Processing”。Privacy 方面也持续有文章值得关注，比如“Are Large Pre-Trained Language Models Leaking Your Personal Information?”。</p>
</li>
<li><p><strong>Personalization：</strong>Personalization 在工业界（搜索，推荐，广告）和学术界关注度都很高， 比较吃惊的是和Jason Eisner的聊天中他提到最近他也对Personalization很感兴趣并期待和工业界合作。</p>
</li>
</ul>
<h1 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h1><h3 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h3><p>《GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models》</p>
<p>《RLPrompt:Optimizing Discrete Text Prompts With Reinforcement Learning》关于对prompting paradigms的介绍可以借鉴</p>
<p>《Frustrating Simple but Surprisingly strong:Using Language-Independent<br>  Features for Zero-shot Cross-lingual Semantic Parsing》关于对多语言跨语言语义理解的方法的介绍可以借鉴</p>
<p>《Cross-Lingual Ability of Multilingual Masked Language Models：A Study of<br>  Language Structure》关于多语言预训练模型以及探究多语言预训练模型的写法可以借鉴</p>
<h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>《GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models》介绍instruction都有哪些优化方法</p>
<p>更广一些：《RLPrompt:Optimizing Discrete Text Prompts With Reinforcement Learning》介绍prompt的优化方法：从soft prompt到discrete prompt的优化方法都各有什么缺点</p>
<p>《RLPrompt:Optimizing Discrete Text Prompts With Reinforcement Learning》</p>
<p><strong>Discrete Prompt Optimization with RL&#x2F;1.The Discrete Prompt Optimization Problem</strong> <u>更详细的提出离散提示优化方法的问题。一些公式以及表述内容都很值得借鉴</u></p>
<h3 id="background"><a href="#background" class="headerlink" title="background"></a>background</h3><p>《Large Language Models are Zero-Shot Reasoners》	关于大模型和提示的介绍</p>
<p><strong>Large language models and prompting</strong> A language model (LM), is a model that looks to estimate the probability distribution over text. Recently, scaling improvements through larger model sizes (from a few million [Merity et al., 2016] to hundreds of millions [Devlin et al., 2019] to hundreds of billions [Brown et al., 2020] parameters) and larger data (e.g. webtext corpora [Gao et al., 2020]) have enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream NLP tasks. Besides the classic “pre-train and fine-tune” paradigm [Liu et al., 2021b], models scaled to 100B+ parameters exhibit properties conducive to few-shot learning [Brown et al., 2020], by way of in context learning, where one can use a text or template known as a prompt to strongly guide the generation to output answers for desired tasks, thus beginning an era of “pre-train and prompt” [Liu et al., 2021a]. In work, we call such prompts with explicit conditioning on few task examples as few-shot prompts, and other template-only prompts as zero-shot prompts.</p>
<h3 id="方法methodology"><a href="#方法methodology" class="headerlink" title="方法methodology"></a>方法methodology</h3><p>《RLPrompt:Optimizing Discrete Text Prompts With Reinforcement Learning》写法很可以借鉴 1.提出离散提示优化方法的问题2.提出自己的方法的overview 3.介绍overview的左边部分机制 4.介绍overview右边部分的机制</p>
<h3 id="Study-Design"><a href="#Study-Design" class="headerlink" title="Study Design"></a>Study Design</h3><p>《Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure》可以借鉴</p>
<p>In this section, we introduce the design of our study, including the three language structure properties, and the overall setup. We also detail the pre-training and fine-tuning settings for better re- production.</p>
<h3 id="conclusion"><a href="#conclusion" class="headerlink" title="conclusion"></a>conclusion</h3><p>这项工作提出了… (用…方法来完成…任务)</p>
<p>我们表明，实验结果表明… （…特征可以显著提高…的性能）</p>
<p>在未来，… 我们计划…</p>
<p>《Cross-Lingual Ability of Multilingual Masked Language Models：A Study of<br>  Language Structure》写的很标准的introduction 可以借鉴</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Paper-Reading/">#Paper Reading</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>论文集思</div>
      <div>http://example.com/2022/07/15/论文集思/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月15日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/15/%E3%80%8ALanguage-Models-are-Few-Shot-Learners%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="《Language Models are Few-Shot Learners》论文阅读笔记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">《Language Models are Few-Shot Learners》论文阅读笔记</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/14/%E3%80%8AHow-Can-We-Know-What-Language-Models-Know-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="《How Can We Know What Language Models Know?》论文阅读笔记">
                        <span class="hidden-mobile">《How Can We Know What Language Models Know?》论文阅读笔记</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
