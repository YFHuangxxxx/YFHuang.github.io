

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  
    <meta name="description" content="开一篇文用于记录日常读paper时好的英文表达。 Sentence The target task can be performed conditioning the LLM with task-specific prompts, a small portion of parameters, or features. 在…有着…的条件下 The optimization can be highly">
<meta property="og:type" content="article">
<meta property="og:title" content="英文论文写作表达积累">
<meta property="og:url" content="http://example.com/2022/07/13/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E8%A1%A8%E8%BE%BE%E7%A7%AF%E7%B4%AF/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="开一篇文用于记录日常读paper时好的英文表达。 Sentence The target task can be performed conditioning the LLM with task-specific prompts, a small portion of parameters, or features. 在…有着…的条件下 The optimization can be highly">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-07-13T03:20:26.000Z">
<meta property="article:modified_time" content="2022-08-13T08:54:18.593Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Paper Writing">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>英文论文写作表达积累 - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>YFHuang&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/test.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="英文论文写作表达积累"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-07-13 11:20" pubdate>
          2022年7月13日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          10k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          86 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">英文论文写作表达积累</h1>
            
            
              <div class="markdown-body">
                
                <p>开一篇文用于记录日常读paper时好的英文表达。</p>
<h2 id="Sentence"><a href="#Sentence" class="headerlink" title="Sentence"></a>Sentence</h2><ol>
<li>The target task can be performed <strong>conditioning</strong> the LLM <strong>with</strong> task-specific prompts, a small portion of parameters, or features. 在…有着…的条件下</li>
<li>The optimization can be highly efficient since it does not require backpropagation, <strong>where</strong> （修饰backpropagation） the computation complexity <strong>is proportional</strong> <strong>to</strong> the model size and therefore can be expensive or even infeasible for LLMs.  与…成正比</li>
<li>It has been demonstrated that LLMs can <strong>achieve competitive performance</strong> <strong>on a broad range of tasks</strong> with limited or even zero labeled data. </li>
<li>Most works in LMaaS also <strong>focus on few-shot or zero-shot settings.</strong></li>
<li>…lead to a surge of improvements for downstream NLP tasks 导致推进了下游NLP任务的快速发展</li>
<li><strong>Whilst</strong> learning linguistic knowledge,  在…的同时</li>
<li>We present an in-depth analysis of the ….  in a wide range of stste-of-the-art pretrained language models 我们做了一个更深层次的分析</li>
<li>BERT <strong>does remarkably well</strong> on … against …</li>
<li>certain types of factual knowledge <strong>are learned much more readily</strong> than…</li>
<li>the <strong>surprisingly strong ability</strong> of … demonstrates their potential as …</li>
<li><strong>they are optimised to</strong> either predict the next word in a sequence or some masked word anywhere in a given sequence.</li>
<li>…is <strong>crucial</strong> for current state-of-art results</li>
<li>Moreover, errors can easily <strong>propagate and accumulate</strong> throughout the pipeline</li>
<li>language models <strong>come with various attractive properties</strong></li>
<li>beyond gathering a better… 除了…</li>
<li>we discuss each step in detail next and provide considerations on the probe below</li>
<li>we <strong>cover</strong> a variety of sources</li>
<li><strong>to what extent</strong> aligned texts （对应的文本）exist 在某种程度上</li>
<li>one can expect that …, and this is indeed the case:</li>
<li>with respect to 关于…</li>
<li>…will become a viable alternative to …将会成为一个可行的替代方案</li>
<li>recent work has presented <strong>intriguing</strong> results</li>
<li>these prompts are usually manually created, and quite possibly suboptimal</li>
<li>Because of this, given an inappropriate prompt, wemight <strong>fail to retrieve facts that the LM does know,</strong> and thus any given prompt only <strong>provides a lower bound estimate of</strong> the knowledge contained in an LM.</li>
<li>achieve a number of intriguing results <strong>regarding</strong> the knowledge</li>
<li>Thus it is quite possible that a fact that the LMdoes know cannot be retrieved due to the prompts not being effective queries for the fact.</li>
<li>写法可以借鉴：<strong>In this paper we ask the question:</strong> “How can we <strong>tighten this lower bound and get a more accurate estimate of the knowledge contained in state-of-the-art LMs</strong>?” <strong>This is interesting both scientifically,</strong> as a probe of the knowledge that LMs contain, <strong>and from an engineering perspective</strong>, as it will result in higher recall when using LMs as part of a knowledge extraction system.</li>
<li>an English-language benchmark <strong>devised to</strong> test the ability 被设计用来…</li>
<li>We perform extensive analysis and ablations, <strong>gleaning insights</strong> both about how to best query the knowledge stored in LMs and about potential directions for incorpo- rating knowledge into LMs themselves 收集了关于…的想法（洞察力）</li>
<li>to facilitate future experiments  以方便今后的实验 </li>
<li>a “good” prompt should <strong>trigger the LM to predict</strong> the ground- truth objects as often as possible</li>
<li>we <strong>tackle</strong> prompt generation: 也可以这样给出定义，用这个词</li>
<li>then <strong>uses the phrase spanning from</strong> the left- most word <strong>to</strong> the rightmost word in the dependency path as a prompt. spanning from从…到…</li>
<li><strong>be prone to noise</strong> 容易产生噪声</li>
<li>we <strong>assess the extent to which our prompts can improve fact prediction performance</strong>, raising the lower bound on the knowledge we dis-cern is contained in LMs.</li>
<li>to mitigate this problem 为了缓和这种问题&#x2F;解决</li>
<li>we also <strong>depict</strong> the performance of…  描述&#x2F;描绘</li>
<li>prompts will <strong>yield</strong> different predictions 产生不同结果用这个词</li>
<li><strong>it is of interest to know whether</strong> … or whether they can …</li>
<li>recent years have featured a trend towards…</li>
<li>many <strong>exaggerate</strong> actual performance on the <strong>underlying task</strong> 夸大了实际的底层任务的性能</li>
<li>aside from pointing to a conceptual limitation in our current NLP techniques 除了指出当前NLP技术的概念局限性之外</li>
<li>to be broadly useful, we would someday ….</li>
<li><strong>one potential route</strong> towards <strong>addressing these issues</strong> is …</li>
<li>results far inferior to 结果远不如</li>
<li><strong>Another recent trend in language modeling may offer a way forward</strong>. In recent years the capacity of transformer language models <strong>has increased substantially,</strong> from 100 million parameters, to 300 million parameters , to 1.5 billion parameters, to 8 billion parameters, 11 billion parameters, and finally 17 billion parameters.  描写transformer的语言模型最近规模的增长的</li>
<li>the results in this case are particularly <strong>striking</strong> 结果十分出色，引人注目</li>
<li>achieve <strong>promising</strong> results in the zero-shot and one-shot settings 达到了十分有前途的结果</li>
<li>the few-shot setting <strong>is sometimes competitive with or even occasionally surpasses state-of-the-art</strong>     few-shot的设置有时可以与最先进的技术竞争，甚至偶尔会超过。</li>
<li><strong>A heuristic sense of</strong> the overall results can be seen in Figure，which <strong>aggregates the various tasks</strong> 对整体结果的启发式认识可以从图中看出，该图汇总了各种任务</li>
<li>we also <strong>undertake a systematic study</strong> of… 我们还对…进行了系统的研究</li>
<li>Finally, given the broad spectrum of capabilities dispalyed by… 鉴于GPT-3所展示的广泛能力</li>
<li>attempt a preliminary analysis of … 尝试对…进行初步分析。</li>
<li>dataset <strong>constituting</strong> nearly a trillion words 由…组成</li>
<li>Large language models have recently been shown to <strong>attain reasonable zero-shot generalization</strong> on a diverse set of tasks</li>
<li>It has been <strong>hypothesized</strong> that …</li>
<li>to test this question <strong>at scale</strong> …</li>
<li>Recent work has shown that large language models <strong>exhibit the ability</strong> to perform reasonable zero- shot generalization to new tasks  大型语言模型展现出…样的能力</li>
<li>to evaluate zero-shot generalization to new tasks 为了评估对于新任务的零样本泛化性能</li>
<li>we err on the side of … as opposed to 我们偏向于，而不是…</li>
<li>Recent advancements in prompting large language models (LMs) such as GPT-3 (Brown et al., 2020) show that <strong>pretrained LMs can be used to perform NLP tasks via textual prompts containing a task description and a few examples, without the need for task-specific tuning (Radford et al., 2019; Brown et al., 2020)</strong></li>
<li>In this scenario, the performance of an LM <strong>is critically dependent on finding the most appropriate prompt</strong> for a given task, <strong>otherwise known as prompt-engineering</strong> (Liu et al., 2021b).  说有prompt engineering的一种方式</li>
<li>Following Webson and Pavlick (2021), we <strong>characterize</strong> instructions <strong>as</strong> a natural language description of the task that includes what is required for a person to complete the task correctly. 我们定义instruction的特征为…</li>
<li>instructions should <strong>be semantically coherent to</strong> humans 与人类的语法保持一致</li>
<li><strong>For purposes of improving model performance via instructions</strong>, <strong>Mishra et al. (2022b)（可以改写这里面的人名）</strong> <strong>provide a set of guidelines</strong> for manually rewriting instructions that were …</li>
<li>sb <strong>decompose</strong> complex tasks <strong>into</strong> self-contained sub-tasks 将复杂的问题分解为…</li>
<li>a <strong>follow-up</strong> work reveals that … 用于写related work中的内容，一个接下来的工作揭示了…</li>
<li>may <strong>prohibitively expensiv</strong>e to compute 过分的昂贵</li>
<li>our search does not <strong>utilize any task-specific properties</strong> 利用任何特定任务的属性</li>
</ol>
<h3 id="Abstract："><a href="#Abstract：" class="headerlink" title="Abstract："></a>Abstract：</h3><ol>
<li>Prompting <strong>has shown impressive success</strong> in enabling large pretrained language models (LMs) to perform diverse NLP tasks, especially when only few downstream data are available</li>
<li>many existing work <strong>resorts to</strong> tuning soft prompt <strong>which falls short of</strong> 很多工作致力于… 有….样的缺点</li>
<li>Unfortunately, for languages other than English, annotated data is limited <strong>and so is the performance of</strong> the developed parsers.</li>
</ol>
<h3 id="Introduction："><a href="#Introduction：" class="headerlink" title="Introduction："></a>Introduction：</h3><ol>
<li><p>在写多语言论文的introduction的时候可以这样借鉴：</p>
<p>In this paper, we <strong>focus on explicitly training language models in a supervised and massively multi- task fashion</strong>. <strong>Our approach uses</strong> a training mixture consisting of a large set of different tasks speci-fied in natural language prompts. <strong>Our goal is to</strong> induce a model to better generalize to held-out tasks without requiring massive scale, as well as being more robust to the wording choices of the prompts. To convert a large set of natural language tasks into prompted form, we use a simple templating language for structured datasets. <strong>We develop an interface for prompt collection</strong> from public contributors that facilitated the collection of a large multitask mixture with multiple prompts per dataset (Bach et al., 2022). <strong>We then train a variant of the T5 encoder-decoder model</strong> (Raffel et al., 2020; Lester et al., 2021) <strong>on a subset of the tasks</strong> (each with multiple datasets) and <strong>then evaluate tasks and prompts that the model was not trained on</strong>.</p>
<p><strong>Our experiments study two questions.</strong> First, does multitask prompted training improve generalization to held-out tasks? Second, does training on a wider range of prompts improve robustness to prompt wording? For the first question, <strong>we find that multitask training enables zero-shot task generalization by showing that our model matches or exceeds the performance of GPT-3 (Brown et al., 2020) on 9 out of 11 held-out datasets, despite being about 16× smaller.</strong> We also show that the model improves over a large baseline language model on 13 out of 14 tasks in the BIG-bench benchmark (BIG-bench collaboration, 2021). <strong>For the second question, we find that training on more prompts per dataset consistently improves the median and decreases the variability of performance on held-out tasks.</strong> Training on prompts from a wider range of datasets also generally improves the median but does not consistently decrease the variability.</p>
</li>
<li><p>However, <strong>the extent to which this success depends on</strong> the semantic meaningfulness of the prompts has been challenged (Webson and Pavlick, 2021; Logan et al., 2021). Thus, in this work, <strong>we remain agnostic as to why prompts support generalization.</strong> </p>
</li>
<li><p>by its nature 就其性质而言</p>
</li>
<li><p>the discrete nature of the prompts <strong>render</strong>s the optimization very difficult 其离散的性质让优化变得十分困难</p>
</li>
<li><p>RL for prompt optimization <strong>poses new challenges to</strong> learning efficiency</p>
</li>
<li><p>…，echoing the recent research 与最近的一些研究结果相符合</p>
</li>
<li><p><strong>Previous work either</strong> approximate gradients over z using their continuous LM embeddings (Shin et al., 2020) <strong>or tweak</strong> human-written prompts <strong>with heuristics</strong> </p>
<p>以前的工作要么使用连续的LM嵌入对z进行近似梯度优化，要么用启发式方法调整人工写的提示取得一些成功。</p>
</li>
<li><p>to address this shortcoming, … have  proposed …, <strong>which feed LLMs with</strong> the step-by-step reasoning examples rather than …</p>
</li>
<li><p>the performance <strong>jumps up with the</strong> size of the language models 跳跃式增长</p>
</li>
<li><p>Importantly, our … <strong>is versatile and task-agnostic</strong>，unlike most prior task-specific prompt engineering in the forms of examples (few-shot) or templates (zero-shot) [Liu et al., 2021b]:  我们的方法是通用的和任务无关的</p>
</li>
<li><p>it can <strong>facilitate</strong> step-by-step answers <strong>across</strong> various reasoning tasks, including … 它可以促进各种推理任务的逐步回答</p>
</li>
<li><p>their performance <strong>deteriorate</strong> if … 变坏</p>
</li>
</ol>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ol>
<li>可以用来描写之前没有zero-shot的设定只有few-shot的方法 Notably, <strong>few-shot learning was taken as a given for tackling such difficult tasks</strong>, and the zero-shot baseline performances were not even reported in the original work 值得注意的是，在处理这种困难的任务时，few-shot learning被认为是既定的，而在最初的工作中甚至没有报告zero-shot的baseline的表现</li>
</ol>
<h3 id="reult的结果分析："><a href="#reult的结果分析：" class="headerlink" title="reult的结果分析："></a>reult的结果分析：</h3><ol>
<li>we <strong>see significant jumps</strong> in accuracy 准确性都有明显的跳跃</li>
<li>Search Improvements <strong>Correlate with</strong> Model Sensitivity to Instructions …的改进与模型对指令的敏感度有关（与…有关）</li>
<li>these findings <strong>build upon</strong> results from … 这些发现建立在…的结果上</li>
<li>说结果的时候：We find that our search <strong>is effective in this setting across all models</strong>, improving accuracy by roughly 2 points. 可以先算一个平均的准确率大概提高了多少</li>
<li>our trained prompts <strong>performs better on average with lower variance</strong> than manual prompts,</li>
<li>some transfer, <strong>as evidenced by</strong> uniformly better performance than random prompt   一些transfer表现出其性能一致地优于随机提示 </li>
<li>prompts learned from larger models <strong>see sharp performance declines</strong> when applied to … 例如，从较大的模型中学到的提示在应用于较小的模型时，性能急剧下降。</li>
<li><strong>this opens up a promising and exciting direction for future research</strong></li>
<li><strong>enabled by</strong> the transferrability across LMs   通过跨LM的转移性</li>
<li>our method gives <strong>on-par</strong> performances for the remaining two tasks 在剩余两个任务中表现平平</li>
<li>this result <strong>aligns with</strong> the few-shot experiment 这个结果与few-shot实验的</li>
</ol>
<h2 id="Word"><a href="#Word" class="headerlink" title="Word"></a>Word</h2><p>大模型的另一种表达：pretrained high-capacity language models</p>
<p>extract relational data from <strong>text or other modalities</strong>  从文本数据或者一些其它模态的数据</p>
<p><strong>populate</strong>：populate knowledge bases 填充数据库</p>
<p>off-the-shelf 现成的 in pretrained off-the-shelf language models</p>
<p>knowledge base completion literature</p>
<p>canonical ways 权威的</p>
<p>单词缩写：<strong>language models(LM)</strong> by having the <strong>LM</strong></p>
<p>prompt ensemble prompt集成</p>
<p>substantially 大量的</p>
<p>deterministic 确定性的查询</p>
<p><strong>retrive&#x2F;elicite</strong> knowledge from the LM 从语言模型里获取知识</p>
<p>be affiliated with the religion 有着…的信仰</p>
<p><strong>concatenate</strong> them with… 把…拼接起来</p>
<p><strong>be conducive to</strong>…有助于</p>
<p>one-shot and few-shot <strong>proficiency</strong>  one-shot和few-shot的熟练程度</p>
<p><strong>unscrambling</strong> words解密单词</p>
<p>performe arithmetic 执行算术</p>
<p><strong>inflating</strong> results 一些夸大的结果</p>
<p>data <strong>contamination</strong> 数据污染</p>
<p>crowd-sourced 众包</p>
<p>subjective interpretation 主观解释</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Paper-Writing/">#Paper Writing</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>英文论文写作表达积累</div>
      <div>http://example.com/2022/07/13/英文论文写作表达积累/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>John Doe</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年7月13日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/13/%E3%80%8ALanguage-Models-as-Knowledge-Bases-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="《Language Models as Knowledge Bases?》论文阅读笔记">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">《Language Models as Knowledge Bases?》论文阅读笔记</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/07/11/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B9%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%9C%8D%E5%8A%A1-LMaaS/" title="研究方向之语言模型及服务(LMaaS)">
                        <span class="hidden-mobile">研究方向之语言模型及服务(LMaaS)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
