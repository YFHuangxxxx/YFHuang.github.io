<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>《Big Model Systems and Application》课程笔记---L1_NLP_BM_basics</title>
    <link href="/2022/07/28/%E3%80%8ABig-Model-Systems-and-Application%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-L1-NLP-BM-basics/"/>
    <url>/2022/07/28/%E3%80%8ABig-Model-Systems-and-Application%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-L1-NLP-BM-basics/</url>
    
    <content type="html"><![CDATA[<p>课程地址：<a href="https://www.bilibili.com/video/BV1UG411p7zv?spm_id_from=333.337.search-card.all.click&amp;vd_source=efcc1f9c3d8e741f72e8dad953138166">https://www.bilibili.com/video/BV1UG411p7zv?spm_id_from=333.337.search-card.all.click&amp;vd_source=efcc1f9c3d8e741f72e8dad953138166</a></p><p>课程体系、相关参考资料、拓展阅读可访问课程官网：<a href="https://www.openbmb.org/community/course">https://www.openbmb.org/community/course</a></p><h2 id="L1-NLP-BM-basics"><a href="#L1-NLP-BM-basics" class="headerlink" title="L1_NLP_BM_basics"></a>L1_NLP_BM_basics</h2><p><img src="https://img-blog.csdnimg.cn/e52aab796d6d4730a2b7a04a0bb35234.png" alt="在这里插入图片描述"></p><p>NLP基础任务：</p><p>词性标注：给每一个词的词性进行标注</p><p>命名实体识别：识别出一句话中有哪些名词指的是我们现实世界中的一些实体，比如说人名&#x2F;地名&#x2F;机构名&#x2F;日期等</p><p>共指消解：提及过的人名&#x2F;地名会用哪些代词代替，指向前面的哪个实体</p><p>依赖关系识别：成分之间的依存关系</p>]]></content>
    
    
    
    <tags>
      
      <tag>Course Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《RLPrompt:Optimizing Discrete Text Prompts With Reinforcement Learning》论文阅读笔记</title>
    <link href="/2022/07/18/%E3%80%8ARLPrompt-Optimizing-Discrete-Text-Prompts-With-Reinforcement-Learning%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/18/%E3%80%8ARLPrompt-Optimizing-Discrete-Text-Prompts-With-Reinforcement-Learning%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>Paper: <a href="https://arxiv.org/abs/2205.12548">https://arxiv.org/abs/2205.12548</a></p><p>Code: <a href="https://github.com/mingkaid/rl-prompt">https://github.com/mingkaid/rl-prompt</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《GrIPS:Gradient-free, Edit-based Instruction Search for Prompting Large Language Models》论文阅读笔记</title>
    <link href="/2022/07/17/%E3%80%8AGrIPS-Gradient-free-Edit-based-Instruction-Search-for-Prompting-Large-Language-Models%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/17/%E3%80%8AGrIPS-Gradient-free-Edit-based-Instruction-Search-for-Prompting-Large-Language-Models%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>Paper: <a href="https://arxiv.org/abs/2203.07281">https://arxiv.org/abs/2203.07281</a></p><p>Code: <a href="https://github.com/archiki/GrIPS">https://github.com/archiki/GrIPS</a></p><p><img src="https://img-blog.csdnimg.cn/9090169e95554d3d8702c89d493165c3.png" alt="在这里插入图片描述"></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在提示中提供自然语言instruction是一种有用的新范式，可以在zero-shot的设置下提高大型语言模型的任务性能。最近的工作有以下两种方法来改善这种指令式的提示：</p><ul><li>manual rewirting：手动改写很耗时，并且需要主观解释；</li><li>gradient-based tuning（基于梯度的微调）：对于大型模型来说是非常复杂的，并且需要完全访问模型权重，这对于基于API的模型来说可能是不可用的；</li></ul><p>所以在这项工作中，作者介绍了<strong>Gradient-free Instructional Prompt Search（GRIPS）</strong>，这是一种无梯度的、基于编辑的搜索方法，用于改进大型语言模型的任务指示。<strong>GRIPS接受为人类设计的指令，并自动返回一个经过验证的、经过编辑的提示，同时允许基于API的调整</strong>。在我们的搜索中，使用四种操作（删除、添加、交换、转述）对短语级别的文本进行反复编辑。</p><p>通过InstructGPT模型，GRIPS在NATURAL-INSTRUCTIONS数据集的<strong>8个分类任务上</strong>，平均任务性能提高了4.30个百分点。我们看到，仅有指令的提示和K-shot示例+指令的提示都有提高。最终得到的结论是，按照Mishra等人（2022b）的指南，<strong>GRIPS优于人工改写，在控制可用的计算和数据预算的情况下，也优于纯粹的基于例子的提示。</strong>最后，作者还对编辑过的指令进行了定性分析，包括几个规模的GPT模型。</p><h2 id="Instroduction"><a href="#Instroduction" class="headerlink" title="Instroduction"></a>Instroduction</h2><p>我认为i是对前面abstract更详细的扩写</p><p><u>第一段：第一段介绍prompr-engineering，以及最新范式中prompt的形式：instruction)。</u>最近在提示大型语言模型（LM）方面的进展，如GPT-3（Brown等人，2020），表明预训练的LM可以<strong>通过包含任务描述和一些例子的文本prompt</strong>来执行NLP任务，而不需要特定的任务tuning（Radford等人，2019；Brown等人，2020）。在这种情况下，LM的性能关键<strong>取决于为特定的任务找到最合适的提示，也就是所谓的prompt engineering</strong>（Liu等人，2021b）。这个领域的大部分工作都集中在few-shot learning上，其中模型依赖于包含输入-输出例子对的文本提示（示范性提示）。然而，当提供给人类一组相关的指令或任务描述时，人类往往能够执行一项新的任务，而不一定包括任何例子。在这个方向上，过去的工作探索了一种新的教学提示范式，即通过包括自然语言的instruction，为特定的任务定制提示（Efrat和Levy，2020；Mishra等人，2022a，b）。继Webson和Pavlick（2021）之后，我们<strong>将指示描述为对任务的自然语言描述，包括一个人正确完成任务所需的内容</strong>。</p><p><u>第二段：这一段写改善构建instruction的第一种方法—手工改写。</u>为了通过指令提高模型性能，Mishra等人（2022b）提供了一套指导原则，用于手动改写最初为数据收集目的而为众包工人编写的指令（Efrat and Levy, 2020; Mishra等人，2022a）。然而，<strong>这种改写过程需要大量的人工努力和对instruction的主观解释</strong>。此外，Mishra等人（2022b）的一个基本假设是，指令对人类来说应该是语义一致的。然而，最能提高模型性能的提示有可能在某些方面对人类来说是语义混乱的。</p><p><u>第三段：写改善构建instruction的第一种方法—基于梯度的方法。</u>过去的工作试图通过prompt tuning来自动提高大型语言模型的提示质量（Liu等人，2021b）。现有的提示调整方法<strong>使用基于梯度的方法</strong>，但这些方法有几个明显的<strong>缺点</strong>。<strong>首先，用大型语言模型计算梯度的计算量大得惊人</strong>。<strong>第二，当使用只能通过API访问的模型时，这种方法是完全不可行的，因为模型的梯度和权重不是标准的可访问的</strong>。<strong>第三，这些方法中的大多数输出连续的表示，可能不会直接映射到原始词汇中的标记</strong>（Lester等人，2021；李和梁，2021；秦和Eis-ner，2021）。含有无法解释的矢量表征的提示是有问题的，因为我们无法验证模型是否对其做出合理的反应（Khashabi等人，2021）。对于人类可读的提示，我们至少可以评估哪些词&#x2F;短语触发了某些模型行为，以及模型是否合理地回应了它们（例如，当模型从不连贯的提示中学习时，我们会感到惊讶）。</p><p><u>第四段：提出无梯度的指令式的prompt搜索方法，介绍了这个方法的pipeline。</u>在本文中，我们提出了Gradient-free Instructional Prompt Search（GRIPS），这是一个<strong>通过迭代、基于编辑和无梯度搜索</strong>来改进教学提示的自动程序（如下图所示）。</p><p><img src="https://img-blog.csdnimg.cn/c95cc23bd42d46c8be3a12e8da9bd69a.png" alt="在这里插入图片描述"></p><p>与基于梯度的提示调整不同，我们的方法允许我们改进任意（包括基于API的）语言模型的提示说明，同时保持结果说明的可读性（即避免使用连续提示）。我们将指令视为一个参数空间，并通过编辑操作在这个离散的文本空间上进行搜索（Andreas等人，2018）。如上图所示，GRIPS是一种<strong>离散的局部搜索算法，提示语以给定的指令初始化，然后迭代编辑以获得改善下游性能的指令，直到满足停止标准</strong>。在每次迭代中，根据小分值集上的性能选择修改后的指令。<strong>对文本的编辑操作包括删除、添加、交换和准短语</strong>，每项操作都在短语层面上进行，以便探索可能的指令的广阔空间。</p><p><u>第五段：介绍了这个方法所达到的最新成果。</u>在NATURAL-INSTRUCTIONS（Mishra等人，2022a）的八个分类任务中，GRIPS将GPT-2 XL和InstructGPT（GPT-3）模型的平均精度提高了2.36至9.36个百分点。此外，我们搜索得出的指令比Mishra等人(2022b)提出的手工改写得到的指令平均高出1.5个百分点，用于Instruct-GPT curie。<strong>在相同的数据和计算预算下，GRIPS在InstructGPT babbage和curie上的表现分别比搜索好1.54和1.62个百分点</strong>。最后，我们尝试用特定任务的指令（来自NATURAL- INSTRUCTIONS）初始化GRIPS，而不是任务无关指令。虽然GRIPS使用这两种指令都能提高性能，但使用特定任务的指令进行初始化时，性能总体上更高。</p><p><u>Contribution。</u>综上所述，这项工作的贡献如下：</p><ol><li><p>我们提出了GRIPS，一种在教学提示上的自动无梯度搜索，<strong>使GPT模型在NATURAL-INSTRUCTIONS上的准确度提高了2.36到9.36分</strong>。</p></li><li><p>我们证明：(a)<strong>对于InstructGPT模型</strong>，<strong>GRIPS优于人工改写和对示例提示的搜索</strong>；(b)<strong>对于包含指令和示例的提示，GRIPS提高了性能</strong>。</p></li><li><p>当使用少至20个数据点的性能信号（分数集），以及从特定任务或与任务无关的指令进行初始化时，GRIPS可以改进指令。</p></li><li><p>我们证实并加强了Webson和Pavlick（2021）的研究结果，即<strong>模型可以从语义不连贯的指令中受益</strong>，即使有较大的InstructGPT 语言模型。</p></li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>这就是之前我导想让在related work中做的事情，从刘鹏飞教授的那篇综述中提炼出来关于prompt tuning的方法。</p><h3 id="1-Exemplar-Prompts"><a href="#1-Exemplar-Prompts" class="headerlink" title="1.Exemplar Prompts"></a>1.Exemplar Prompts</h3><p>用于语言模型执行NLP任务的few-shot learning是一个活跃的研究领域。<strong>这一相关问题的prompt主要由一些input-output的例子组成</strong>。这些提示中的附加文本通常是提示-模板本身的一部分（如cloze问题&#x2F;模式），包含关于任务的有限信息。相比之下，我们的工作侧重于指令式的提示，如下所述。</p><h3 id="2-Instructional-Prompts"><a href="#2-Instructional-Prompts" class="headerlink" title="2.Instructional Prompts"></a>2.Instructional Prompts</h3><p>instructional的提示主要包含对基本任务的详细自然语言描述。<strong>最近的工作重点是在数据收集过程中包含给人类注释者的指令的提示</strong>。然而，即使是强大的LM，如GPT-3，也往往难以有效地使用众包指令来完成复杂的NLP任务（Efrat和Levy，2020）。为了弥补这一点，Mishra等人（2022a）将复杂的任务分解为独立的子任务，允许模型使用针对每个子任务的指令来单独执行每个任务。<strong>然而，后续的工作显示，即使是这些分解的指令，仍然比基于例子的提示表现得差</strong>（Mishra等人，2022b）。基于对GPT-3的error的一些分析，他们<strong>提出了手工重写指令的方法</strong>，以提高模型的性能。同样，Webson和Pavlick（2021）在对自然语言推理（NLI）的masked LM性能分析中表明，LM可能难以真正理解指令提示，但他们仅限于参数&lt;1B的小模型。Wei等人（2022年）发现，<strong>大型LM在以巨大的多任务方式对指令和少量提示进行微调后，能够更好地从指令中学习新任务</strong>。最后，Weller等人（2020）提供了一个数据集，其中任务描述被表述为对应于多个段落的问题。这些问题明显较短（12个单词），并且都与三个领域中的一个有关，而NATURAL- INSTRUCTIONS中的指令较长，并且对应于更多的任务（Mishra等人，2022a）。</p><h3 id="3-Prompt-Tuning"><a href="#3-Prompt-Tuning" class="headerlink" title="3.Prompt Tuning"></a>3.Prompt Tuning</h3><p>最近的工作表明，使用<strong>连续vector embbeding可以提高下游任务的性能，而不是将提示局限于自然语言文本</strong>。这些连续的提示更有表现力，因为不需要将tokens映射到真实的单词。然而，<strong>性能的提高是以人类是否理解与可读为代价的</strong>。此外，<strong>连续提示需要额外的学习参数</strong>，这些参数假定来自语言模型的梯度是可用的，其计算成本可能过高，或者对于只能通过API（如GPT-3）访问的模型来说根本不可用。</p><h3 id="4-Prompt-Search"><a href="#4-Prompt-Search" class="headerlink" title="4.Prompt Search"></a>4.Prompt Search</h3><p>一个典型的提示文本包含多个可以改进的元素。Zhao等人（2021年）认为，<strong>训练例子的选择、例子的顺序排列和提示的模板是导致few-shot learning的性能变化的三个要素</strong>。在寻找基于这三个要素的最佳提示方面，已经有大量的研究。Liu等人（2021a）研究了<strong>（1）从训练集中选择可以包含在提示语中的例子</strong>。Lu等人（2022）以及Kumar和Talukdar（2021）也进一步探讨了<strong>（2）决定这些例子的顺序</strong>。许多先前的工作已经研究了为NLP任务<strong>（3）手动编写几个有效的提示模板</strong>（Petroni等人，2019；Brown等人，2020；Schick和Schütze，2021b，a，c）。原则上，所有的提示搜索方法都将提示中的文本视为需要优化的参数空间，与Andreas等人（2018）的早期工作相似。在这些方法中，Jiang等人（2020）和Gao等人（2021）使用了提示模板的自动解读。受这些工作的启发，GRIPS也有对指令中的选定短语进行转述的功能，在§3.2.2中描述。Jiang等人（2020）（也就是LPAQA）特别关注寻找新的模式来表达基于关系的任务的关系，他们的搜索也涉及挖掘特定的语料库来寻找模板中的相邻词。相比之下，<strong>我们的搜索没有利用任何特定的任务属性，因此不受限于任何特定的任务</strong>。同时，Shin等人（2020）在Wallace等人（2019）的基础上，使用基于梯度的搜索来寻找能够形成提示模板的触发词。上述方法侧重于对提示模板的改变，以改变LM处理其输入的方式。在我们的工作中，我们反而专注于设计一种专门用于编辑任务指令的搜索方法，因为这是一个未被充分开发但很有前景的方向。</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="1-Prompt-Modes"><a href="#1-Prompt-Modes" class="headerlink" title="1.Prompt Modes"></a>1.Prompt Modes</h3><p>我们<strong>通过两种提示模式</strong>包括任务指示。<strong>Instruction-Only</strong>和<strong>Instruction+Examples</strong>（如下图所示）。</p><p><img src="https://img-blog.csdnimg.cn/0db6771798c043f7a1b073bd228dbc46.png" alt="在这里插入图片描述"></p><p>这里，指令指的是描述任务和标签的句子集合，而提示模式指的是三个组成部分（指令、语境中的例子和测试实例）的选择和安排，它们在前面适当地加上 “指令”、”输入 “和 “输出”。这些提示模式与Mishra等人（2022a）使用的模式相同（详情见附录B）。为了得到每一种提示，我们将其每个组成部分的文本连接起来。例如，指示+例子的提示包含指示，然后是例子，接着是测试实例。</p><h3 id="2-Gradient-free-Instructional-Prompt-Search-GRIPS"><a href="#2-Gradient-free-Instructional-Prompt-Search-GRIPS" class="headerlink" title="2.Gradient-free Instructional Prompt Search (GRIPS)"></a>2.Gradient-free Instructional Prompt Search (GRIPS)</h3><p>虽然指令性提示改善了大型LM的zero-shot任务性能，<strong>但这些提示的离散性和这类模型的巨大计算成本使得它们难以通过梯度更新来优化</strong>。在这项工作中，我们提出了Gradient-free Instructional Prompt Search (GRIPS)，它通过迭代地编辑指令和贪心搜索最佳修改来缓解这一问题（完整的伪代码显示在算法1）。</p><p><img src="https://img-blog.csdnimg.cn/f5d0ca7d5cc24ca08b3f0d8b15b6b470.png" alt="在这里插入图片描述"></p><p>这种搜索由模型在一小部分不属于测试集的例子上的表现来指导（称为分数集S，|S| &#x3D; 100，除非另有说明）。分数集可以被认为是每个任务的小型训练集。请注意，分数集中的例子可能有一个倾斜的标签分布，所以我们使用平衡精度作为我们的评分指标，也就是说，我们对整个S的精度进行重新加权，以平等地计算所有的类（下面的BalancedAccuracy）。受Lu等人（2022）的启发，<strong>我们还将模型预测的熵纳入评分函数，以促进产生不同标签的编辑指令</strong>。让Y是一个任务的所有标签的空间，其中y和ˆy分别是ground-truth和模型预测。如果H是熵，α是用于结合准确度和熵的比例因子（我们使用α&#x3D;10），那么得分函数为：</p><p><img src="https://img-blog.csdnimg.cn/03f1d5f8838541758a3bc6379e075c8a.png" alt="在这里插入图片描述">。</p><p>如下图所示，GRIPS算法从一个初始的基本指令开始，然后在每次迭代中，通过随机选择并对每个候选者应用l个短语级的编辑操作，产生m个新的候选者。这导致每次迭代中总共有m×l个采样操作（<strong>短语选择在下文第2.1节描述，编辑操作在第2.2节描述</strong>）。然后<strong>根据模型在S上的表现对这些候选者进行评分。如果最佳候选者的分数超过了当前基础指令的分数，那么该候选者就被指定为下一次迭代的基础。否则，就用同一基础指令继续搜索。当S上的得分在P次迭代中没有提高或达到最大的总迭代次数n时，搜索就会停止。</strong></p><p><img src="https://img-blog.csdnimg.cn/c95cc23bd42d46c8be3a12e8da9bd69a.png" alt="在这里插入图片描述"></p><p>在附录C中，<strong>我们考虑在GRIPS中加入模拟退火法</strong>（Pirlot, 1996），这样在搜索过程中，即使分数没有提高，我们也可以探索新的候选。然而，我们并没有看到平均的改善，所以我们总是使用贪心的选择规则。</p><h4 id="2-1-Splitting-Instructions-into-Phrases（短语的选择）"><a href="#2-1-Splitting-Instructions-into-Phrases（短语的选择）" class="headerlink" title="2.1 Splitting Instructions into Phrases（短语的选择）"></a>2.1 Splitting Instructions into Phrases（短语的选择）</h4><p>由于每条指令都是一个句子的集合，编辑操作可以在<strong>单词、短语或句子层面进行</strong>。在我们的初步实验中，我们发现在中间层次，<strong>即短语</strong>，工作是最有帮助的。这可能是因为短语级别的拆分使我们能够保持指令的一般结构，同时为编辑提供足够的灵活性。<strong>为了有效地将每个句子分割成短语，我们使用最先进的基于CRF的成分分析器</strong>（Zhang等人，2020a）。<strong>使用成分树，我们将叶子组合起来，直到我们从一个句子中获得不相交的短语级成分（S、VP、NP和其他短语块）。</strong>这一点通过图1中指示文本中的蓝色方括号进行说明。</p><h4 id="2-2-Edit-Operations（编辑操作）"><a href="#2-2-Edit-Operations（编辑操作）" class="headerlink" title="2.2 Edit Operations（编辑操作）"></a>2.2 Edit Operations（编辑操作）</h4><p>下面，我们描述一下本工作中使用的四种主要编辑操作：</p><p><strong>删除（del）：</strong>我们从指令中删除所有输入短语的出现。<strong>被删除的短语被储存起来，以便随后在添加操作中使用（我觉得这是很聪明的一点）。</strong></p><p><strong>交换（swap）</strong>：我们将两个短语作为输入，用第二个短语替换指令中的第一个短语的所有出现，反之亦然。</p><p><strong>转述（par）</strong>：我们<strong>用HuggingFace（Wolf等人，2020）公开的基于PEGASUS</strong>（Zhang等人，2020b）的意译模型生成的相应意译来替换输入短语的所有出现。</p><p><strong>增加（add）</strong>：我们对前几次迭代中删除的短语进行抽样，并在随机的短语边界处将其添加回指令中。</p><p>我们选择这些编辑操作，因为它们允许我们探索可能的指令的广泛空间，其中包括各种更简单、更少细节的抽象指令。让编辑操作逐渐简化指令是很重要的，因为这让GRIPS有机会实施Mishra等人（2022b）建议的一些准则，这些准则主要是限制指令中的细节和抽象。另一方面，我们也想让GRIPS探索不同的措辞风格，如果已经删除了细节，就把它们重新添加到指令中，因为指令的这些属性可能偶尔还是对模型有用。我们从Kumar等人（2020）的句子简化工作中获得灵感。<strong>在附录G中，我们表明GRIPS确实利用了我们所有的四个编辑操作</strong>。<u>（这本来也是我想问的，既然在前文说是随机选择对短语的编辑操作，如何保证模型确实利用四个所有的四个操作，这里作者还是十分严谨的证明了这一点）</u></p><h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><h3 id="1-Dataset"><a href="#1-Dataset" class="headerlink" title="1. Dataset"></a>1. Dataset</h3><p><strong>NATURAL-INSTRUCTIONS数据集</strong>（Mishra等人，2022a）由一组任务组成，<strong>每个任务由任务指令和标记的例子组成</strong>（同时还有一个理由或解释，证明有限的例子集的输出是合理的）。我们使用的是<strong>V2版</strong>，其中的数据集已经以开源的方式进行了扩展，包括更多的任务。（Dataset link: <a href="https://github.com/allenai/natural-instructions%EF%BC%89%E7%94%B1%E4%BA%8E%E6%88%90%E6%9C%AC%E5%92%8CAPI%E9%85%8D%E9%A2%9D%E7%9A%84%E9%99%90%E5%88%B6%EF%BC%8C%E5%9C%A8%E8%BF%99%E9%A1%B9%E5%B7%A5%E4%BD%9C%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E8%87%AA%E5%B7%B1%E9%99%90%E5%88%B6%E5%9C%A8%E8%BF%99%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84**8%E4%B8%AA%E4%B8%8D%E5%90%8C%E7%9A%84%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AD%90%E9%9B%86%E4%B8%8A**%E3%80%82%E5%85%B3%E4%BA%8E%E6%9B%B4%E5%A4%9A%E7%9A%84%E7%BB%86%E8%8A%82%EF%BC%8C%E8%AF%B7%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0%E4%B8%AD%E7%9A%84%E9%99%84%E5%BD%95A%E3%80%82">https://github.com/allenai/natural-instructions）由于成本和API配额的限制，在这项工作中，我们将自己限制在这个数据集中的**8个不同的二元分类任务的子集上**。关于更多的细节，请参考文章中的附录A。</a></p><p><strong>测试集。</strong>按照Mishra等人（2022a）的做法，我们<strong>从上述数据集中对例子进行子抽样以创建测试集</strong>。对于主要结果（第5.1节），<strong>测试集由每个任务的300个随机样本组成</strong>。由于财务成本，<strong>第5节中的所有其他分析和消融实验都是在每个任务的100个测试例子的子集上进行评估的</strong>（因此，我们的主表1和后续表格中的数字有所不同）。在所有的测试集中，数据的取样是尽可能的平衡，因为有些任务有高度倾斜的标签。如果一个标签缺乏足够的数据点来完美地平衡数据，我们就使用该标签的所有例子，然后从其他标签中随机抽样来填补这个集合。我们还确保测试集和分数集S（可以理解为训练集）之间没有例子重叠。</p><h3 id="2-Models"><a href="#2-Models" class="headerlink" title="2. Models"></a>2. Models</h3><p>我们使用<strong>参数≥1B的GPT模</strong>型（Radford等人，2018，2019；Brown等人，2020），特别是<strong>GPT-2 XL（1.5B参数）、InstructGPT babbage和curie</strong>。相对于标准的GPT-3模型，InstructGPT模型被专门设计为遵循任务指令，因此是我们工作中的自然选择（Ouyang等人，2022）。鉴于运行详细实验的高成本和API配额限制，我们没有用davinci引擎（最大的模型）进行实验，众所周知，它在一些NLP任务上表现出更强的性能（Brown等人，2020）。</p><p><strong>为了使用这些模型进行分类，我们遵循Zhao等人（2021）的程序，计算标签标记的对数-概率</strong>。<strong>最终的预判是通过对这些标签概率进行argmax来获得的</strong>。 请注意，我们的设置与Mishra等人（2022b,a）不同，我们没有将分类制定为以ROUGE为评价指标的文本生成任务。</p><h3 id="3-Hyperparameters"><a href="#3-Hyperparameters" class="headerlink" title="3. Hyperparameters"></a>3. Hyperparameters</h3><p>搜索中的主要超参数包括：每个candidate的编辑操作数l，每个迭代中的candidate数m，迭代数n，以及用于早期停止的patience P。在我们的实验中，我们设定l&#x3D;1，m&#x3D;5，n&#x3D;10，P&#x3D;2，除非另有提及，否则每个任务的搜索都是以3个不同的种子进行的。有关其他细节，请参考文章中的附录F。</p><h2 id="Results-and-Discussion"><a href="#Results-and-Discussion" class="headerlink" title="Results and Discussion"></a>Results and Discussion</h2><h3 id="1-Effectiveness-of-GRIPS"><a href="#1-Effectiveness-of-GRIPS" class="headerlink" title="1. Effectiveness of GRIPS"></a>1. Effectiveness of GRIPS</h3><p>实验的主要结果显示在下表中：</p><p><img src="https://img-blog.csdnimg.cn/b5d942ab091a4b0399afe440328fdc78.png" alt="在这里插入图片描述"></p><p>在不同的任务中，<strong>GRIPS平均提高了GPT-2 XL、InstructGPT babbage和Curie的准确性，分别为9.36、4.29和2.36个百分点</strong>。我们通过对实例和随机种子重新取样10万次的引导法（Efron和Tibshirani，1994）对这些改进进行双侧假设测试。每种方法的准确率在测试数据、种子和任务中取平均值。InstructGPT babbage和curie的准确率提高的p值分别为0.015和0.011，表明GRIPS的改进在p&lt;0.05的水平上具有统计学意义（任务级性能见下图）。</p><p><img src="https://img-blog.csdnimg.cn/a8f752685c614808bdd7056440352a59.png" alt="在这里插入图片描述"></p><p>这张图中无阴影的是搜索前的表现，有阴影的点是的搜索后的表现，在不同的任务和模型中使用仅有指令的提示。并且误差条显示的是95%的置信区间。尽管与babbage相比，curie的改进幅度较小，但curie的结果显示出更大的稳定性（见上表中较小的置信区间）。</p><p>虽然上图显示搜索前的准确率以及不同任务的改进幅度有相当大的差异，<strong>但我们发现GRIPS在babbage的所有任务和curie的所有任务（即除任务021和022外的所有任务）中都提高了性能</strong>。我们注意到，准确率的下降可能是由于我们评分函数中的熵项，它有利于产生具有更平衡标签分布的预测的指令。我们的结果也证实了，在仅有指令的情况下，较大的指令GPT模型优于较小的非指令GPT模型（Ouyang等人，2022）。我们看到，在搜索前，从GPT-2 XL到InstructGPT babbage作为基础模型，以及从babbage到Curie，准确性都有明显的跳跃。</p><h3 id="2-GRIPS-Outperforms-Manual-Rewriting"><a href="#2-GRIPS-Outperforms-Manual-Rewriting" class="headerlink" title="2. GRIPS Outperforms Manual Rewriting"></a>2. GRIPS Outperforms Manual Rewriting</h3><p>Mishra等人（2022b）提供了通过改写改进指令式提示的指南。他们的四个关键建议是</p><ol><li>将抽象的句子改写成简明而有针对性的低级指令；</li><li>以列表的形式列举长的指令；</li><li>将否定的句子（包含像do not X这样的短语）改写成语义等同的肯定实例（包含像do Y这样的短语）;</li><li>重新强调输出的限制（与分类任务有关）。后者是通过在每个数据点的输入部分之后增加一行，提及可能的标签集（如 “预期输出：A&#x2F;B”，其中A和B是任务标签）。</li></ol><p>由于Mishra等人(2022b)中的重写指令没有公开，我们根据这些准则进行了自己的重写任务（在附录D中描述）。然后，我们将这些手工改写的提示语与GRIPS自动获得的提示语进行比较。我们使用两种条件进行人工改写：在第一个 “人工改写 “条件下，我们将建议（1）、（2）和（3）结合起来；在第二个 “人工改写+限制输出 “条件下，我们使用所有四个建议。</p><p><img src="https://img-blog.csdnimg.cn/e96505803b384dd4bd554daaba3efb4e.png" alt="在这里插入图片描述"></p><p>上表显示，<strong>我们的搜索优于所有模型的人工改写，对于GPT-2 XL、InstructGPT babbage和curie，分别提高了5.56、2.29和1.50分</strong>。此外，GRIPS的改进在不同的任务中更为一致。在所有的模型中，GRIPS至少提高了一半任务的性能（在表2的括号中显示），而手动重写在所有条件下提高了不到一半的任务，除了babbage没有增加限制输出。与Mishra等人（2022b）不同，我们发现在提示中包括一个额外的句子来重申标签空间（上表中的Limit Output）会损害InstructGPT模型的性能。而GPT-2 XL的情况则相反，它有一些性能上的提高。这可能是因为Mishra等人（2022b）将分类视为一项生成任务，而我们直接使用语言模型计算标签标记的概率。</p><h3 id="3-Learning-From-Instructions-vs-Examples"><a href="#3-Learning-From-Instructions-vs-Examples" class="headerlink" title="3. Learning From Instructions vs Examples"></a>3. Learning From Instructions vs Examples</h3><p>以前关于提示搜索的工作研究了<strong>k-shot学习的例子的选择和排序</strong>。由于GRIPS能够搜索到更好的指令，因此直接比较这两种搜索的性能成为可能。我们在下面描述这样一个实验，同时保持相同的数据和计算预算，以进行公平的比较。</p><p>我们使用一个简单而有效的算法来进行仅有实例的搜索。在搜索的每一步，我们<strong>从分数集中随机抽出k个输入例子</strong>，然后计算模型在分数集中剩余点上的性能。搜索一直运行到达到最大的迭代次数，然后返回具有最佳性能的例子集并在测试集上进行评估。请注意，k会因任务的不同而不同；我们在1024个标记的空间中尽可能多地适应例子（对我们的任务来说，在8到28之间）。首先，由于我们从分数集中抽出例子，所以我们为纯例子搜索和GRIPS使用了相同数量的数据，并且我们为每个例子使用了相同的分数集。其次，我们可以简单地对提议的例子集进行评分，直到我们达到与GRIPS中执行的模型查询的最大数量相同。</p><p>下表就包含了这种比较的结果。</p><p><img src="https://img-blog.csdnimg.cn/e96505803b384dd4bd554daaba3efb4e.png" alt="在这里插入图片描述"></p><p>对于GPT-2 XL来说，只用例子的搜索优于GRIPS。然而，当我们使用InstructGPT模型时，这些模型被设计成能更好地遵循文本指令（Ouyang等人，2022），GRIPS优于范例提示搜索（对于babbage和curie分别为1.54和1.62分）。GRIPS和纯实例搜索的任务级比较见附录E。在这个实验中，我们使用了一个相当简单的只用例子的搜索方法，它使用了与GRIPS相同的资源，但我们注意到，更复杂的方法也可以考虑。相对于我们的例子搜索，可以使用遗传算法（Kumar和Talukdar，2021），为每个测试实例找到不同的例子集（Liu等人，2021a），或者使用不依赖标记分数集的搜索启发式方法（Lu等人，2022）。</p><h3 id="4-Task-Specific-vs-Task-Agnostic-Instructions"><a href="#4-Task-Specific-vs-Task-Agnostic-Instructions" class="headerlink" title="4. Task-Specific vs Task-Agnostic Instructions"></a>4. Task-Specific vs Task-Agnostic Instructions</h3><p>GRIPS是取决于我们用来初始化搜索的指令的。这就提出了一个自然的问题：<strong>初始指令的语义是如何影响搜索和最终性能的。</strong>我们旨在通过比较两种具有不同语义的初始指令的设置来了解这一点，即特定任务和任务无关的指令（例子见下表）。</p><p><img src="https://img-blog.csdnimg.cn/b4b4f5fbdf0c4052a98d1a3dca13088a.png" alt="在这里插入图片描述"></p><p>特定任务的指令是来自NATURAL INSTRUCTIONS数据集的指令，包含了<strong>关于任务、预期输出和特定输出正确的条件的信息</strong>。<strong>在任务无关的设置中，初始指令包含一些通用文本和与任务对应的所有可能的标签列表（通过手动阅读每个任务的原始指令获得），但它不包含关于任务的其他有意义的信息。我们在本实验中使用的任务无关指令的模板如下：</strong></p><p><img src="https://img-blog.csdnimg.cn/4407aceb9b87428fbe9911556692833a.png" alt="在这里插入图片描述"></p><p>下表显示了这种比较的结果。</p><p><img src="https://img-blog.csdnimg.cn/f0768c67294e42e89a3c4b30a5da1e15.png" alt="在这里插入图片描述"></p><p>我们发现，<strong>GRIPS在特定任务和任务无关设置中都很有效，分别提高了5.30和2.42分</strong>。有趣的是，与特定任务的指令相比，GPT-2 XL在任务不可知的指令下一直表现得更好。另一方面，InstructGPT系统在搜索前后，与任务无关的指令相比，显示出更好的性能。与Webson和Pavlick（2021年）使用基于BERT的LMs相比，我们看到对于InstructGPT模型，（初始）指令的任务相关语义在任务表现中可以发挥重要作用。</p><h3 id="5-GRIPS-is-Effective-for-Smaller-Score-Sets"><a href="#5-GRIPS-is-Effective-for-Smaller-Score-Sets" class="headerlink" title="5. GRIPS is Effective for Smaller Score Sets"></a>5. GRIPS is Effective for Smaller Score Sets</h3><p>虽然我们默认使用一个大小为|S| &#x3D; 100的分数集，但在其他条件相同的情况下，<strong>最好是使用尽可能少的数据</strong>。因此，<strong>我们研究了GRIPS在分数集可用数据有限的情况下的有效性</strong>。我们使用InstructGPT babbage进行搜索，每个任务的分数集中有100、50或20个数据点。结果如图4所示。</p><p><img src="https://img-blog.csdnimg.cn/29c85babce134090bdfd16e672276c52.png" alt="在这里插入图片描述"></p><p>我们首先观察到，随着分数集大小的减少，搜索的改进幅度也在减少（当|S| &#x3D; 100时，获得4.27分，而当|S| &#x3D; 20时，获得1.0分）。这种趋势是可以预期的，因为在S中使用较少的例子相当于有一个较小的训练集，因此我们预期模型的概括性会更差。对于非常有限的数据设置，我们看到使用少至|S| &#x3D; 20个数据点，准确率提高了1.0点，这仍然是很有用的。另一方面，当有更多的数据可用时，我们的结果表明，将|S|的大小增加到100以上将导致模型性能的进一步改善（尽管我们预计这将在一个点之后趋于平稳）。</p><h3 id="6-Search-Improvements-Correlate-with-Model-Sensitivity-to-Instructions"><a href="#6-Search-Improvements-Correlate-with-Model-Sensitivity-to-Instructions" class="headerlink" title="6. Search Improvements Correlate with Model Sensitivity to Instructions"></a>6. Search Improvements Correlate with Model Sensitivity to Instructions</h3><p><strong>我们观察到，GRIPS在某些任务上比其他任务效果更好。在此，我们试图了解哪些因素可以解释这种差异性（发现实验结果的问题并且去发现问题）</strong>。我们发现，<strong>一个模型对不同指令的敏感性</strong>是解释搜索性能提高的一个重要因素。<strong>对于一个给定的任务和模型，我们将模型的指令敏感性定义为每个候选任务指令在搜索的第一次迭代中获得的分数的标准偏差</strong>。当这个数字较大时，模型的性能对指令的变化更加敏感。有趣的是，在下表中，我们发现，一项任务的指令敏感性与GPT-2 XL和InstructGPT babbage模型的性能改进幅度密切相关（相关性 Pearson’s r &gt; 0.7)（P &lt; 0.05）。</p><p><img src="https://img-blog.csdnimg.cn/e95e0aa9d3ec4e15898525047e481843.png" alt="在这里插入图片描述"></p><p>然而，对于curie，相关性相对较弱（r &#x3D; 0.51），并且在p &lt; 0.05时不显著。总的来说，我们观察到灵敏度值和最终的改进之间有适度到强烈的相关性，<strong>我们鼓励未来的工作在完全运行搜索之前首先检查任务的灵敏度，作为我们方法有效性的一个指标</strong>。</p><h3 id="7-Semantics-of-Searched-Instructions"><a href="#7-Semantics-of-Searched-Instructions" class="headerlink" title="7. Semantics of Searched Instructions"></a>7. Semantics of Searched Instructions</h3><p>前面那张大表包含了任务021、137和195中搜索到的指令的一些例子（其他任务的搜索指令见附录H）。我们在下面对这些例子进行分析，<strong>讨论GRIPS所做的在人类读者看来合理的编辑，以及使指令在语义上不连贯的编辑</strong>。</p><p>对于任务021，我们看到搜索到的指令的一致性有很大的变化。语义上最连贯的指令对应于InstructGPT curie，其中主要的修改是将 “grammatical”缩短为简单的 “errors”。这不仅保留了意思，而且还简化了指令的第一句。对于InstructGPT babbage来说，有两个关键变化：”是正确的 “短语和实体列表被删除。虽然由此产生的指令在某些方面更简单，但在某些方面完全不连贯。对于GPT-2 XL来说，”is correct”短语与 “indicating no”短语的反复重新放置使得指令不连贯，并具有主动误导性（即，<strong>如果正确就通过 “不”来回应，这与原始指令相反</strong>），但这种改变仍然提高了模型性能。</p><p>对于任务137，我们观察到GRIPS在使用 “任务 “的时候，会提前停止，并返回原始指令。在使用GPT-2 XL模型时，GRIPS提前停止并返回原始指令。对于InstructGPT babbage，我们看到GRIPS转述了toxicity的定义和最后一句陈述任务的标签，然而这些改变不一定能简化原始指令。有趣的是，对于Instruct- GPT curie，toxicity的定义被完全删除。最后，我们看到任务195发生了语义不连贯的编辑。<strong>GRIPS一直在重新编辑不连贯的指令，其中关于可能的标签（”正面 “或 “负面”）的信息被删除。虽然这对人类来说可能是反直觉的，但对模型来说却很有效，并导致了性能的提高</strong>。</p><p>这些发现建立在Webson和Pavlick（2021）的结果之上，<strong>他们观察到 “irrelevant(不相关的)”或 “confusing(混乱的)”指令（在人们眼中）的表现与 “good”指令一样好，有时甚至更好</strong>。我们表明，除了Webson和Pavlick（2021）中的∼300M参数的遮蔽LM之外，这一趋势对于具有≥1B参数的较大的自回归LM也是成立的，而且这一趋势对于专门为遵循指示而设计的InstructGPT模型也成立。同时，先前在第5.4节中的观察表明，对于InstructGPT模型来说，用与任务相关的指令进行初始化是有益的，这些指令超出了列出可能的标签。总的来说，我们的结果表明，这些语言模型能够在一定程度上对指令中的语义变化做出明智的反应。与语境中学习机制的研究类似（Xie等人，2022；Razeghi等人，2022；Min等人，2022），<strong>指令如何被模型内部利用在很大程度上仍是未知数，值得进一步研究</strong>。</p><h3 id="8-Effectiveness-of-GRIPS-on-“Instruction-Examples”-Prompts"><a href="#8-Effectiveness-of-GRIPS-on-“Instruction-Examples”-Prompts" class="headerlink" title="8. Effectiveness of GRIPS on “Instruction + Examples” Prompts"></a>8. Effectiveness of GRIPS on “Instruction + Examples” Prompts</h3><p>最后，我们表明<strong>GRIPS也可以应用于指令+例子的提示</strong>，这些提示在测试实例之前包含额外的k个输入-输出例子对。虽然我们在这种情况下仍然搜索任务指令，但在提示中包括例子可以提高基础分数并改变搜索的流程。与前面第三个实验不同的是，我们将<strong>所有任务的例子数k&#x3D;4</strong>，因为更高的k值会使经济成本过大。为了消除提示中的多数标签的bias（Zhao等人，2021年），我们确保每个标签中的例子数量相等，包括在提示中。由于提示中四个例子的选择随这里的随机种子而变化，在我们的API配额下，如果可能的话，我们在这些实验中使用更大数量的种子。</p><p><img src="https://img-blog.csdnimg.cn/901391e603be4cf6a8634234ca51d439.png" alt="在这里插入图片描述"></p><p>在上表中，我们比较了GRIPS对指令+例子的提示（k&#x3D;4）与GRIPS对指令-纯提示和纯例子的搜索。我们发现，在这种情况下，我们的搜索对所有模型都是有效的，大约提高了2个点的准确率。对于InstructGPT模型来说，仅有指令和指令+示例模式之间的性能差异出乎意料地小，因为两种模式的准确率相差不到0.1个百分点。然而，对于Babbage和Curie来说，包含指令的提示比仅有示例的提示要好，大约是1.6个百分点。<strong>只有对GPT-2 XL来说，”纯实例 “搜索是最好的方法，这可能是因为这个模型的设计方式不像InstructGPT模型那样可以得到指令</strong>。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>我们介绍了<strong>GRIPS，这是一种自动搜索算法，它可以编辑为人类设计的任务指令，并返回能改善下游任务性能的指令</strong>。我们证明GRIPS对GPT-2 XL、InstructGPT babbage和Curie的纯指令和指令+例子提示是有效的。与手工改写和只用实例搜索的比较表明，GRIPS优于这些方法，表明广泛探索模型指令的空间是提高模型性能的有效方法。我们表明，当用与任务无关的指令进行初始化时，我们的搜索是有效的，而且在分数集中只有20个例子的情况下，它也能发挥作用。定性分析证实，即使是1B+大小的InstructGPT模型也可以通过语义不连贯的指令得到改善。在未来的工作中，如果有更多的资源，看看我们在最强大的InstructGPT davinci引擎上的搜索效果将是非常有趣的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《Multitask Prompted Training Enables Zero-shot Task Generalization》论文阅读笔记</title>
    <link href="/2022/07/16/%E3%80%8AMultitask-Prompted-Training-Enables-Zero-shot-Task-Generalization%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/16/%E3%80%8AMultitask-Prompted-Training-Enables-Zero-shot-Task-Generalization%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>Paper：<a href="https://arxiv.org/abs/2110.08207">https://arxiv.org/abs/2110.08207</a></p><p>Code：<a href="https://github.com/bigscience-workshop/t-zero">https://github.com/bigscience-workshop/t-zero</a></p><p><img src="https://pic4.zhimg.com/80/v2-6bf61978a2f8aaf53cb3984c6532156f_1440w.jpg" alt="img"></p><p>这篇论文是我上一篇工作中最主要参考的工作，所以其实已经读过很多遍了，这篇论文由Hugging Face牵头，用一连串数字可以来概括这篇论文：</p><ul><li>一共收集了<strong>171个</strong>多任务数据集，总共创建了<strong>1939个</strong>prompt，平均每个数据集有<strong>11.3个</strong>prompt；</li><li>共有来自<strong>8个</strong>国家、<strong>24家</strong>机构的<strong>36位</strong>人员贡献prompt；</li><li>基于包含prompt的数据集进行多任务学习（模型为11B的T5），Zero-Shot性能大幅超越大16倍的GPT-3模型；</li><li>与Google的同期工作Instruction Tuning（FLAN模型）相比，Zero-Shot性能在各数据集上几乎均有提升或可比，而模型参数<strong>减少10倍</strong>（Google的FLAN模型为137B）；</li></ul><p><img src="https://pic3.zhimg.com/80/v2-38f2a0fb3b04301c4c2cd4b9c3eaf9a2_1440w.jpg" alt="img"></p><p>我们可以发现：如此“耗资巨大”的工程，<strong>将prompt+多任务学习紧密结合起来，也许是提升Zero-Shot性能的“完美配方”。</strong></p><p>此外，很多读者也许会发现，这篇论文不就是Instruction Tuning方法吗？</p><p>事实上，本篇论文与Google的Instruction Tuning思想相同，都是将<strong>包含prompt的数据集进行多任务学习</strong>，在下游未见任务进行Zero-Shot性能测试。不过，仍有很多细节和性能不相同，接下来就让我们一起一探究竟吧～</p><h2 id="1、Instruction-Tuning回顾"><a href="#1、Instruction-Tuning回顾" class="headerlink" title="1、Instruction Tuning回顾"></a><strong>1、Instruction Tuning回顾</strong></h2><p>当前，NLP发展正进入第四范式**[2]<strong>——</strong>prompting时代**：预训练语言模型加持下的Prompt Learning。</p><p>CMU博士后研究员刘鹏飞在综述论文**[3]**中定义了Prompt的两种主要形式：</p><ul><li>填充文本字符串空白的完形填空（Cloze）prompt；</li><li>用于延续字符串前缀的前缀 (Prefix) prompt；</li></ul><p>而Google的Instruction Tuning中的Prompt形式更像是一种更明显的指令&#x2F;指示（可以归为第三种Prompt形式）：</p><p><img src="https://pic4.zhimg.com/80/v2-a2df064f10fc9c2292e7795c758d2423_1440w.jpg" alt="img"></p><p><strong>Instruction Tuning仍属于prompting的范畴</strong>，其核心要点是：</p><ul><li>构建了大量的多任务数据集；</li><li>为每个数据构建了“指令式”的prompt；</li><li>采用多任务学习机制进行训练；</li><li>更加关注下游任务的Zero-Shot性能；</li></ul><p>需要注意的是：Instruction Tuning采用多任务学习机制，整个LM模型参数是需要tuned的。本篇论文继承了Instruction Tuning思想，所以在阅读论文时候我们重点介绍本篇论文的数据集选择和Prompt设计。</p><p>在这篇论文中，实验研究了两个问题：</p><ul><li><p>首先，多任务提示的训练是否能提高对未完成任务的泛化？</p></li><li><p>第二，在更广泛的提示上进行训练是否能提高对提示措辞的稳健性？</p><p><strong>对于第一个问题，</strong>我们发现多任务训练能够实现zero-shot任务的泛化，表明我们的模型在11个保留任务的数据集中有9个与GPT-3的性能相匹配或超过，尽管它的体积要小16倍。我们还表明，在BIG-bench基准的14个任务中，该模型比大型基线语言模型有13项改进。<strong>对于第二个问题，</strong>我们发现，在每个数据集上训练更多的提示语，可以持续地提高中位数，并减少在保持任务上的性能变化。</p></li></ul><h2 id="2、多任务数据集选择"><a href="#2、多任务数据集选择" class="headerlink" title="2、多任务数据集选择"></a><strong>2、多任务数据集选择</strong></h2><p>我们首先假设NLP数据集被基本划分为任务。<strong>我们用 “任务 “一词来指代由一组特定数据集测试的一般NLP能力。</strong>为了评估对新任务的zero-shot泛化，我们在一个任务的子集上进行训练，并在一组被保留的任务上进行评估。</p><p>不幸的是，<strong>NLP任务的分类是模糊的</strong>，特别是当人们试图分离出一种独特的技能时。例如，许多数据集评估常识性知识，一些多任务工作将常识性知识定义为一个独立的任务。然而，常识数据集的差别很大。</p><p><strong>注意到按任务分组是一个不完美的启发式方法，我们在组织我们的任务分类学时，偏向于根据任务格式，而不是根据文献中的惯例要求技能</strong>（Khashabi等人，2020b；Vu等人，2020；Ye等人，2021）。我们从这些论文中收集所有的数据集，并排除那些非英语的数据集（这也排除了编程语言和结构化注释，如解析树）或如果它们需要特殊的领域知识（如生物医学）。这就产生了<strong>12个任务和62个数据集</strong>，这些数据集在我们的训练和评估混合物中都有公开的提示，截至目前。所有的实验都使用 “huggingface “数据集库中的数据集。</p><p>为了测试zero-shot泛化性能，我们保留了四个任务的所有组成数据集：自然语言推理（NLI）、核心推理解决、句子完成和词义歧义。<strong>我们选择自然语言推理作为保留任务</strong>，是因为人类也会将自然语言推理作为保留任务进行zero-shot泛化。大多数人从来没有接受过明确的训练来对一个前提句子是否包含或违背一个假设句子进行分类，但是他们发现不经过训练就可以直观地执行这项任务（Williams等人，2020）。出于同样的原因，我们也排除了核心推理和词义歧义。我们进一步排除了句子完成的任务，因为它可能与NLI过于相似（附录D.2详细讨论了这一点）。此外，我们不在Brown等人（2020）用于评估的任何数据集上训练我们的主要模型，这样我们的主要结果将是一个公平的零次比较。我们还在附录E中验证了这些任务的数据没有通过预训练语料库而被泄露。</p><p>最后，<strong>我们对BIG-bench的数据集的一个子集进行了进一步的评估</strong>，BIG-bench是一个最近由社区驱动的基准，以创建一个多样化的困难任务集来测试大型语言模型的能力。BIG-bench的子集包括一个面向语言的任务选择，BIG-bench的维护者已经为其准备了初步结果，这些任务构成了T5标记器的词汇量（即只包含英语文本，没有表情符号或其他特殊字符）。<strong>BIG-bench的所有任务都是我们训练中的新任务。</strong></p><p>总的来说，如下图所示，本篇论文在构造多任务数据集（共171个）时，将黄色部分的任务数据作为训练集，而绿色部分的任务数据作为Zero-Shot测试集。其中，BIG-Bench是一个新的基准评测，创建了多样化的困难任务集合来评测大型语言模型的能力。</p><p>数据构建的基本原则就是：Zero-Shot测试集中的数据未在训练集中出现。</p><p><img src="https://pic4.zhimg.com/80/v2-84a2a47f68a7b3a9d79a0a8fdd94256f_1440w.jpg" alt="img"></p><h2 id="3、Prompt设计"><a href="#3、Prompt设计" class="headerlink" title="3、Prompt设计"></a><strong>3、Prompt设计</strong></h2><p><img src="https://pic4.zhimg.com/80/v2-6357ab9af7e7003b128d954312025663_1440w.jpg" alt="img"></p><p>这里，以QQP任务为例（如上图所示），来介绍本篇论文的Prompt设计，共由两部分模板组成：</p><ul><li>input template：*{Question1}{Question2}Pick one:These questions are duplicates or not duplicates.*</li><li>output template：*{Choices[label]}* 当label&#x3D;0时，输出为Not duplicates</li></ul><p>Hugging Face开发了1个交互式程序用于编写Prompt。为了使模型更加鲁棒，鼓励用户以自己的风格开发创建更加多样化的prompt。共有来自8个国家、24家机构的36位人员参与了prompt贡献。</p><p>Prompt开发地址为：<a href="https://link.zhihu.com/?target=https://github.com/bigscience-workshop/promptsource">https://github.com/bigscience-workshop/promptsource</a> ，感兴趣的小伙伴可以尝试下：</p><p><img src="https://pic1.zhimg.com/80/v2-753ad77f6dd18fab5df4a50ccffd07e8_1440w.jpg" alt="img"></p><p>例如,NLI数据集一个的prompt可以通过模板语言jinja构建：</p><p><img src="https://pic3.zhimg.com/80/v2-9b2623cb492fa19fc6be8fb97bf1256a_1440w.jpg" alt="img"></p><p>本篇论文最终共收集了1939个prompt，所有构建的prompt集合P3（Public Pool of Prompts）也进行了开源(见论文附录G)。</p><h2 id="4、实验结果"><a href="#4、实验结果" class="headerlink" title="4、实验结果"></a><strong>4、实验结果</strong></h2><p>论文是基于T5+LM模型（基于T5进一步做LM训练）进行训练的，模型参数为11B。经过多任务prompt训练的模型为：</p><ul><li>T0：基于构建的171个多任务数据集进行训练；</li><li>T0+：除T0数据集外，新增GPT-3的验证集；</li><li>T0++：除T0数据集外，新增GPT-3和SuperGLUE的验证集；</li></ul><h3 id="Generalization-to-Held-out-Tasks"><a href="#Generalization-to-Held-out-Tasks" class="headerlink" title="Generalization to Held-out Tasks"></a>Generalization to Held-out Tasks</h3><p><img src="https://pic2.zhimg.com/80/v2-648845ff45efb282fec5611350a924fd_1440w.jpg" alt="img"></p><p>作者首先的研究问题是，<strong>多任务提示训练是否能提高对保留任务的泛化能力</strong>。将T0与我们的T5+LM基线在四个暂不执行的任务上进行比较。我们的方法在所有的数据集上都比我们的基线有明显的提高，这表明多任务提示训练比只用相同的模型和提示的语言建模训练有好处。</p><p>接下来，我们将T0与截至目前可用的最大的语言模型的zero-shot性能进行比较，即各种GPT-3模型，最高可达175B参数。我们发现T0在11个保留数据集中的8个匹配或超过了所有GPT-3模型的性能。而T0模型比GPT-3比小16倍，GPT-3预训练过程也可看作是基于prompt进行多任务学习的。</p><p>为了在更多的保留任务上评估我们的模型，作者还在BIG-bench（BIG-bench协作，2021）的一个子集上评估了T0、T0+和T0++的zero-shot性能。</p><p><img src="https://img-blog.csdnimg.cn/4de6c05093d345418101ae55a2f3f2c7.png" alt="在这里插入图片描述"></p><p>BIG-bench的任务涵盖了我们的训练任务中没有包括的各种新技能，例如推断物体序列的顺序、解决逻辑网格谜题，以及区分真实陈述和常见的错误概念。BIG-bench的维护者为每个数据集提供了一个提示，我们用它将我们的模型与一系列由谷歌训练并由BIG-bench维护者评估的初步诊断基线模型进行比较。这些模型是在一个标准的语言建模目标上训练的仅有解码器的Transformer语言模型，模型大小不一。我们发现，除了StrategyQA之外，至少有一个T0变体在所有任务上的表现超过了所有的基线模型（图5）。在大多数情况下，<strong>我们模型的性能随着训练数据集数量的增加而提高（即T0++优于T0+，T0+优于T0）</strong>。</p><h3 id="Effect-of-Prompts-from-More-Datasets"><a href="#Effect-of-Prompts-from-More-Datasets" class="headerlink" title="Effect of Prompts from More Datasets"></a>Effect of Prompts from More Datasets</h3><p><img src="https://pic1.zhimg.com/80/v2-4da0020f88088c5f396a3196ffc4bcb8_1440w.jpg" alt="img"></p><p>不过实验也发现：增加更多训练集数据会不会一致性提升Zero-Shot性能（如上图所示）。</p><h3 id="Effect-of-More-Prompts-per-Dataset"><a href="#Effect-of-More-Prompts-per-Dataset" class="headerlink" title="Effect of More Prompts per Dataset"></a>Effect of More Prompts per Dataset</h3><p><img src="https://pic3.zhimg.com/80/v2-82c07f76e96ed00d584e0b9aec43f4b6_1440w.jpg" alt="img"></p><p>此外，实验也表明：增加更多的Prompt数量，会提升Zero-Shot泛化性能。</p><h2 id="5、T0-vs-FLAN"><a href="#5、T0-vs-FLAN" class="headerlink" title="5、T0 vs FLAN"></a><strong>5、T0 vs FLAN</strong></h2><p>上文提到过，本文的T0模型与Google的FLAN模型均属于Instruction Tuning思想，但仍有一些细节区别：</p><p><img src="https://pic4.zhimg.com/80/v2-57ca171e6c71b4c54263b2b322a7f40b_1440w.jpg" alt="img"></p><p>此外：</p><ul><li>T0++模型几乎在所有任务上超越或比肩FLAN模型，T0模型比FLAN模型小10倍。然而相同量级(8B)的FLAN模型，在多任务学习之后的Zero-Shot性能会下降。</li><li>FLAN模型发现prompt个数增加反而会降低性能，而T0模型不会。这说明本篇论文构建的prompt更加多样化，从而使模型更加鲁棒、泛化能力更强。</li></ul><h2 id="6、总结与思考："><a href="#6、总结与思考：" class="headerlink" title="6、总结与思考："></a><strong>6、总结与思考：</strong></h2><p>这篇论文构建的prompt数目多达1939个，虽然有程序界面进行设计，但仍然逃脱不了需要人工参与。</p><p>prompting时代或许更应该关注prompt的高效设计，比如：如何自动挖掘模板。而prompt-tuning怎样更好地融入多任务学习中，也值得进一步探讨。</p><p>此外，本文采用T5这种条件生成模型对所有不同任务进行了统一建模，更让人觉得：当前NLP发展正进入一个“<strong>大一统时代</strong>”：</p><ul><li><strong>框架统一</strong>：不同NLP任务可采用统一的模型框架建模，如Seq2Seq框架基本上可以建模所有NLP任务。</li><li><strong>数据统一</strong>：不同NLP任务的数据可以融合prompt构建统一的数据形式，如指令式的prompt。</li><li><strong>训练统一</strong>：训练方式可采取统一的多任务学习机制。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《Language Models are Few-Shot Learners》论文阅读笔记</title>
    <link href="/2022/07/15/%E3%80%8ALanguage-Models-are-Few-Shot-Learners%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/15/%E3%80%8ALanguage-Models-are-Few-Shot-Learners%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>Paper：<a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p><p>Code：<a href="https://github.com/openai/gpt-3">https://github.com/openai/gpt-3</a></p><p><img src="https://img-blog.csdnimg.cn/859bd5fb16f04e7d99a6d90a8575fa5c.png" alt="在这里插入图片描述"></p><p>这篇文章非常长，也是一篇有着非常重要成果的文章，但是我只阅读到实验评估那里，关于后面的一些实验结果的分析之后若是阅读到别的文章有一些结果的对比需要看或者是需要阅读相关代码我再来看。</p><p><strong>摘要：</strong></p><p>最近的工作表明，在许多NLP任务和基准上，通过对大型文本语料库进行预训练，然后对特定的任务进行微调，可以获得巨大的收益。<strong>虽然这种方法在结构上通常与任务无关，但它仍然需要针对特定任务的数千或数万个例子的微调数据集。</strong>相比之下，人类通常只需通过几个例子或简单的指令就能完成一项新的语言任务–而目前的NLP系统在很大程度上仍难以做到这一点。在这里，我们展示了扩大语言模型的规模，大大改善了与任务无关的、少量的性能，有时甚至达到了与之前最先进的微调方法的竞争力。具体来说，我们训练了GPT-3，一个具有1750亿个参数的自回归语言模型，比以前的任何非稀疏语言模型多10倍，并测试了它在少数情况下的性能。对于所有的任务，<strong>GPT-3的应用没有任何梯度更新或微调，纯粹通过与模型的文本互动来指定任务和少量演示</strong>。<strong>GPT-3在许多NLP数据集上取得了强大的性能，包括翻译、回答问题和cloze任务，以及一些需要即时推理或领域适应的任务，如解读单词、在句子中使用一个新词或进行3位数的算术</strong>。同时，我们也发现了一些数据集，在这些数据集中，GPT-3的几率学习仍然很困难，还有一些数据集，GPT-3面临着与大型网络语料库训练有关的方法学问题。最后，我们发现，GPT-3可以生成人类评价者难以区分的新闻文章样本。我们讨论了这一发现和GPT-3总体上的更广泛的社会影响。</p><p>总的来说：常见的预训练模型需要大量的监督数据在特定特务上进行微调，而GPT-3仅仅需要文本交互来指定任务和少量演示即可。GPT-3在众多nlp任务上取得了出色的性能。</p><p><strong>1、导言</strong></p><p>目前预训练模型的主要局限性在于，尽管体系结构与任务无关，但仍需要特定于任务的数据集和特定于任务的微调：要在所需任务上实现出色的性能，通常需要对数据集进行微调特定于该任务的数千到数十万个示例。出于以下几个原因，我们希望消除此限制。</p><p><strong>首先，从实践的角度来看，每个新任务都需要有大量带标签示例的数据集，这限制了语言模型的适用性。</strong>存在各种各样可能的有用的语言任务，包括从纠正语法到生成抽象概念的示例到撰写短篇小说的任何事情。对于这些任务中的许多任务而言，很难收集大型的有监督的训练数据集，尤其是当必须为每个新任务重复执行该过程时。</p><p><strong>第二，利用训练数据的虚假相关性的潜力从根本上随模型的表达能力和训练范围的缩小而增长。</strong>这会给预训练模型带来问题，在该模型中，<strong>模型被设计得很大，可以在预训练期间吸收信息，但随后会在非常狭窄的任务分布上进行微调。有证据表明，这种方法下实现的泛化效果可能很差，因为该模型过于针对训练分布，并且无法很好地泛化该模型</strong>。因此，在特定基准上经过微调的模型的性能，即使名义上处于人为水平，也可能夸大了基础任务的实际性能。</p><p><strong>第三，人类不需要大型的监督数据集即可通过简短的自然语言指令学习大多数语言任务（例如“请告诉我这句话描述的是快乐还是悲伤”）或（例如“这里有两个举止勇敢的人的例子；请举第三个勇敢的例子”），这些足以使人们至少能够以合理的能力执行一项新任务。</strong>除了指出当前NLP技术的概念局限性之外，这种适应性还具有实际优势–它允许人类无缝地混合许多任务技能或在许多任务和技能之间切换，例如在冗长的对话中进行添加内容。<strong>为了广泛使用，我们希望有一天我们的NLP系统具有相同的流动性和通用性。</strong></p><p><u>(在这篇文章中我第一次知道，原来基于指令的prompt的发展最开始是从人类身上学习来的，人类可以通过指令学习nlp任务，所以语言模型是不是也能)</u></p><p>解决这些问题的一种潜在途径是<strong>元学习</strong>，在语言模型的上下文中，这意味着该模型在训练时会开发出广泛的技能和模式识别能力，然后在推理时使用这些能力快速适应或识别所需的能力任务。</p><p>最近的工作尝试通过所谓的<strong>“上下文学习”</strong>来完成此任务，使用预先训练的语言模型的文本输入作为任务说明的形式：<strong>该模型以自然语言指令的集合来说明该任务，然后仅通过预测下一步文本来完成该任务的更多实例。</strong>尽管它已显示出一些初步的进步，但此方法仍取得了远不及微调的结果。</p><p>近年来，transformer语言模型的容量已从1亿个参数增至170亿个参数。每次增加都带来了文本合成或下游NLP任务的改进，并且有证据表明，与许多下游任务密切相关的log loss遵循随着规模而改善的平稳趋势。由于上下文学习涉及吸收模型参数内的许多技能和任务，因此上下文学习能力可能在规模上显示出类似的优势。</p><p>在本文中，我们<strong>通过训练1750亿个参数自回归语言模型（称为GPT-3）并测量其在上下文中的学习能力来检验该假设</strong>。具体来说，我们评估了超过<strong>十二个NLP数据集的GPT-3</strong>，以及旨在测试快速适应不太可能直接包含在训练集中的任务的几个新颖任务。对于每项任务，我们<strong>在3种情况下评估GPT-3：（a）few-shot learning或上下文学习，其中允许尽可能多的例子适合模型的上下文窗口（通常为10到100），（b）one-shot learning，其中我们只允许一个例子；（c）zero-shot learning</strong>，<strong>其中不允许例子，并且仅向模型提供自然语言的说明</strong>。GPT-3原则上也可以在传统的微调环境中进行评估，但我们将其留待以后的工作。</p><p>下图说明了我们研究的内容，并显示了对简单任务的few-shot learning的结果，该简单任务要求模型从单词中删除多余的符号。通过添加自然语言任务描述以及模型上下文中的示例数K，可以提高模型性能。Few-shot learning性能也随着模型的大小而大大改善。尽管在这种情况下的结果特别引人注目，但对于我们研究的大多数任务，模型大小和上下文中示例数量的总体趋势仍然成立。我们强调，这些“学习”曲线涉及非梯度更新或微调，只是增加了作为条件的演示次数。</p><p><img src="https://img-blog.csdnimg.cn/6c7dd23c1a77496a88de1f22daa901aa.png" alt="在这里插入图片描述"></p><p>广义上讲，在NLP任务上，GPT-3在zero-slot learning和one-shot learning中取得了可喜的结果，在few-slot learning中，有时甚至可以与最先进的结果竞争。<strong>GPT-3在旨在测试快速适应性或即时推理的任务上也能显示one-shot learning和few-slot learning的熟练程度，其中包括解密单词，执行算术和使用看到的一个句子中仅仅被使用过一次的新词</strong> 。我们还展示了在few-slot learning下，GPT-3可以生成人工评估人员难以与人工生成的文章区分开的综合新闻文章，与此同时，我们还发现了<strong>一些短时性能难以克服的任务。这包括自然语言推理任务（例如ANLI数据集）和一些阅读理解数据集（例如RACE或QuAC）</strong>。通过展现GPT-3的优缺点的广泛特征，包括这些局限性，我们希望能够激发对语言模型的少量学习的研究，并提请人们关注最需要进步的地方。下图汇总了各种任务的性能（尽管它本身不应被视为严格或有意义的基准）。</p><p><img src="https://img-blog.csdnimg.cn/a5763029426645c99147f25ac4fc972d.png" alt="在这里插入图片描述"></p><p>我们还对<strong>“数据污染”</strong>进行了系统的研究-当训练包含诸如Common Crawl之类的数据集的高容量模型时，这是一个日益严重的问题，<strong>该数据集可能包含测试数据集中的内容，因为此类内容通常存在于网络中</strong>。（这样就无法保证是zero-shot的。）在本文中，我们开发了系统的工具来测量数据污染并量化其失真影响。尽管我们发现数据污染对大多数数据集对GPT-3的性能影响很小，但我们确实确定了一些可能夸大结果的数据集，并且我们不报告这些数据集的结果，或者根据结果标注星号。除了上述所有功能之外，我们还训练了一系列较小的模型（参数范围从1.25亿到130亿个参数），以便将其在zero-slot learning、one-shot learning和few-slot learning的性能与GPT-3进行比较。概括地说，<strong>对于大多数任务，我们发现在所有三个设置中，模型容量都相对平滑地缩放</strong>。一个值得注意的模式是，三种模型性能之间的差距通常随模型容量而增大，<strong>这可能表明较大的模型是更熟练的元学习者</strong>。最后，鉴于GPT-3展示的广泛功能，我们讨论有关偏见，公平和更广泛的社会影响的担忧，并尝试就此方面对GPT-3的特征进行初步分析。</p><p><strong>2、方法</strong></p><p>我们的基本预训练方法（包括模型，数据和训练）与gpt2中描述的过程相似，模型尺寸，数据集大小和多样性以及训练时间的扩展相对简单。我们在上下文学习中的使用也类似于gpt2，但是在这项工作中，<strong>我们系统地探索了在上下文中进行学习的不同设置</strong>。因此，我们从明确定义和对比将要评估GPT-3或原则上可以评估GPT-3的不同设置开始本节。这些设置可以看作取决于它们倾向于依赖多少特定于任务的数据。具体而言，我们可以至少识别四个点（有关说明，请参见图2.1）</p><p><img src="https://img-blog.csdnimg.cn/f222ad7033994d66b1fbf35b3c2d6976.png" alt="在这里插入图片描述"></p><p><strong>Fine-tuing（FT）</strong>是近年来最常用的方法，它涉及通过在特定于所需任务的监督数据集上进行训练来更新预训练模型的权重。通常使用成千上万的标记示例。微调的<strong>主要优点是在许多基准上均具有出色的性能。主要缺点是，每个任务都需要一个新的大型数据集，存在泛化分布不佳的潜在可能性，以及利用训练数据的虚假特征，这可能会导致与人类绩效的不公平比较。</strong>在这项工作中，我们不会微调GPT-3，因为我们<strong>专注于与任务无关的性能</strong>，但原则上可以微调GPT-3，这是未来工作的有希望的方向。</p><p><strong>Few-Shot（FS）</strong>是我们将在本工作中使用的术语，是指这样的设置，<strong>在该设置中，在推理时给模型一些任务演示作为条件，但不允许权重更新</strong>。如图2.1所示，典型数据集一个示例具有上下文和所需的完成度（例如，英语句子和法语翻译），并通过给出K个上下文和完成度的示例，再给出一个上下文的最终示例来演示少量例子，并期望模型完成。通常，将范围设置为10到100，因为这可以在模型的上下文窗口中容纳多少示例（nctx &#x3D; 2048）。<strong>Few-Shot的主要优点是大大减少了对特定于任务的数据的需求，并减少了从大型但狭窄的微调数据集中学习过窄分布的潜力。主要缺点是，迄今为止，此方法的结果比最新的微调模型差很多。而且，仍然需要少量的任务特定数据。</strong></p><p><strong>One-shot（1S）</strong>与Few-Shot相同，除了对任务的自然语言描述外，只允许进行一次演示，如图所示在上图中。区分这两种方法的原因是，它与某些任务传达给人类的方式最接近。例如，当要求人类在计算机上生成数据集时人工服务（例如MechanicalTurk），通常会演示一项任务。相反，如果没有给出示例，有时很难传达任务的内容或格式。</p><p><strong>Zero-shot（0S）</strong>与One-shot相同，不同之处<strong>在于不允许进行演示，并且仅向模型提供描述任务的自然语言指令</strong>。这种方法提供了最大的便利性，潜在的鲁棒性和避免了虚假的相关性（除非它们在大量的预训练数据中非常广泛地发生），但这也是最具挑战性的设置。在某些情况下，如果没有以前的示例，人甚至可能很难理解任务的格式，<strong>因此，在某些情况下，此设置“不公平”。</strong>例如，如果有人要求“制作200m的世界记录表”，此请求可能会很含糊，因为可能无法确切知道表格应采用的格式或应包含的格式（甚至在仔细澄清后，也很难准确地了解所需内容）。<strong>尽管如此，至少在某些设置上，zero-shot最接近人类执行任务的方式–</strong>例如，在图2.1中的翻译示例中，人类可能仅会从文本指令中知道要做什么。</p><p><strong>2.1 模型架构</strong></p><p>我们<strong>使用与GPT-2相同的模型和体系结构</strong>，包括其中描述的修改后的初始化，预规范化和可逆的分词，不同之处在于，我们在层的各层使用交替的密集和局部条带稀疏模式transformer。<strong>为了研究ML性能对模型大小的依赖性，我们训练了8种不同大小的模型</strong>，范围从1.25亿个参数到1,750亿个参数，超过三个数量级，最后一个模型称为GPT-3。先前的工作建议，在具有足够的训练数据的情况下，验证损失的缩放比例应近似为大小函数的平稳幂定律；多种大小的训练模型使我们能够针对验证损失和下游语言任务测试这一假设。表2.1列出了8种模型的大小和体系结构。这里参数是可训练参数的总数，层是层的总数，dmodel是每个瓶颈层的单位数（我们始终将前馈层设为瓶颈层大小的四倍，d &#x3D; 4 dmodel），而dhead是每个注意头。所有模型均使用nctx &#x3D; 2048个令牌的上下文窗口。我们将模型沿着深度和宽度维度跨GPU划分，以最大程度地减少节点之间的数据传输。根据计算效率和跨GPU的模型布局中的负载平衡来选择每个模型的精确架构参数。先前的工作表明，验证损失对这些参数在相当宽的范围内不是很敏感。</p><p><img src="https://img-blog.csdnimg.cn/7841e392388c4779a7af575e9bd1832a.png" alt="在这里插入图片描述"></p><p><strong>2.2 数据集</strong></p><p>语言模型的数据集已迅速扩展，最终达到了<strong>将近一万亿个单词的Common Crawl数据集</strong>。如此大的数据集足以训练我们最大的模型，而无需两次更新相同的序列。但是，我们发现，与经过精心挑选的数据集相比，未经过滤或经过轻微过滤的Common Crawl版本的质量往往较低。因此，<strong>我们采取了3个步骤来提高数据集的平均质量：（1）基于与一系列高质量参考语料库的相似性，下载并过滤了Common Crawl版本，（2）在文档级执行了重复数据删除，在数据集内和数据集之间，以防止冗余并保留我们保留的验证集的完整性，以作为过度拟合的精确度量;（3）我们还向训练组合中添加了已知的高质量参考语料库，以增强Common Crawl并增加其多样性。</strong>附录A中描述了前两点（Common Crawl的处理）。对于第三点，我们添加了多个精选的高质量数据集，包括WebText数据集的扩展版本，这些数据集是通过较长时间的爬网链接收集的时间，这是两种基于互联网的图书资料集（Books1和Books2）和英语Wikipedia。表2.2显示了我们在训练中使用的最终数据集。Common Crawl数据是从涵盖2016年至2019年的每月Common Crawl的41个分片中下载的，构成了过滤前的45TB压缩明文和过滤后的570GB，大致相当于4000亿字节对编码的词。请注意，在训练过程中，并非按大小对数据集进行采样，而是我们认为更高质量的数据集采样频率更高，因此Common Crawl和Books2数据集在训练过程中采样少于一次，而其他数据集则采样2-3次。这本质上是接受少量的过度拟合以换取更高质量的训练数据对于在大量互联网数据上进行预训练的语言模型，尤其是具有记忆大量内容的能力的大型模型，主要的方法论关注是通过在预训练期间无意中看到它们的测试或开发集，可能对下游任务造成污染。为了减少此类污染，我们进行了搜索并尝试消除与本文研究的所有基准测试的开发和测试集之间的任何重叠之处。不幸的是，筛选中的错误导致我们忽略了一些重叠之处，并且由于训练的代价是重新训练模型是不可行的。在第4节中，我们描述了剩余的重叠的影响，在以后的工作中，我们将更加积极地消除数据污染。</p><p><img src="https://pic1.zhimg.com/80/v2-c4fb78631baae0e94f87eb1dd49bf00c_1440w.jpg" alt="img"></p><p><strong>2.3 训练过程</strong></p><p>正如在[KMH+20, MKAT18]中发现的那样，较大的模型通常可以使用较大的批次规模，但需要较小的学习率。我们在训练过程中测量梯度噪声规模，并使用它来指导我们对批次大小的选择[MKAT18]。表2.1显示了我们使用的参数设置。</p><p><img src="https://img-blog.csdnimg.cn/7841e392388c4779a7af575e9bd1832a.png" alt="在这里插入图片描述"></p><p>为了在不out-of-memory的情况下训练更大的模型，<strong>我们在每个矩阵乘法内使用模型并行，并在网络的各层使用模型并行</strong>。所有模型都是在微软提供的高带宽集群的一部分的<strong>V100 GPU</strong>上训练的。训练过程和超参数设置的细节在附录B中描述。</p><p><strong>2.4评估</strong></p><p>对于few-shot learning，我们<strong>通过从任务训练集中随机抽取K个示例作为条件来评估评估集中的每个示例，根据任务以1或2个换行符分隔</strong>。对于LAMBADA和Storycloze，没有可用的监督训练集，因此我们从开发集中提取条件示例并评估测试集。对于Winograd（原始版本，不是SuperGLUE版本），只有一个数据集，因此我们直接从中绘制条件示例，K可以是0到模型上下文窗口允许的最大数量之间的任何值，对于所有模型而言，ctct &#x3D; 2048，通常适合10至100个示例。较大的K值通常但并非总是更好，因此，可以使用单独的开发和测试集时，我们在开发集上尝试一些K值，然后在测试集上运行最佳值。对于某些任务，除了演示之外，我们还使用自然语言提示。</p><p>在涉及从多个选项中选择一个正确补全（多项选择）的任务中，我们提供k个能够正确补全的例子，然后跟着问题文本，一一比较不同的补全方式的语言模型可能性。在涉及二进制分类的任务中，我们提供更多选项具有语义意义的名称（例如“ True”或“ False”，而不是0或1），然后将任务视为多项选择；有关具有自由形式完成的任务，我们使用与gpt2相同的参数进行波束搜索：波束宽度4的长度惩罚&#x3D; 0：6。<strong>我们根据手头数据集的标准使用F1相似性评分，BLEU或完全匹配对模型进行评分。</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文集思</title>
    <link href="/2022/07/15/%E8%AE%BA%E6%96%87%E9%9B%86%E6%80%9D/"/>
    <url>/2022/07/15/%E8%AE%BA%E6%96%87%E9%9B%86%E6%80%9D/</url>
    
    <content type="html"><![CDATA[<p>idea：</p><p>《How Can We Know What Language Models Know》(LPAQA)</p><p><strong>在写指令式的prompt的时候可以通过实验找到最适合不同的下游任务的句式，多样化的指令式的prompt也可以引入学习权重，而不是简单的进行集合。</strong></p><p>《GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models》</p><p><strong>related work</strong>很可以借鉴</p><p><strong>第七个实验对我很有启发：</strong>how instructions are internally utilize by models remains largely unknown and merits further study. <strong>指令是如何被模型内部所利用的</strong> 非常值得去探讨，很多混乱的或者是不相关的指令往往有时候也会与人类所认为的好的指令起到一样的提升模型性能的效果。其实一直对我来说，在做给数据集增加离散的指令的事情，但这个领域一直像一个黑盒一样，我并不知道为什么添加了指令可以给模型带来性能的提高，或者说指令对模型来说是否真的像对人类一样是帮助理解任务的，在内部是如何被利用的，非常值得去揭秘</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《How Can We Know What Language Models Know?》论文阅读笔记</title>
    <link href="/2022/07/14/%E3%80%8AHow-Can-We-Know-What-Language-Models-Know-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/14/%E3%80%8AHow-Can-We-Know-What-Language-Models-Know-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>paper： <a href="https://arxiv.org/abs/1911.12543">https://arxiv.org/abs/1911.12543</a></p><p>code： <a href="https://github.com/jzbjyb/LPAQA">GitHub - jzbjyb&#x2F;LPAQA: Language model Prompt And Query Archive</a>  </p><p><img src="https://img-blog.csdnimg.cn/0a9ec5ea0a51434fb3c7bce9a3d3f9a0.png" alt="在这里插入图片描述"></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>最近的工作提出了耐人寻味的结果，通过让语言模型（LM）填写 “Obama is a by profession”这样的提示信息，来研究语言模型（LM）中包含的知识。这些提示通常是人工创建的，而且很可能是次优的；另一个提示如 “Obama worked as a”可能会导致更准确地预测正确的职业。正因为如此，给定一个不恰当的提示，我们可能无法检索到语言模型确实知道的事实，因此任何给定的提示只能提供语言模型所含知识的下限估计。在本文中，我们试图通过自动发现更好的提示来更准确地估计LM中所包含的知识，以便在这个查询过程中使用。具体来说。论文<strong>提出了基于挖掘（mining-based）和基于释义（paraphrasing-based）的方法来自动生成高质量和多样化的提示，以及集成方法来组合来自不同提示的答案</strong>，用以更准确地估计 LM 中包含的知识，主要使用的数据集是LAMA。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，语言模型（LM）的主要作用从<strong>生成或评估自然文本的流畅性</strong>过渡到<strong>成为文本理解的有力工具</strong>。这种理解主要是通过使用语言修改作为特征提取器的预训练任务来实现的。其中，通过语言建模目标学到的隐藏向量随后被用于下游语言理解系统。有趣的是，现在也越来越明显地发现 <strong>LM本身可以作为文本理解的工具</strong>，通过用自然语言制定查询，并直接生成文本答案，或者评估多个选择并挑选出最有可能的一个。例如，LM被用来回答事实性问题，回答常识性查询，或提取关于实体之间关系的事实性知识。无论最终的任务是什么，LM中包含的知识都是通过提供提示，让LM生成前缀的延续（例如 “巴拉克-奥巴马出生于”），或者预测cloze-style模板中的缺失单词（例如 “巴拉克-奥巴马是一个职业”）来探究。</p><p>然而，虽然这种范式已经被用来实现一些关于LMs所表达的知识的有趣的结果，但它们通常依赖于基于实验者的直觉而手工创建的提示语。<strong>这些手动创建的提示（如 “巴拉克-奥巴马出生在”）可能是次优的</strong>，因为中枢神经系统可能已经在他们的训练中从大大不同的语境（如 “巴拉克-奥巴马的出生地是夏威夷的檀香山”）中学习了目标知识。因此，很有可能由于提示不是对事实的有效查询，LM所知道的事实不能被检索出来。因此，现有的结果只是对LM所包含的知识程度的一个下限，事实上，LM的知识可能比这些初步结果所显示的还要丰富。<strong>在本文中，我们提出了一个问题。”我们如何才能收紧这个下限，并对最先进的LM所包含的知识有一个更准确的估计？”</strong> 这在科学上是很有趣的，因为它是对LM所包含的知识的探测，从工程的角度来看，当使用LM作为知识提取系统的一部分时，它将导致更高的召回率。</p><p>特别是，我们专注于Petroni等人（2019）的设定（LAMA），他们研究提取有关实体之间关系的知识。我们提出了两种自动方法来系统地改善用于查询关系存在的提示的广度和质量。<strong>这些方法是基于挖掘的方法</strong>，其灵感来自以前的关系提取方法，<strong>以及基于转述的方法，该方法采用一个种子提示（无论是手动创建的还是自动挖掘的）</strong>，<strong>并将其转述为其他几个语义相似的表达</strong>。此外，由于在查询不同的主客体对时，不同的提示语可能效果更好，<strong>我们还研究了轻量级的集合方法，将不同提示语的答案组合在一起</strong>。</p><p>我们在LAMA基准上进行了实验，这是一个英语基准，用来测试LM检索实体之间关系的能力。我们首先证明，改进后的提示明显提高了这项任务的准确性，我们的方法提取的最佳提示在BERT-base上将准确性从31.1%提高到34.1%，在BERT-large上也获得了类似的收益。我们进一步证明，通过合奏使用多样化的提示，进一步提高了准确性，达到39.6%。我们进行了广泛的分析和消减，<strong>既收集了关于如何最好地查询存储在LM中的知识的见解，也收集了关于将知识纳入LM本身的潜在方向的见解</strong>。最后，我们发布了由此产生的LM提示和查询档案（LPAQA），以促进未来对LM中包含的知识进行探测的实验。</p><h2 id="Prompt-Generation"><a href="#Prompt-Generation" class="headerlink" title="Prompt Generation"></a>Prompt Generation</h2><p>论文为每种实体关系考虑了两种提示模板生成方法：基于挖掘的方法和基于释义的方法。</p><p>1）<strong>基于挖掘的方法</strong></p><p>​    首先使用远程监督，从维基百科中提取出包含LAMA数据集主客体并描述了它们之间的关系的句子。</p><p><strong>Middle-word Prompts</strong> 以往的研究表明，主客中间的词往往表示关系，因此可以直接用这些词作为提示，例如，通过用占位符替换主语和宾语，“Barack Obama was born in Hawaii” 可以被转换为提示：“x was born in y”。</p><p><strong>Dependency-based Prompts</strong> 论文还指出，主客体之间没有出现文字时（比如“The capital of France is Paris”），可以使用依存解析器解析句子，找出主客间最短依存路径作为提示，比如上述例子的最短依存路径如下图所示， 可以转换为提示：“capital of x is y”。</p><p><img src="https://img-blog.csdnimg.cn/83dee73ae6c04900867b8c50b57ea199.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5YyX5Zyo5ZOq,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img"></p><p>2）<strong>基于释义的方法</strong></p><p>​    将原有的提示改为其他语义相似或相同的表达来实现，比如说，如果原始提示是“x shares a border with y”，那可以改写为“x has a common border with y”和“x adjoins y”。这在概念上类似于信息检索中使用的查询扩展技术，后者重新制定给定的查询以提高检索性能。</p><p>​    <strong>论文使用回译的方法来实现释义</strong>，即先将原始提示翻译成其他语言的个候选，然后对于每一个候选再将其翻译回英语，这样就可以得到个提示，最后根据round-trip概率（如下）进行排序选最优的top T个。</p><p><img src="https://img-blog.csdnimg.cn/535b998eec834db69f6d55f083240db6.png" alt="img"></p><h2 id="Prompt-Selection-and-Ensembling"><a href="#Prompt-Selection-and-Ensembling" class="headerlink" title="Prompt Selection and Ensembling"></a>Prompt Selection and Ensembling</h2><p>1）Top-1 Prompt Selection</p><p>  对于每一种关系，分别使用候选提示对训练集预测，只选择使得训练集准确率最高的一个提示作为最终提示。</p><p>2）Rank-based Ensemble</p><p>​    首先，对于每一种关系，根据训练集准确率对候选提示进行排序，选择前个提示与输入拼接进行预测，简单将所有提示的输出概率取平均，作为最终输出概率，然后再softmax。</p><p><img src="https://img-blog.csdnimg.cn/96f2c661b26e495ca948d77b9048f082.png" alt="在这里插入图片描述"></p><p>3）Optimized Ensemble</p><p>​    前面所有的方法都是将top K个prompts平等对待，直接进行加权平均，在第三种方法中引入了学习权重，对于每一种关系，引入可学习权重，最终输出概率为top 个提示输出概率的加权和。</p><p><img src="https://img-blog.csdnimg.cn/056a360380614669b92d7d8d7c7bdc4c.png" alt="在这里插入图片描述"></p><h2 id="Main-Experiment"><a href="#Main-Experiment" class="headerlink" title="Main Experiment"></a>Main Experiment</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p><strong>Dataset：</strong>T-REx subset of LAMA；T-REx-UHN；T-REx-train</p><p><img src="https://img-blog.csdnimg.cn/c5d16dd0d8814ff0a38229956fba94dc.png" alt="在这里插入图片描述"></p><p><strong>Model：</strong>BERT-base；BERT-large；ERNIE；KnowBert</p><p><strong>Evaluation Metrics：</strong>micro-averaged accuraccy；macro-averaged accuracy</p><p><strong>Methods：</strong>Mine+Man vs Mine+Para vs Man+Para</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>所有的实验结果都是围绕以下两张表的结果展开的：</p><p><img src="https://img-blog.csdnimg.cn/8fde89fb1abf4780a4a447d01837c6be.png" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/070251def5334d3dbd41dccad0f26eb9.png" alt="在这里插入图片描述"></p><p>将基于挖掘的方法生成的提示与手动设计的提示使用可学习权重进行集成，在BERT-base和BERT-large上都取得了最优结果。</p><h4 id="1-Single-Prompt-Experiment"><a href="#1-Single-Prompt-Experiment" class="headerlink" title="1) Single Prompt Experiment"></a>1) Single Prompt Experiment</h4><p>当只使用一个提示时（在两个表中的Top1列中），所提出的提示生成方法中最好的方法在BERT-base上将微观平均准确率从31.1%提高到34.1%，在BERT-large上从32.3%提高到39.4%。这表明手动创建的提示是一个有点弱的下限；还有其他提示可以进一步提高从LM查询知识的能力。<br>表4显示了一些挖掘出来的提示语，与人工提示语相比，这些提示语带来了很大的性能提升。</p><p><img src="https://img-blog.csdnimg.cn/87bb968051f04492bec9ff3d1f94643e.png" alt="在这里插入图片描述"></p><p>对于宗教关系，”x who converted to y”比人工定义的提示 “x is affiliated with the y religion”提高了60.0%；对于关系subclass_of，”x is a type of y”比 “x is a subclass of y”提高了22.7%的准确性。可以看出，<strong>使用挖掘出来的提示语的最大收益似乎发生在人工定义的提示语在句法上更复杂的情况下（例如前者），或者使用比挖掘出来的提示语更不常见的措辞时（例如后者）。</strong></p><h4 id="2-Prompt-Ensembling（Prompt集成）"><a href="#2-Prompt-Ensembling（Prompt集成）" class="headerlink" title="2) Prompt Ensembling（Prompt集成）"></a>2) Prompt Ensembling（Prompt集成）</h4><p>然后将第1栏中的单一提示结果与下面三栏中的组合结果进行比较，我们可以看到，组合多个提示几乎总是能带来更好的性能。在不同的提示生成方法中，<strong>Top3和Top5中使用的简单平均值优于Top1</strong>。优化后的合集在BERT-base和BERT-large上进一步将微观平均准确率分别提高到38.9%和43.7%，比基于等级的合集要好得多。这两组结果表明，<strong>不同的提示可以以不同的方式查询LM，而基于优化的方法能够找到有效地将不同提示结合在一起的权重</strong>。</p><p>我们在表5中列出了所学到的前3名提示的权重，以及与只使用前1名提示相比的准确度提升。</p><p><img src="https://img-blog.csdnimg.cn/6600edd7bf0041c796595bbcd22ec921.png" alt="在这里插入图片描述"></p><p>权重倾向于集中在一个特定的提示上，而其他提示则作为补充。我们还在图2中描述了基于等级的合集方法的表现，与提示的数量有关。</p><p><img src="https://img-blog.csdnimg.cn/62f40ebe8c1547239e0b2149585b0d26.png" alt="在这里插入图片描述"></p><p>对于挖掘出来的提示语，前2名或前3名通常会给我们最好的结果，而对于转述的提示语，前5名是最好的。<strong>纳入更多的提示语并不总是能提高准确性，这一发现与基于优化的方法所学到的权重迅速下降相一致。</strong>Oracle和Opti.之间的差距表明，使用更好的集合方法仍有改进空间。</p><h4 id="3-Mining-vs-Paraphrasing"><a href="#3-Mining-vs-Paraphrasing" class="headerlink" title="3) Mining vs. Paraphrasing"></a>3) Mining vs. Paraphrasing</h4><p>对于rank-based的合集（Top1, 3, 5），通过转述产生的prompt通常比挖掘的提示语表现得更好，而对于基于优化的合集（Opti.），挖掘的提示语表现得更好。我们推测这是因为与意译相比，挖掘出来的提示语表现出更多的变化，而适当的加权是至关重要的。这种变化的差异可以从每一类提示语之间的平均编辑距离中观察到，挖掘的提示语和意译的提示语的编辑距离分别为3.27和2.73。然而，与仅仅使用一个提示语相比，集合释义所带来的改进仍然是显著的（Top1 vs. Opti.），在BERT-base上的微观平均准确率从32.7%提高到36.2%，在BERT- large上从37.8%提高到40.1%。<strong>这表明，即使对prompt进行小的修改，也会导致预测的相对较大的变化。</strong></p><p><img src="https://img-blog.csdnimg.cn/846350ee7c1444bdb915056c975e0c2a.png" alt="在这里插入图片描述"></p><p>表6显示了修改一个词（无论是功能词还是内容词）就能显著提高准确率的情况，表明大规模的LM对查询方式的微小变化仍有一定的影响。</p><h4 id="4-Middle-word-vs-Dependency-based"><a href="#4-Middle-word-vs-Dependency-based" class="headerlink" title="4) Middle-word vs. Dependency-based"></a>4) Middle-word vs. Dependency-based</h4><p><img src="https://img-blog.csdnimg.cn/92a39f54f9c14c708a791e6a4c6c0218.png" alt="在这里插入图片描述"></p><p>作者在表7中比较了仅使用中间词提示和将其与基于依赖关系的提示相连接的性能。这些改进证实了我们的直觉，即<strong>属于依存关系路径但不在主语和宾语中间的词也是关系的指示</strong>。</p><h4 id="5-Micro-vs-Macro"><a href="#5-Micro-vs-Macro" class="headerlink" title="5) Micro vs. Macro"></a>5) Micro vs. Macro</h4><p>对比表2和表3，我们可以看到，macro-averaged accuracy 比micro-averaged accuracy低得多，这表明<strong>macro-averaged accuracy是一个更具挑战性的指标</strong>，它评估了LM知道多少独特的对象。我们基于优化的方法在BERT基础上将macro-averaged accuracy从22.8%提高到25.7%，在BERT基础上从25.7%提高到30.1%。这再次证实了集合多个提示的有效性，但收益稍小。值得注意的是，在我们基于优化的方法中，合集权重是在训练集的每个例子上进行优化的，这更有利于优化micro-averaged accuracy。优化以提高macro-averaged accuracy是未来工作的一个有趣的方向，可能会导致提示更普遍地适用于不同类型的obeject。</p><h4 id="6-Performance-of-Different-LMs"><a href="#6-Performance-of-Different-LMs" class="headerlink" title="6) Performance of Different LMs"></a>6) Performance of Different LMs</h4><p><img src="https://img-blog.csdnimg.cn/234f002631b447d494ddc0bf50ce4977.png" alt="在这里插入图片描述"></p><p>在表8中，作者还将不同的模型：BERT与ERNIE和KnowBert进行了比较，后者通过更加明确地纳入实体嵌入来增强外部知识。ERNIE即使在手动定义提示的情况下也比BERT高出1分，但我们的提示生成方法进一步强调了两种方法之间的差异，使用Mine+Man方法的最高准确率数字相差了4.2分。<strong>这表明，如果对LM进行有效查询，高性能模型之间的差异可能会变得更加明显。</strong>KnowBert在LAMA上的表现不如BERT，这与Peters等人（2019）的观察相反。这可能是因为在Peters等人（2019）中，KnowBert被用来评估多标记主体&#x2F;对象，而LAMA只包含单标记对象。</p><h4 id="7-LAMA-UHN-Evaluation"><a href="#7-LAMA-UHN-Evaluation" class="headerlink" title="7) LAMA-UHN Evaluation"></a>7) LAMA-UHN Evaluation</h4><p><img src="https://img-blog.csdnimg.cn/cf53960052554e0286563522f28ce1ca.png" alt="在这里插入图片描述"></p><p>表9报告了在LAMA-UHN基准上的表现。尽管与原始LAMA基准测试（表2）的表现相比，总体表现急剧下降，但优化后的组合仍能在很大程度上胜过人工提示，<strong>表明我们的方法在检索不能根据表面形式推断的知识方面是有效的</strong>。</p><h4 id="8-Performance-on-Google-RE"><a href="#8-Performance-on-Google-RE" class="headerlink" title="8) Performance on Google-RE"></a>8) Performance on Google-RE</h4><p><img src="https://img-blog.csdnimg.cn/005abb5350c34c27a4d96eb49490fead.png" alt="在这里插入图片描述"></p><p>作者还在表10中报告了优化后的组合在Google- RE子集上的表现。同样，合集的多样化的提示提高了BERT-base和BERT-large模型的准确性。与T-REx子集相比，收益略小，这可能是由于只有3个关系，其中一个关系（预测一个人的出生日期）特别难，以至于只有一个提示产生非零的准确性。</p><h3 id="实验分析"><a href="#实验分析" class="headerlink" title="实验分析"></a>实验分析</h3><h4 id="1）不同提示的预测一致性分析"><a href="#1）不同提示的预测一致性分析" class="headerlink" title="1）不同提示的预测一致性分析"></a>1）不同提示的预测一致性分析</h4><p>使用以下公式计算同一关系下，不同提示产生的预测之间的发散度：</p><p><img src="https://img-blog.csdnimg.cn/eed43790e1c4443fbf18b1e6de3a5420.png" alt="在这里插入图片描述"></p><p>若提示能够引导模型预测出正确结果，则，否则为0。以两个提示之间的编辑距离为横轴，预测发散度为纵轴，绘制箱型图如下：</p><p><img src="https://img-blog.csdnimg.cn/bd6703c4935c45cd90cf95f0013e0ed4.png" alt="在这里插入图片描述"></p><p>随着编辑距离变大，发散度增加，这证实了我们的直觉，<strong>即非常不同的提示往往会导致不同的预测结果</strong>。Pearson 相关系数为 0.25，说明这两个量之间存在弱相关。</p><h4 id="2）基于词性的提示有效性分析"><a href="#2）基于词性的提示有效性分析" class="headerlink" title="2）基于词性的提示有效性分析"></a>2）基于词性的提示有效性分析</h4><p>符合以下三种句法规则的提示比其他提示的平均排名要高：</p><p><img src="https://img-blog.csdnimg.cn/8f50381e24454a7a92d2d97656b72d26.png" alt="在这里插入图片描述"></p><p>我觉得这个实验分析的结果对之后的工作还是非常有启发性的，对于后续更多想要开发不同pattern的prompt具有很大的借鉴意义。</p><h4 id="3）跨模型一致性"><a href="#3）跨模型一致性" class="headerlink" title="3）跨模型一致性"></a>3）跨模型一致性</h4><p>最后，我们有兴趣知道，我们所提取的提示是否是针对某一特定模型的，或者<strong>它们是否可以在不同的模型中通用</strong>。为此，我们使用了两种设置：一种是比较BERT-base和BERT-large，即具有不同规模的相同模型架构；另一种是比较BERT-base和ERNIE，即具有相同规模的不同模型架构。在每一种情况下，我们都会比较基于优化的合集是<strong>在同一模型上训练，并在另一个模型上进行测试</strong>。如表12和表13所示，我们发现，一般来说，在跨模型的情况下，性能通常会有一些下降（第三和第五列），但损失往往很小，在查询BERT-base时，最高的性能实际上是由在BERT-large上优化的权重实现的。值得注意的是，在另一个模型上优化的权重的最佳准确率为40.1%和42.2%（表12）和39.5%和40.5%（表13），仍然比手动提示获得的准确率高得多，这表明优化的提示仍然能在不同的模型上提供大的收益。另一个有趣的观察是，<strong>在ERNIE上的性能下降（表13的最后两列）比在BERT-base上使用优化权重的BERT-large（表12的最后两列）要大，表明共享相同结构的模型从相同的提示中受益更多</strong>。</p><h4 id="4）线性与对数线性的结合"><a href="#4）线性与对数线性的结合" class="headerlink" title="4）线性与对数线性的结合"></a>4）线性与对数线性的结合</h4><p><img src="https://img-blog.csdnimg.cn/d069eb0cfdf347aa97510234a84b9ab9.png" alt="在这里插入图片描述"></p><p>我们从上图可以看到，对数线性组合胜过线性组合，因为对数概率使我们有可能惩罚那些在任何特定提示下都是非常不可能的对象。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这篇文章中，作者研究了用于从语言模型中检索事实知识的提示语的重要性。<strong>作者提出了mining-based和paraphrasing-based的方法</strong>，以系统地生成不同的提示语来查询特定的关系知识片段。<strong>这些提示语结合在一起时，将事实知识的检索准确率提高了8%，比人工设计的提示语要好得多</strong>。我们的分析表明，LMs确实比以前的结果所显示的更有知识，但它们对我们如何查询它们也相当敏感。<strong>这表明了未来的潜在方向，例如：（1）可以用不同的方式查询但仍然返回类似的结果的更强大的LM，（2）LM中的事实知识的方法，以及（3）在优化LM的知识查询方法方面的进一步改进。</strong></p><h2 id="My-idea"><a href="#My-idea" class="headerlink" title="My idea"></a>My idea</h2><p>这篇文章对我比较有启发意义的是其实它的最主要的实验做得比较简单，但是实验结果的多维度分析以及关于很多因素的讨论是我十分需要学习的，在上一次写paper去完善自己的实验的时候，很多时候是不知道应该去做一些什么样的实验的，这里首先给了我一个想法：<strong>在写指令式的prompt的时候可以通过实验找到最适合不同的下游任务的句式，多样化的指令式的prompt也可以引入学习权重，而不是简单的进行集合。</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《Language Models as Knowledge Bases?》论文阅读笔记</title>
    <link href="/2022/07/13/%E3%80%8ALanguage-Models-as-Knowledge-Bases-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/13/%E3%80%8ALanguage-Models-as-Knowledge-Bases-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>paper：<a href="https://arxiv.org/abs/1909.01066">https://arxiv.org/abs/1909.01066</a></p><p>code：<a href="https://github.com/facebookresearch/LAMA">https://github.com/facebookresearch/LAMA</a></p><p>来源：EMNLP 2019</p><p>作者单位：Facebook AI 研究院，伦敦大学学院</p><p><img src="https://pic2.zhimg.com/v2-6cfa5024e7ca097a7b43ee6e3008a4a0_1440w.jpg?source=172ae18b" alt="【EMNLP 2019】Language Models as Knowledge Bases?"></p><p><strong>这项研究所揭示的重要结论</strong>：</p><ul><li>BERT-large 模型捕获了（准确的）关系知识，该知识与使用现成的关系提取器和基于oracle 的实体链接器从已知表示相关知识的语料库中提取的知识库相当。</li><li>BERT-large 在开放域 QA 中取得了显着的结果，与使用任务特定的监督关系提取系统构建的知识库取得的 63.5% precision@10 相比，它取得了 57.1 % 的效果。</li><li>BERT-large 在恢复事实和常识性知识方面始终优于其他语言模型。</li><li>事实知识可以从预训练语言模型中意外地很好地恢复，但是，对于某些关系（特别是 N 对M 关系）的性能非常差。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>语言模型用于预测句子中的下一个单词，或者给定句子预测其中任何位置的被掩盖的单词。预训练模型需要存储大量的对下游任务有用的语言学知识。通常通过以原始模型产生的潜在上下文表示为条件，或通过使用原始模型权重来初始化特定于任务的模型，然后进一步进行微调，来访问此知识。此类知识转移对于当前在各种任务上的最新成果至关重要。</p><p>与此对比，知识库是通过使用查询来访问带注释的有严格标准关系的数据的有效方案。 但是，在实践中，我们经常需要<strong>从文本或其他方式中提取关系数据以填充这些知识库</strong>。 这需要复杂的 NLP 流水线，包括实体提取，共指解析，实体链接和关系提取，这些组件通常需要监督数据和固定模式。 而且，错误很容易在整个流水线中传播和累积。相反，我们可以尝试通过要求以关系数据的形式查询神经语言模型，例如“ Dante出生于[Mask]”。在这种情况下，<strong>语言模型具有各种吸引人的属性：它们不需要架构工程，不需要人工注释，并且支持一组开放的查询</strong>。</p><p><img src="https://img-blog.csdnimg.cn/ca85281766ab45e080354686663bc17b.png" alt="在这里插入图片描述"></p><p>​                                                在知识库和语言模型中查询事实类知识</p><p>鉴于语言模型的上述特性可以作为关系知识的潜在表示形式，作者表示对预先训练的现成语言模型（例如 ELMo 和 BERT）中已经存在的关系知识感兴趣。 他们存储多少关系知识？ 对于不同类型的知识（例如有关实体的事实、常识和一般性问答），这有何不同？ 与从文本中自动提取的符号知识库相比，无需微调的性能如何？ 除了收集对这些模型的更好的一般理解之外，我们认为这些问题的答案可以帮助我们设计更好的无监督知识表示，这些知识表示可以将事实知识和常识性知识可靠地转移到下游任务，例如常识（视觉）问题解答或增强学习。</p><p>为了解答上述问题，该文介绍了 LAMA（LAnguage Model Analysis），<strong>包含一系列知识源，每个知识源包含一组事实。作者定义，一个语言模型知道一个事实（主体，关系，客体）当它在完形填空任务中能成功预测被掩盖的客体时。</strong></p><p>作者测试各种类型的知识：存储在 Wikidata 中的实体之间的关系，ConceptNet 概念之间的常识关系以及回答 SQuAD 中自然语言问题所必需的知识。 在后一种情况下，作者手动映射SQuAD 问题的子集以结束句子。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>在背景介绍了几种语言模型：</p><p><img src="https://img-blog.csdnimg.cn/fbc3c87c337c4def9bae9d02ce8ff6d5.png" alt="在这里插入图片描述"></p><h3 id="1-Unidirectional-language-models"><a href="#1-Unidirectional-language-models" class="headerlink" title="1.Unidirectional language models"></a>1.Unidirectional language models</h3><p>给定一个输入序列 w &#x3D; [w1, w2, … , wN] ，单向语言模型会将整个输入序列按下面的方式进行分解p (w) :</p><p><img src="https://img-blog.csdnimg.cn/b868f60f4a754574b67433c5c68c2ee0.png" alt="在这里插入图片描述"></p><p>一个比较通用的方法是通过神经网络来估计概率<br><img src="https://img-blog.csdnimg.cn/e859bf750ecf43c19efe779f363807fd.png" alt="在这里插入图片描述"><br>其中，ht ∈ R^k 是神经网络在位置 t 的输出向量，W∈R^(∣V∣×k)是一个可学习参数，用于将ht映射为词表 V 中每个词的非标准化分数。获得 ht 的神经网络结构可能有所不同，比如有多层感知器，卷积层，循环神经网络以及自注意力机制。</p><p>典型的单向语言模型为<strong>fairseq-fconv</strong>和<strong>Transformer-XL。</strong>。</p><h3 id="2-Bidirectional-language-models"><a href="#2-Bidirectional-language-models" class="headerlink" title="2.Bidirectional language models"></a>2.Bidirectional language models</h3><p>单向语言模型通过上文词语来预测下一个词。但是，一个词的含义是同时由上下文决定的。因此，给定输入序列 w &#x3D; [w1, w2, … , wN]和一个位置 1 ≤ i ≤ N，那么双向语言模型期望估计概率p (wi∣w1, … , wi−1, wi+1, … , wN) 会用这个词的上下文。</p><p>典型的单向语言模型为<strong>ELMO</strong>和<strong>BERT。</strong></p><h2 id="LAMA-探针"><a href="#LAMA-探针" class="headerlink" title="LAMA 探针"></a>LAMA 探针</h2><p>论文引入的LAMA探针可以用来测试语言模型中的事实和常识知识。该探针本质上提供了一组由<strong>事实</strong>组成的知识源。这里的事实是指subject-relation-object三元组或者问答对。每个事实都会被转换为“完型填空”形式的陈述句(prompt)，然后用来从语言模型中查询目标token。举例来说，给定一个事实(dante, born-in, florence)，如果要查询是否包含该知识，可以将其转换为陈述句Dante was born-in ___。</p><p> 在评估效果时，会根据真实token在候选词表中的位置进行评估，排名越靠前，则认为模型包含越多的知识。</p><h3 id="1-知识源"><a href="#1-知识源" class="headerlink" title="1. 知识源"></a>1. 知识源</h3><p>为了评估分析在前面介绍语言模型的时候不同的语言模型，在这里作者介绍包含了很多源的事实和常识知识。</p><h4 id="1-1-Google-RE"><a href="#1-1-Google-RE" class="headerlink" title="1.1 Google-RE"></a>1.1 Google-RE</h4><p> 语料Google-RE是从wikipedia中人工抽取的、包含60K事实的知识源，其覆盖了5种关系。但LAMA探针仅考虑其中的三种：place of birth、date of birth和place of death。排除另外两种的原因是，在评估中不支持多token对象。对于三元组事实中的每个关系，都会定义一个模板，例如：”[S] was born in [O]“为关系place of birth的模板。</p><h4 id="1-2-T-REx"><a href="#1-2-T-REx" class="headerlink" title="1.2 T-REx"></a>1.2 T-REx</h4><p> 知识源T-REx是wikipedia三元组的子集，其要比Google-RE大得多，且拥有更加广泛的关系集合。LAMA中考虑了41个wikipedia中的关系，并且每种关系采样1000个事实。同Google-RE数据集一样，我们人工为每个关系定义了模板(prompt)。</p><h4 id="1-3-ConceptNet"><a href="#1-3-ConceptNet" class="headerlink" title="1.3 ConceptNet"></a>1.3 ConceptNet</h4><p> ConceptNet是一个多语言知识库，该知识库是从Open Mind Common Sense(OMCS)中的句子构造出来的。LAMA中仅考虑ConceptNet中英语部分的事实，其中有16种关系具有单个token的ojbect。对于任意ConceptNet三元组，可以从OMCS中找到同时包含subject和object的句子。对该句子中的object进行mask，从而构成一个prompt。若三元组对应多个句子，则随机挑选一个。</p><h4 id="1-4-SQuAD"><a href="#1-4-SQuAD" class="headerlink" title="1.4 SQuAD"></a>1.4 SQuAD</h4><p> SQuAD是一个常见的问答数据集，LAMA从SQuAD的开发集中挑选了305个具有单token答案且上下文不敏感的问题。人工从这些问题中创建完型填空风格的问题。例如，将”Who developed the theory of relativity?“重写为”The theory of relativity was developed by ___”。</p><h3 id="2-模型"><a href="#2-模型" class="headerlink" title="2. 模型"></a>2. 模型</h3><p> 论文中测试的语言模型有：fairseq-fconv(Fs)、Transformer-XL large(Txl)、ELMo original(Eb)、ELMo 5.5B(E5B)、BERT-base(Bb)和BERT-large(Bl)。</p><p> 模型的目标是预测特定位置t处的token。对于单向语言模型，使用t-1处网络生成的向量      （ <strong>h</strong>t−1）进行预测。对于ELMo，则会使用前向的（ <strong>h</strong>t−1）和后向的（ <strong>h</strong>t+1）。对于BERT，则遮盖t处的token，然后将（ <strong>h</strong>t）输入softmax层。为了公正的比较，生成一个所有模型词表的交集，然后在该交集词表上预测token。</p><h3 id="3-基线"><a href="#3-基线" class="headerlink" title="3. 基线"></a>3. 基线</h3><p> 为了比较语言模型与传统系统，论文考虑了下面的baseline。</p><h4 id="3-1-Freq"><a href="#3-1-Freq" class="headerlink" title="3.1 Freq"></a>3.1 Freq</h4><p> 给定一个subject和relation关系对，该baseline会基于<strong>测试集</strong>中该关系对中出现的所有object的频率进行单词排序。该baseline是那些总是预测相同object的模型的上边界。</p><h4 id="3-2-RE"><a href="#3-2-RE" class="headerlink" title="3.2 RE"></a>3.2 RE</h4><p> 对于基于关系的知识源，使用一个预训练好的关系抽取模型RE，该模型在Wikidata上进行训练。该模型是基于LSTM和注意力机制的编码器，用于从句子中抽取三元组。RE对包含事实的句子进行三元组抽取，并构建知识图谱。在测试时，在图谱上查询指定的subject，然后基于RE返回的置信分数来排序object。</p><h4 id="3-3-DrQA"><a href="#3-3-DrQA" class="headerlink" title="3.3 DrQA"></a>3.3 DrQA</h4><p> DrQA是一个开放域问答系统，其使用两阶段的pipeline来回答自然语言问题。首先，使用TF-IDF从大量文档中检索出相关的文章，然后在检索出的topK的文章中，使用神经阅读理解模型来抽取答案。这里会显著DrQA只预测单个token，从而可以与语言模型进行比较。</p><h3 id="4-评估指标"><a href="#4-评估指标" class="headerlink" title="4. 评估指标"></a>4. 评估指标</h3><p> 使用基于rank的评估指标。</p><h3 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5. 注意事项"></a>5. 注意事项</h3><h4 id="5-1-人工定义模板"><a href="#5-1-人工定义模板" class="headerlink" title="5.1 人工定义模板"></a>5.1 人工定义模板</h4><p> 对于每种关系，人工定义一个模板来查询关系种的object。显然，模板的选择会对预测结果产生影响。因此，LAMA探针任务可以看做是衡量语言模型中包含知识的下边界。此外，传统知识库只能通过一种方式来查询关系知识，例如查询关系<strong>works-For</strong>时，如果用户使用 <strong>is-working-for</strong>，那么准确率就为0。</p><h4 id="5-2-单个token"><a href="#5-2-单个token" class="headerlink" title="5.2 单个token"></a>5.2 单个token</h4><p> 在预测任务中仅考虑单个token。限制单个token的原因是，多token解码会引入额外的可调参数，这会导致不好衡量模型中的知识量。此外，准确确定多token仍然是一个有挑战的问题，特别是对于双向语言模型。</p><h4 id="5-3-Object槽"><a href="#5-3-Object槽" class="headerlink" title="5.3 Object槽"></a>5.3 Object槽</h4><p> 在预测任务中仅对三元组中的object进行预测，因为通过反向关系也可以预测subject。没有查询relation slot的原因有二。首先，关系通常会跨越多个token，但这目前还是挑战。其次，即使能够预测多token的relation，但关系可以由不同的词表达，这会对衡量精度带来问题。</p><h4 id="5-4-词表交集"><a href="#5-4-词表交集" class="headerlink" title="5.4 词表交集"></a>5.4 词表交集</h4><p> 待比较的模型是在不同的词表上进行训练的。例如，ELMo有800K的词表，BERT则仅使用30K的词表。显然，词表大小会影响LAMA探针中不同模型的表现。词表越大，那么就越难从大量token中预测出真正的目标。因此，LAMA中仅考虑一个大小写敏感的21K词表，其是所有待比较模型词表的交集。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://img-blog.csdnimg.cn/8464bf7bb26a4bb7bc1bc6c9146785ca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p> 上表中汇总了主要的结果，显示了不同模型在不同语料上的top1平均准确率。下面分别讨论不同语料上的结果。</p><h4 id="6-1-Google-RE"><a href="#6-1-Google-RE" class="headerlink" title="6.1 Google-RE"></a>6.1 Google-RE</h4><p> BERT的base版和large版明显优于其他模型。在整体准确率上，相较于基于知识库的方法有2.2至2.9个准确率的提升。BERT-large的效果虽然很好，但不意味着其是以正确的方式得到的答案。因为，Google-RE中的句子很可能是BERT的训练语料，BERT-large可能并没有理解这些结果，只是通过共现模式学习到了subject和object的关系。(什么是真正的理解，人是理解了关系还是记住了更多的共现？)</p><h4 id="6-2-T-REx"><a href="#6-2-T-REx" class="headerlink" title="6.2 T-REx"></a>6.2 T-REx</h4><p> Google-RE中仅包含了较少的事实和仅有的3种关系，因此继续在更大的T-REx上进行实验。但是，实验结果与Google-RE一致。所以，BERT在检索事实知识方面的性能接近于现有的关系抽取系统和自动构建的知识库系统。按关系分类来看，BERT在 1-to-1 关系上的表现最好，在 N-to-M 的关系上表现最差。</p><p> 此外，下游模型可以利用语言模型输出的向量表示来学习，正确答案即使不排在第1，也会排的足够靠前。下图展示了所有模型的P@k曲线。对于BERT来说，正确的object被排在top10的有60%，排在top100的有80%。</p><p><img src="https://img-blog.csdnimg.cn/9f448e6b181144b09d1ef5a5b7bd78b1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p> 此外，BERT-large即使预测不对object，但也能预测出object的正确类型。(这个性质有益于使用prompt预测实体的类型)</p><p> 为了研究预训练语言模型对同一个事实的不同询问方式的变化(prompt的模板)。论文分析了每个关系中至多100个事实，并从T-REx中随机挑选出10个对齐的句子。每个句子中，遮盖掉object并使用模型进行预测。这可以测试一些语言模型从训练数据中记忆和召回的能力，因为这些模型已经在Wikipedia上训练过。下图展示了每个事实在10个不同查询上排序的平均分布。BERT和ELMo 5.5B的变化程度最低，正确的object接近平均的顶部。令人惊讶的是，ELMo original的表现也与BERT相差不大，但其并没有在训练时见过Wikipedia数据。Fairseq-fconv和Transformer-XL的变化程度高，因为其在训练时没有见过很多的Wikipedia数据。</p><p><img src="https://img-blog.csdnimg.cn/772c90faf7784e5f87fd45c285d9a8c2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="6-3-ConceptNet"><a href="#6-3-ConceptNet" class="headerlink" title="6.3 ConceptNet"></a>6.3 ConceptNet</h4><p> 在ConceptNet上检索事实的结果与Google-RE、T-REx一致，BERT-large的模型表现的最好。</p><h4 id="6-4-SQuAD"><a href="#6-4-SQuAD" class="headerlink" title="6.4 SQuAD"></a>6.4 SQuAD</h4><p> 在开发域问答上BERT-large和DrQA还是有一定的差距(也就是有改进的空间)。但是，预训练语言模型是完全无监督的，且没有专门的信息检索系统。此外，还比较了DrQA和BERT-large的P@10，发现差距十分的小。BERT-large为57.1，而DrQA为63.5。(如果top1更准的话，BERT可以直接作为问答系统)</p><h2 id="讨论与结论"><a href="#讨论与结论" class="headerlink" title="讨论与结论"></a>讨论与结论</h2><p>作者通过事实和常识问题的系统性分析，发现 BERT-large 能够比其竞争对手更好地回忆起这些知识，并且在非神经和有监督的替代品方面具有明显的竞争力。 请注意，作者没有比较相应的架构和目标在给定的正文中捕获知识的能力，而是将重点放在现有的预训练模型权重中所存在的知识上，这些模型已被许多研究人员用作研究的起点。了解我们常用的模型和学习算法正在捕获哪些数据方面是至关重要的研究领域，并且本文对许多专注于所学习数据的语言特性的研究进行了补充。</p><p>作者发现从与标准性能相当的文本中提取知识库，直接使用预训练的 BERT-large 并非难事。尽管为关系提取基线仅提供了可能表达目标事实的数据，从而减少了假阴性的可能性，以及使用了慷慨的实体链接预言。作者怀疑 BERT 可能由于其处理的数据量较大而具有优势，因此将 Wikitext-103 作为附加数据添加到关系提取系统，并且观察到性能没有明显变化。这表明尽管可能无法通过更多数据来提高关系提取性能，但将来在不断增长的语料库上训练的语言模型可能会成为将来从文本中提取的传统知识库的可行替代方案。</p><p>除了使用 LAMA 探针测试未来的预训练语言模型外，我们还希望量化关于各种自然语言模板的回忆事实知识的方差。此外，评估多记号答案仍然是作者评估设置面临的开放挑战。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>英文论文写作表达积累</title>
    <link href="/2022/07/13/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E8%A1%A8%E8%BE%BE%E7%A7%AF%E7%B4%AF/"/>
    <url>/2022/07/13/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E8%A1%A8%E8%BE%BE%E7%A7%AF%E7%B4%AF/</url>
    
    <content type="html"><![CDATA[<p>开一篇文用于记录日常读paper时好的英文表达。</p><h2 id="Sentence"><a href="#Sentence" class="headerlink" title="Sentence"></a>Sentence</h2><ol><li>The target task can be performed <strong>conditioning</strong> the LLM <strong>with</strong> task-specific prompts, a small portion of parameters, or features. 在…有着…的条件下</li><li>The optimization can be highly efficient since it does not require backpropagation, <strong>where</strong> （修饰backpropagation） the computation complexity <strong>is proportional</strong> <strong>to</strong> the model size and therefore can be expensive or even infeasible for LLMs.  与…成正比</li><li>It has been demonstrated that LLMs can <strong>achieve competitive performance</strong> <strong>on a broad range of tasks</strong> with limited or even zero labeled data. </li><li>Most works in LMaaS also <strong>focus on few-shot or zero-shot settings.</strong></li><li>…lead to a surge of improvements for downstream NLP tasks 导致推进了下游NLP任务的快速发展</li><li><strong>Whilst</strong> learning linguistic knowledge,  在…的同时</li><li>We present an in-depth analysis of the ….  in a wide range of stste-of-the-art pretrained language models 我们做了一个更深层次的分析</li><li>BERT <strong>does remarkably well</strong> on … against …</li><li>certain types of factual knowledge <strong>are learned much more readily</strong> than…</li><li>the <strong>surprisingly strong ability</strong> of … demonstrates their potential as …</li><li><strong>they are optimised to</strong> either predict the next word in a sequence or some masked word anywhere in a given sequence.</li><li>…is <strong>crucial</strong> for current state-of-art results</li><li>Moreover, errors can easily <strong>propagate and accumulate</strong> throughout the pipeline</li><li>language models <strong>come with various attractive properties</strong></li><li>beyond gathering a better… 除了…</li><li>we discuss each step in detail next and provide considerations on the probe below</li><li>we <strong>cover</strong> a variety of sources</li><li><strong>to what extent</strong> aligned texts （对应的文本）exist 在某种程度上</li><li>one can expect that …, and this is indeed the case:</li><li>with respect to 关于…</li><li>…will become a viable alternative to …将会成为一个可行的替代方案</li><li>recent work has presented <strong>intriguing</strong> results</li><li>these prompts are usually manually created, and quite possibly suboptimal</li><li>Because of this, given an inappropriate prompt, wemight <strong>fail to retrieve facts that the LM does know,</strong> and thus any given prompt only <strong>provides a lower bound estimate of</strong> the knowledge contained in an LM.</li><li>achieve a number of intriguing results <strong>regarding</strong> the knowledge</li><li>Thus it is quite possible that a fact that the LMdoes know cannot be retrieved due to the prompts not being effective queries for the fact.</li><li>写法可以借鉴：<strong>In this paper we ask the question:</strong> “How can we <strong>tighten this lower bound and get a more accurate estimate of the knowledge contained in state-of-the-art LMs</strong>?” <strong>This is interesting both scientifically,</strong> as a probe of the knowledge that LMs contain, <strong>and from an engineering perspective</strong>, as it will result in higher recall when using LMs as part of a knowledge extraction system.</li><li>an English-language benchmark <strong>devised to</strong> test the ability 被设计用来…</li><li>We perform extensive analysis and ablations, <strong>gleaning insights</strong> both about how to best query the knowledge stored in LMs and about potential directions for incorpo- rating knowledge into LMs themselves 收集了关于…的想法（洞察力）</li><li>to facilitate future experiments  以方便今后的实验 </li><li>a “good” prompt should <strong>trigger the LM to predict</strong> the ground- truth objects as often as possible</li><li>we <strong>tackle</strong> prompt generation: 也可以这样给出定义，用这个词</li><li>then <strong>uses the phrase spanning from</strong> the left- most word <strong>to</strong> the rightmost word in the dependency path as a prompt. spanning from从…到…</li><li><strong>be prone to noise</strong> 容易产生噪声</li><li>we <strong>assess the extent to which our prompts can improve fact prediction performance</strong>, raising the lower bound on the knowledge we dis-cern is contained in LMs.</li><li>to mitigate this problem 为了缓和这种问题&#x2F;解决</li><li>we also <strong>depict</strong> the performance of…  描述&#x2F;描绘</li><li>prompts will <strong>yield</strong> different predictions 产生不同结果用这个词</li><li><strong>it is of interest to know whether</strong> … or whether they can …</li><li>recent years have featured a trend towards…</li><li>many <strong>exaggerate</strong> actual performance on the <strong>underlying task</strong> 夸大了实际的底层任务的性能</li><li>aside from pointing to a conceptual limitation in our current NLP techniques 除了指出当前NLP技术的概念局限性之外</li><li>to be broadly useful, we would someday ….</li><li><strong>one potential route</strong> towards <strong>addressing these issues</strong> is …</li><li>results far inferior to 结果远不如</li><li><strong>Another recent trend in language modeling may offer a way forward</strong>. In recent years the capacity of transformer language models <strong>has increased substantially,</strong> from 100 million parameters, to 300 million parameters , to 1.5 billion parameters, to 8 billion parameters, 11 billion parameters, and finally 17 billion parameters.  描写transformer的语言模型最近规模的增长的</li><li>the results in this case are particularly <strong>striking</strong> 结果十分出色，引人注目</li><li>achieve <strong>promising</strong> results in the zero-shot and one-shot settings 达到了十分有前途的结果</li><li>the few-shot setting <strong>is sometimes competitive with or even occasionally surpasses state-of-the-art</strong>     few-shot的设置有时可以与最先进的技术竞争，甚至偶尔会超过。</li><li><strong>A heuristic sense of</strong> the overall results can be seen in Figure，which <strong>aggregates the various tasks</strong> 对整体结果的启发式认识可以从图中看出，该图汇总了各种任务</li><li>we also <strong>undertake a systematic study</strong> of… 我们还对…进行了系统的研究</li><li>Finally, given the broad spectrum of capabilities dispalyed by… 鉴于GPT-3所展示的广泛能力</li><li>attempt a preliminary analysis of … 尝试对…进行初步分析。</li><li>dataset <strong>constituting</strong> nearly a trillion words 由…组成</li><li>Large language models have recently been shown to <strong>attain reasonable zero-shot generalization</strong> on a diverse set of tasks</li><li>It has been <strong>hypothesized</strong> that …</li><li>to test this question <strong>at scale</strong> …</li><li>Recent work has shown that large language models <strong>exhibit the ability</strong> to perform reasonable zero- shot generalization to new tasks  大型语言模型展现出…样的能力</li><li>to evaluate zero-shot generalization to new tasks 为了评估对于新任务的零样本泛化性能</li><li>we err on the side of … as opposed to 我们偏向于，而不是…</li><li>Recent advancements in prompting large language models (LMs) such as GPT-3 (Brown et al., 2020) show that <strong>pretrained LMs can be used to perform NLP tasks via textual prompts containing a task description and a few examples, without the need for task-specific tuning (Radford et al., 2019; Brown et al., 2020)</strong></li><li>In this scenario, the performance of an LM <strong>is critically dependent on finding the most appropriate prompt</strong> for a given task, <strong>otherwise known as prompt-engineering</strong> (Liu et al., 2021b).  说有prompt engineering的一种方式</li><li>Following Webson and Pavlick (2021), we <strong>characterize</strong> instructions <strong>as</strong> a natural language description of the task that includes what is required for a person to complete the task correctly. 我们定义instruction的特征为…</li><li>instructions should <strong>be semantically coherent to</strong> humans 与人类的语法保持一致</li><li><strong>For purposes of improving model performance via instructions</strong>, <strong>Mishra et al. (2022b)（可以改写这里面的人名）</strong> <strong>provide a set of guidelines</strong> for manually rewriting instructions that were …</li><li>sb <strong>decompose</strong> complex tasks <strong>into</strong> self-contained sub-tasks 将复杂的问题分解为…</li><li>a <strong>follow-up</strong> work reveals that … 用于写related work中的内容，一个接下来的工作揭示了…</li><li>may <strong>prohibitively expensiv</strong>e to compute 过分的昂贵</li><li>our search does not <strong>utilize any task-specific properties</strong> 利用任何特定任务的属性</li></ol><h3 id="Introduction："><a href="#Introduction：" class="headerlink" title="Introduction："></a>Introduction：</h3><ol><li><p>在写多语言论文的introduction的时候可以这样借鉴：</p><p>In this paper, we <strong>focus on explicitly training language models in a supervised and massively multi- task fashion</strong>. <strong>Our approach uses</strong> a training mixture consisting of a large set of different tasks speci-fied in natural language prompts. <strong>Our goal is to</strong> induce a model to better generalize to held-out tasks without requiring massive scale, as well as being more robust to the wording choices of the prompts. To convert a large set of natural language tasks into prompted form, we use a simple templating language for structured datasets. <strong>We develop an interface for prompt collection</strong> from public contributors that facilitated the collection of a large multitask mixture with multiple prompts per dataset (Bach et al., 2022). <strong>We then train a variant of the T5 encoder-decoder model</strong> (Raffel et al., 2020; Lester et al., 2021) <strong>on a subset of the tasks</strong> (each with multiple datasets) and <strong>then evaluate tasks and prompts that the model was not trained on</strong>.</p><p><strong>Our experiments study two questions.</strong> First, does multitask prompted training improve generalization to held-out tasks? Second, does training on a wider range of prompts improve robustness to prompt wording? For the first question, <strong>we find that multitask training enables zero-shot task generalization by showing that our model matches or exceeds the performance of GPT-3 (Brown et al., 2020) on 9 out of 11 held-out datasets, despite being about 16× smaller.</strong> We also show that the model improves over a large baseline language model on 13 out of 14 tasks in the BIG-bench benchmark (BIG-bench collaboration, 2021). <strong>For the second question, we find that training on more prompts per dataset consistently improves the median and decreases the variability of performance on held-out tasks.</strong> Training on prompts from a wider range of datasets also generally improves the median but does not consistently decrease the variability.</p></li><li><p>However, <strong>the extent to which this success depends on</strong> the semantic meaningfulness of the prompts has been challenged (Webson and Pavlick, 2021; Logan et al., 2021). Thus, in this work, <strong>we remain agnostic as to why prompts support generalization.</strong></p></li></ol><h3 id="reult的结果分析："><a href="#reult的结果分析：" class="headerlink" title="reult的结果分析："></a>reult的结果分析：</h3><ol><li>we <strong>see significant jumps</strong> in accuracy 准确性都有明显的跳跃</li><li>Search Improvements <strong>Correlate with</strong> Model Sensitivity to Instructions …的改进与模型对指令的敏感度有关（与…有关）</li><li>these findings <strong>build upon</strong> results from … 这些发现建立在…的结果上</li><li>说结果的时候：We find that our search <strong>is effective in this setting across all models</strong>, improving accuracy by roughly 2 points. 可以先算一个平均的准确率大概提高了多少</li></ol><h2 id="Word"><a href="#Word" class="headerlink" title="Word"></a>Word</h2><p>大模型的另一种表达：pretrained high-capacity language models</p><p>extract relational data from <strong>text or other modalities</strong>  从文本数据或者一些其它模态的数据</p><p><strong>populate</strong>：populate knowledge bases 填充数据库</p><p>off-the-shelf 现成的 in pretrained off-the-shelf language models</p><p>knowledge base completion literature</p><p>canonical ways 权威的</p><p>单词缩写：<strong>language models(LM)</strong> by having the <strong>LM</strong></p><p>prompt ensemble prompt集成</p><p>substantially 大量的</p><p>deterministic 确定性的查询</p><p><strong>retrive&#x2F;elicite</strong> knowledge from the LM 从语言模型里获取知识</p><p>be affiliated with the religion 有着…的信仰</p><p><strong>concatenate</strong> them with… 把…拼接起来</p><p><strong>be conducive to</strong>…有助于</p><p>one-shot and few-shot <strong>proficiency</strong>  one-shot和few-shot的熟练程度</p><p><strong>unscrambling</strong> words解密单词</p><p>performe arithmetic 执行算术</p><p><strong>inflating</strong> results 一些夸大的结果</p><p>data <strong>contamination</strong> 数据污染</p><p>crowd-sourced 众包</p><p>subjective interpretation 主观解释</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Writing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>研究方向之语言模型及服务(LMaaS)</title>
    <link href="/2022/07/11/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B9%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%9C%8D%E5%8A%A1-LMaaS/"/>
    <url>/2022/07/11/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B9%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%9C%8D%E5%8A%A1-LMaaS/</url>
    
    <content type="html"><![CDATA[<p>在知乎上看到一篇文章，在这里附上链接：<a href="https://zhuanlan.zhihu.com/p/538857729?utm_source=wechat_timeline&utm_medium=social&utm_oi=27925374042112&utm_campaign=shareopn">”语言模型即服务“必读论文</a> ，解决了我很多天以来的疑惑，因为包括自己的毕业论文，自己刚刚投完的EMNLP2022，都是我导给了一个大致的方向，然后自己看了些论文，做了些实验，但其实一直都不知道自己的大方向的可以如何用专业名词来概括，以及自己以后是不是要一直研究这个领域，以及这个领域还能研究些什么，这篇文章都解决了我的这些疑问。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><img src="https://pic4.zhimg.com/80/v2-bb3ef100e1ceedb20ca9aa1d80b5234f_1440w.jpg" alt="img"></p><p>以上这张很精美，在未来可能会成为大趋势的图，就来自知乎的那篇文章。LMaaS，即<strong>Language-Model-as-a-Service</strong> 。一直以来，”无法规模化“，”没有通用的解决方案“是让很多AI初创公司广受吐槽的一点。所以现在更适用于工业界的大厂的方法，是把大规模语言模型的推理功能包装成一个API来给用户提供服务。从商业应用层面上来说，仅需部署一个通用的语言模型，即可支持用户适配很多目标任务，即上图所示。</p><p>然而，在我阅读了一小部分论文后，我也知道这在目前来说还仅仅是一种美好的理想，在众多实验中，仅调用模型推理API的方式却通常很难超过在本地微调一个小模型。这篇文章的作者抱着让NLP狠狠出圈发财致富的愿景（当然也是我的最大愿景），最近调研了适用于LMaaS场景的几个方向并维护了一个论文列表，<a href="https://github.com/txsun1997/LMaaS-Papers">https://github.com/txsun1997/LMaaS-Papers</a> ，目前已经收集了40篇相关论文，未来一定会补充更多新工作加进来。</p><h2 id="细分方向"><a href="#细分方向" class="headerlink" title="细分方向"></a>细分方向</h2><p>经过调研，作者将LMaaS的方向细分为以下五个方面：</p><ol><li><strong>Text prompt.</strong> 手工或自动地构造提示语来诱导大模型说出想要的答案</li><li><strong>In-context learning.</strong> 将少量带标签样本放到输入的上下文中，帮助大模型适配到目标任务</li><li><strong>Black-box optimization.</strong> 仅访问大模型的输出概率来使用黑箱优化的手段优化一小部分任务特定的参数</li><li><strong>Feature-based learning.</strong> 将大模型作为特征抽取器，得到样本特征后在本地训练小模型完成任务</li><li><strong>Data generation.</strong> 使用生成式大模型生成特定任务的训练集，利用生成的训练集在本地训练小模型来完成任务</li></ol><p>目前来说，在这五个方向上所做的工作都可以促进LMaaS的发展，我之前的工作就是在text prompt上一直在进行探索，从作者整理的论文列表可以看出来，text prompt和in-context learning是目前研究最多的方向，有不少有意思的工作，但是现阶段想要突破其实我觉得不容易。“相比之下，其余三个方向方兴未艾，大有可为”。我大致看了一下，大概这四十篇文章中有十篇是我看过的文章，很多都是2022年的新工作，最新有到2022年6月份的，但是涉及到多语言的工作还是很少很少，那么接下来暑假的时间就给了我非常好的一些阅读方向，除了text prompt，其余四个方向我也需要了解，或许能为自己的多语言方面的工作提供一些新的思路。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EMNLP2022投稿总结</title>
    <link href="/2022/07/10/EMNLP2022%E6%8A%95%E7%A8%BF%E6%80%BB%E7%BB%93/"/>
    <url>/2022/07/10/EMNLP2022%E6%8A%95%E7%A8%BF%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>EMNLP2022的论文投稿已经过去两周了，这是第一次完全自己进行投稿的经历，还是值得总结与反思，算是给自己研究生的一个开始，自己的工作其实还是缺少创新性的，所以看看审稿人的意见，再继续好好做实验与推进下一步工作。</p><h3 id="How-to-sumbit-paper-to-EMNLP2022"><a href="#How-to-sumbit-paper-to-EMNLP2022" class="headerlink" title="How to sumbit paper to EMNLP2022"></a>How to sumbit paper to EMNLP2022</h3><p>因为对投稿完全一点都没有经验，流程都不了解，然后又是因为自己的事情拖了几天才开始真的写初稿，所以其实整个时间线上还是很紧的，所以以后投稿一定要先了解会议投稿流程！</p><p>EMNLP 2022：<a href="https://link.zhihu.com/?target=https://2022.emnlp.org/">https://2022.emnlp.org/</a></p><h4 id="01-重要时间"><a href="#01-重要时间" class="headerlink" title="01 重要时间"></a>01 重要时间</h4><p>匿名期开始时间：2022年5月24日</p><p>通过softconf投稿的摘要截止时间：<strong>2022年6月17日</strong></p><p>通过softconf投稿的全文截止时间：<strong>2022年6月24日</strong></p><p>通过ARR投稿的截止时间：2022年7月24日</p><p>作者反馈时间：2022年8月23日-29日</p><p>录用通知时间：2022年10月6日</p><p>终稿提交时间：2022年10月21日</p><p>研讨会&amp;讲习班：2022年12月7日-8日</p><p>大会时间：2022年12月9日-11日</p><p>1）所有截止时间是11:59PM UTC-12h（即地球上的任何地方）</p><p>2）长文与短文的时间轴一致</p><p>3）<strong>如果要直接投稿，则必须在摘要截止时间之前完成摘要提交，否则不允许提交全文</strong></p><h4 id="02-主要变动"><a href="#02-主要变动" class="headerlink" title="02 主要变动"></a>02 主要变动</h4><p>EMNLP 2022延续EMNLP 2021的做法，使用混合投稿模式，即EMNLP 2022同时接收两种投稿方式：</p><ul><li>通过ARR投稿并完成审稿，后续提交到EMNLP</li><li>直接通过softconf系统投稿到EMNLP</li></ul><p>为了保证整个研究社区审稿量的平衡度，我们会提前询问作者是想被ARR还是EMNLP审稿。</p><p><strong>审稿流程</strong>：</p><ul><li><strong>通过EMNLP直接投稿</strong>：与传统会议相同，论文将由3位审稿人审稿，有author response环节，并且在camera-ready前可以完善他们的论文（论文录用的情况下）。</li><li><strong>通过ARR投稿</strong>：论文将由高级领域主席（SAC）处理。作者可以提供author response，但不允许现有的论文。</li></ul><p><strong>混合投稿政策</strong>：</p><ul><li><p>在2022年7月24日前，如果ARR论文得到所有审稿意见和综合审稿意见（meta-review），即可提交（commit）到EMNLP。</p></li><li><ul><li>论文不可修改，但可以附加author response。</li><li>EMNLP会考虑在2022年7月24日前完成审稿的ARR论文。但与ACL和NAACL不同之处在于，ARR并不能保证在EMNLP截稿时间之前完成所有审稿。所以作者需要抉择是通过ARR还是直接投稿到EMNLP。</li><li>非ARR投稿论文的截稿时间是2022年6月24日。</li></ul></li><li><p>在2022年5月24日以前投稿到ARR的论文可以撤回并投稿到EMNLP 2022。</p></li><li><ul><li>如果要直接投稿到EMNLP 2022，论文必须从ARR系统中撤回，或者论文在5月24日前完成上一轮审稿并且没有提交到下一轮ARR。</li><li>作者可以在2022年5月24日前从ARR撤稿（不论已经收到几份审稿意见）。</li></ul></li><li><p>2022年5月24日之后，论文仍然在ARR系统中（不论是新提交还是未能及时撤稿）将不能直接投稿到EMNLP 2022。</p></li><li><p>在审稿期间，提交到EMNLP 2022的论文不能再次投到其他刊物上（包括ARR）。</p></li></ul><h4 id="03-投稿类型和要求"><a href="#03-投稿类型和要求" class="headerlink" title="03 投稿类型和要求"></a>03 投稿类型和要求</h4><p><strong>3.1 论文篇幅及提交</strong></p><p>和往届会议一样，EMNLP 2022将接收两种投稿类型：长文和短文。长文最多8页正文，<strong>短文最多4页正文</strong>，参考文献不计入页数限制。我这次投的就是short paper，刚开始没有很注意这个要求，后来还花了很多时间去删改，这是很不应该的。而且最终的文章把参考文献，图表等都放在了从第五页开始的地方，排版还是不够好看，这都是以后要注意的地方。录用后的论文可增加不超过一页的正文内容。</p><p><strong>3.2 论文署名</strong></p><p>论文列表中应列举所有并且只列举那些对论文工作作出重要贡献的个人。每位作者都将收到EMNLP 2022的通知（投稿、修改、录用结果）<strong>。在EMNLP 2022的摘要截稿（2022年6月17日）之后，将无法增减作者，也不能够更改作者顺序。</strong></p><p><strong>3.3 （新增）论文局限性****（非常重要！）</strong></p><p>我们认为探讨论文工作的不足之处也是非常重要的。EMNLP 2022要求所有论文添加一个“Limitations”章节来探讨论文局限性。这一章将会放在discussion&#x2F;conclusion之后，参考文献之前。需要注意的是，这部分内容不计入页数限制之内。<strong>如果论文中没有添加这一章节，将会自动被拒稿。</strong>如果通过ARR审稿的论文不包含这一章节，可以在提交到EMNLP 2022时附带一个PDF来讨论论文局限性。</p><p><strong>3.4 论文的模板</strong></p><p>更新在EMNLP 2022的官方网站上。另外，<strong>请不要修改样式文件</strong>，也不要使用其他会议的模板。如果论文不满足样式要求将在审稿期前直接被拒稿，这包括纸张大小、边距、字体等限制。</p><p><strong>3.5 引用和对比</strong></p><p>作者应在论文中对比所有已发表的相关文献，但可以因为不了解未发表的文献而没有进行对比（尤其是近期才公开的文献或没有被广泛引用的文献）。如果相关论文的预印本被正式出版，作者需要引用已出版的版本而不是预印本版本。在截稿时间之前3个月内的论文（不论发表与否）可以被认为是同期论文，你不需要进行深度对比，例如进行额外的实验或深度分析等。</p><h3 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h3><p><strong>写论文工具</strong>—overleaf</p><p><strong>画图工具</strong>—draw.io（放入文章里面的图一定是高清矢量图，所以一定要用draw.io画好了把pdf进行导出！）</p><p><strong>文献整理工具</strong>—一定要使用google scholar（或者是国内的一些镜像网站，都是最权威的引用方法）；将arxiv上面的论文变成bibtex形式 链接：<a href="https://arxiv2bibtex.org/?q=2203.12277&amp;format=bibtex">https://arxiv2bibtex.org/?q=2203.12277&amp;format=bibtex</a></p><p><strong>翻译工具</strong>—DeepL翻译器（最优雅的翻译！翻译的是真的好诶）</p><h3 id="Notice—To-do"><a href="#Notice—To-do" class="headerlink" title="Notice—To do"></a>Notice—To do</h3><h4 id="01-论文结构"><a href="#01-论文结构" class="headerlink" title="01 论文结构"></a>01 论文结构</h4><p>论文前前后后改了差不多有五版，第一次大改就是结构基本上都不太对，然后控制的篇幅也不太对，首先论文结构就是一篇好的论文的基础，所以从现在开始看文章不能是只看内容了，还需要看文章的结构，篇幅布局，要多看些文章，多学习文章结构的写法，好的文章都是从模仿开始。</p><h4 id="02-英文表达"><a href="#02-英文表达" class="headerlink" title="02 英文表达"></a>02 英文表达</h4><p>英文表达对于一篇好文章也至关重要，之后还是要继续学习英语，平时注意积累英文表达，读paper的时候尽量先去看英文，而不是依赖于翻译，并且同样注意积累好的表达，很多时候好的句子表达可以让一个段落都活起来。还有一些小的细节需要注意，同一个出现的单词，写法一定要保证一致，比如多任务，全部都改为multitask，不能存在有multi-task；还有比如文章中如果第一次出现缩写的词，需要全拼进行解释，这些都是约定俗成的规矩，需要注意。</p><h4 id="03-其它"><a href="#03-其它" class="headerlink" title="03 其它"></a>03 其它</h4><p>注意引用！别的论文的观点一定要引用，做到严谨！</p><p>实验部分是重点，一定要详细叙述，做实验的时候记录非常重要，对比实验很重要，一定要做全面</p><p>result部分一定要进行重点分析，为什么会出现这些实验结果的原因，是更加有价值的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Submission</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/07/09/hello-world/"/>
    <url>/2022/07/09/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
