<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>《Language Models as Knowledge Bases?》论文阅读笔记</title>
    <link href="/2022/07/13/%E3%80%8ALanguage-Models-as-Knowledge-Bases-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/07/13/%E3%80%8ALanguage-Models-as-Knowledge-Bases-%E3%80%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<p>paper：<a href="https://arxiv.org/abs/1909.01066">https://arxiv.org/abs/1909.01066</a></p><p>code：<a href="https://github.com/facebookresearch/LAMA">https://github.com/facebookresearch/LAMA</a></p><p>来源：EMNLP 2019</p><p>作者单位：Facebook AI 研究院，伦敦大学学院</p><p><img src="https://pic2.zhimg.com/v2-6cfa5024e7ca097a7b43ee6e3008a4a0_1440w.jpg?source=172ae18b" alt="【EMNLP 2019】Language Models as Knowledge Bases?"></p><p><strong>这项研究所揭示的重要结论</strong>：</p><ul><li>BERT-large 模型捕获了（准确的）关系知识，该知识与使用现成的关系提取器和基于oracle 的实体链接器从已知表示相关知识的语料库中提取的知识库相当。</li><li>BERT-large 在开放域 QA 中取得了显着的结果，与使用任务特定的监督关系提取系统构建的知识库取得的 63.5% precision@10 相比，它取得了 57.1 % 的效果。</li><li>BERT-large 在恢复事实和常识性知识方面始终优于其他语言模型。</li><li>事实知识可以从预训练语言模型中意外地很好地恢复，但是，对于某些关系（特别是 N 对M 关系）的性能非常差。</li></ul><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>语言模型用于预测句子中的下一个单词，或者给定句子预测其中任何位置的被掩盖的单词。预训练模型需要存储大量的对下游任务有用的语言学知识。通常通过以原始模型产生的潜在上下文表示为条件，或通过使用原始模型权重来初始化特定于任务的模型，然后进一步进行微调，来访问此知识。此类知识转移对于当前在各种任务上的最新成果至关重要。</p><p>与此对比，知识库是通过使用查询来访问带注释的有严格标准关系的数据的有效方案。 但是，在实践中，我们经常需要<strong>从文本或其他方式中提取关系数据以填充这些知识库</strong>。 这需要复杂的 NLP 流水线，包括实体提取，共指解析，实体链接和关系提取，这些组件通常需要监督数据和固定模式。 而且，错误很容易在整个流水线中传播和累积。相反，我们可以尝试通过要求以关系数据的形式查询神经语言模型，例如“ Dante出生于[Mask]”。在这种情况下，<strong>语言模型具有各种吸引人的属性：它们不需要架构工程，不需要人工注释，并且支持一组开放的查询</strong>。</p><p><img src="https://img-blog.csdnimg.cn/ca85281766ab45e080354686663bc17b.png" alt="在这里插入图片描述"></p><p>​                                                在知识库和语言模型中查询事实类知识</p><p>鉴于语言模型的上述特性可以作为关系知识的潜在表示形式，作者表示对预先训练的现成语言模型（例如 ELMo 和 BERT）中已经存在的关系知识感兴趣。 他们存储多少关系知识？ 对于不同类型的知识（例如有关实体的事实、常识和一般性问答），这有何不同？ 与从文本中自动提取的符号知识库相比，无需微调的性能如何？ 除了收集对这些模型的更好的一般理解之外，我们认为这些问题的答案可以帮助我们设计更好的无监督知识表示，这些知识表示可以将事实知识和常识性知识可靠地转移到下游任务，例如常识（视觉）问题解答或增强学习。</p><p>为了解答上述问题，该文介绍了 LAMA（LAnguage Model Analysis），<strong>包含一系列知识源，每个知识源包含一组事实。作者定义，一个语言模型知道一个事实（主体，关系，客体）当它在完形填空任务中能成功预测被掩盖的客体时。</strong></p><p>作者测试各种类型的知识：存储在 Wikidata 中的实体之间的关系，ConceptNet 概念之间的常识关系以及回答 SQuAD 中自然语言问题所必需的知识。 在后一种情况下，作者手动映射SQuAD 问题的子集以结束句子。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>在背景介绍了几种语言模型：</p><p><img src="https://img-blog.csdnimg.cn/fbc3c87c337c4def9bae9d02ce8ff6d5.png" alt="在这里插入图片描述"></p><h3 id="1-Unidirectional-language-models"><a href="#1-Unidirectional-language-models" class="headerlink" title="1.Unidirectional language models"></a>1.Unidirectional language models</h3><p>给定一个输入序列 w &#x3D; [w1, w2, … , wN] ，单向语言模型会将整个输入序列按下面的方式进行分解p (w) :</p><p><img src="https://img-blog.csdnimg.cn/b868f60f4a754574b67433c5c68c2ee0.png" alt="在这里插入图片描述"></p><p>一个比较通用的方法是通过神经网络来估计概率<br><img src="https://img-blog.csdnimg.cn/e859bf750ecf43c19efe779f363807fd.png" alt="在这里插入图片描述"><br>其中，ht ∈ R^k 是神经网络在位置 t 的输出向量，W∈R^(∣V∣×k)是一个可学习参数，用于将ht映射为词表 V 中每个词的非标准化分数。获得 ht 的神经网络结构可能有所不同，比如有多层感知器，卷积层，循环神经网络以及自注意力机制。</p><p>典型的单向语言模型为<strong>fairseq-fconv</strong>和<strong>Transformer-XL。</strong>。</p><h3 id="2-Bidirectional-language-models"><a href="#2-Bidirectional-language-models" class="headerlink" title="2.Bidirectional language models"></a>2.Bidirectional language models</h3><p>单向语言模型通过上文词语来预测下一个词。但是，一个词的含义是同时由上下文决定的。因此，给定输入序列 w &#x3D; [w1, w2, … , wN]和一个位置 1 ≤ i ≤ N，那么双向语言模型期望估计概率p (wi∣w1, … , wi−1, wi+1, … , wN) 会用这个词的上下文。</p><p>典型的单向语言模型为<strong>ELMO</strong>和<strong>BERT。</strong></p><h2 id="LAMA-探针"><a href="#LAMA-探针" class="headerlink" title="LAMA 探针"></a>LAMA 探针</h2><p>论文引入的LAMA探针可以用来测试语言模型中的事实和常识知识。该探针本质上提供了一组由<strong>事实</strong>组成的知识源。这里的事实是指subject-relation-object三元组或者问答对。每个事实都会被转换为“完型填空”形式的陈述句(prompt)，然后用来从语言模型中查询目标token。举例来说，给定一个事实(dante, born-in, florence)，如果要查询是否包含该知识，可以将其转换为陈述句Dante was born-in ___。</p><p> 在评估效果时，会根据真实token在候选词表中的位置进行评估，排名越靠前，则认为模型包含越多的知识。</p><h3 id="1-知识源"><a href="#1-知识源" class="headerlink" title="1. 知识源"></a>1. 知识源</h3><p>为了评估分析在前面介绍语言模型的时候不同的语言模型，在这里作者介绍包含了很多源的事实和常识知识。</p><h4 id="1-1-Google-RE"><a href="#1-1-Google-RE" class="headerlink" title="1.1 Google-RE"></a>1.1 Google-RE</h4><p> 语料Google-RE是从wikipedia中人工抽取的、包含60K事实的知识源，其覆盖了5种关系。但LAMA探针仅考虑其中的三种：place of birth、date of birth和place of death。排除另外两种的原因是，在评估中不支持多token对象。对于三元组事实中的每个关系，都会定义一个模板，例如：”[S] was born in [O]“为关系place of birth的模板。</p><h4 id="1-2-T-REx"><a href="#1-2-T-REx" class="headerlink" title="1.2 T-REx"></a>1.2 T-REx</h4><p> 知识源T-REx是wikipedia三元组的子集，其要比Google-RE大得多，且拥有更加广泛的关系集合。LAMA中考虑了41个wikipedia中的关系，并且每种关系采样1000个事实。同Google-RE数据集一样，我们人工为每个关系定义了模板(prompt)。</p><h4 id="1-3-ConceptNet"><a href="#1-3-ConceptNet" class="headerlink" title="1.3 ConceptNet"></a>1.3 ConceptNet</h4><p> ConceptNet是一个多语言知识库，该知识库是从Open Mind Common Sense(OMCS)中的句子构造出来的。LAMA中仅考虑ConceptNet中英语部分的事实，其中有16种关系具有单个token的ojbect。对于任意ConceptNet三元组，可以从OMCS中找到同时包含subject和object的句子。对该句子中的object进行mask，从而构成一个prompt。若三元组对应多个句子，则随机挑选一个。</p><h4 id="1-4-SQuAD"><a href="#1-4-SQuAD" class="headerlink" title="1.4 SQuAD"></a>1.4 SQuAD</h4><p> SQuAD是一个常见的问答数据集，LAMA从SQuAD的开发集中挑选了305个具有单token答案且上下文不敏感的问题。人工从这些问题中创建完型填空风格的问题。例如，将”Who developed the theory of relativity?“重写为”The theory of relativity was developed by ___”。</p><h3 id="2-模型"><a href="#2-模型" class="headerlink" title="2. 模型"></a>2. 模型</h3><p> 论文中测试的语言模型有：fairseq-fconv(Fs)、Transformer-XL large(Txl)、ELMo original(Eb)、ELMo 5.5B(E5B)、BERT-base(Bb)和BERT-large(Bl)。</p><p> 模型的目标是预测特定位置t处的token。对于单向语言模型，使用t-1处网络生成的向量      （ <strong>h</strong>t−1）进行预测。对于ELMo，则会使用前向的（ <strong>h</strong>t−1）和后向的（ <strong>h</strong>t+1）。对于BERT，则遮盖t处的token，然后将（ <strong>h</strong>t）输入softmax层。为了公正的比较，生成一个所有模型词表的交集，然后在该交集词表上预测token。</p><h3 id="3-基线"><a href="#3-基线" class="headerlink" title="3. 基线"></a>3. 基线</h3><p> 为了比较语言模型与传统系统，论文考虑了下面的baseline。</p><h4 id="3-1-Freq"><a href="#3-1-Freq" class="headerlink" title="3.1 Freq"></a>3.1 Freq</h4><p> 给定一个subject和relation关系对，该baseline会基于<strong>测试集</strong>中该关系对中出现的所有object的频率进行单词排序。该baseline是那些总是预测相同object的模型的上边界。</p><h4 id="3-2-RE"><a href="#3-2-RE" class="headerlink" title="3.2 RE"></a>3.2 RE</h4><p> 对于基于关系的知识源，使用一个预训练好的关系抽取模型RE，该模型在Wikidata上进行训练。该模型是基于LSTM和注意力机制的编码器，用于从句子中抽取三元组。RE对包含事实的句子进行三元组抽取，并构建知识图谱。在测试时，在图谱上查询指定的subject，然后基于RE返回的置信分数来排序object。</p><h4 id="3-3-DrQA"><a href="#3-3-DrQA" class="headerlink" title="3.3 DrQA"></a>3.3 DrQA</h4><p> DrQA是一个开放域问答系统，其使用两阶段的pipeline来回答自然语言问题。首先，使用TF-IDF从大量文档中检索出相关的文章，然后在检索出的topK的文章中，使用神经阅读理解模型来抽取答案。这里会显著DrQA只预测单个token，从而可以与语言模型进行比较。</p><h3 id="4-评估指标"><a href="#4-评估指标" class="headerlink" title="4. 评估指标"></a>4. 评估指标</h3><p> 使用基于rank的评估指标。</p><h3 id="5-注意事项"><a href="#5-注意事项" class="headerlink" title="5. 注意事项"></a>5. 注意事项</h3><h4 id="5-1-人工定义模板"><a href="#5-1-人工定义模板" class="headerlink" title="5.1 人工定义模板"></a>5.1 人工定义模板</h4><p> 对于每种关系，人工定义一个模板来查询关系种的object。显然，模板的选择会对预测结果产生影响。因此，LAMA探针任务可以看做是衡量语言模型中包含知识的下边界。此外，传统知识库只能通过一种方式来查询关系知识，例如查询关系<strong>works-For</strong>时，如果用户使用 <strong>is-working-for</strong>，那么准确率就为0。</p><h4 id="5-2-单个token"><a href="#5-2-单个token" class="headerlink" title="5.2 单个token"></a>5.2 单个token</h4><p> 在预测任务中仅考虑单个token。限制单个token的原因是，多token解码会引入额外的可调参数，这会导致不好衡量模型中的知识量。此外，准确确定多token仍然是一个有挑战的问题，特别是对于双向语言模型。</p><h4 id="5-3-Object槽"><a href="#5-3-Object槽" class="headerlink" title="5.3 Object槽"></a>5.3 Object槽</h4><p> 在预测任务中仅对三元组中的object进行预测，因为通过反向关系也可以预测subject。没有查询relation slot的原因有二。首先，关系通常会跨越多个token，但这目前还是挑战。其次，即使能够预测多token的relation，但关系可以由不同的词表达，这会对衡量精度带来问题。</p><h4 id="5-4-词表交集"><a href="#5-4-词表交集" class="headerlink" title="5.4 词表交集"></a>5.4 词表交集</h4><p> 待比较的模型是在不同的词表上进行训练的。例如，ELMo有800K的词表，BERT则仅使用30K的词表。显然，词表大小会影响LAMA探针中不同模型的表现。词表越大，那么就越难从大量token中预测出真正的目标。因此，LAMA中仅考虑一个大小写敏感的21K词表，其是所有待比较模型词表的交集。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://img-blog.csdnimg.cn/8464bf7bb26a4bb7bc1bc6c9146785ca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p> 上表中汇总了主要的结果，显示了不同模型在不同语料上的top1平均准确率。下面分别讨论不同语料上的结果。</p><h4 id="6-1-Google-RE"><a href="#6-1-Google-RE" class="headerlink" title="6.1 Google-RE"></a>6.1 Google-RE</h4><p> BERT的base版和large版明显优于其他模型。在整体准确率上，相较于基于知识库的方法有2.2至2.9个准确率的提升。BERT-large的效果虽然很好，但不意味着其是以正确的方式得到的答案。因为，Google-RE中的句子很可能是BERT的训练语料，BERT-large可能并没有理解这些结果，只是通过共现模式学习到了subject和object的关系。(什么是真正的理解，人是理解了关系还是记住了更多的共现？)</p><h4 id="6-2-T-REx"><a href="#6-2-T-REx" class="headerlink" title="6.2 T-REx"></a>6.2 T-REx</h4><p> Google-RE中仅包含了较少的事实和仅有的3种关系，因此继续在更大的T-REx上进行实验。但是，实验结果与Google-RE一致。所以，BERT在检索事实知识方面的性能接近于现有的关系抽取系统和自动构建的知识库系统。按关系分类来看，BERT在 1-to-1 关系上的表现最好，在 N-to-M 的关系上表现最差。</p><p> 此外，下游模型可以利用语言模型输出的向量表示来学习，正确答案即使不排在第1，也会排的足够靠前。下图展示了所有模型的P@k曲线。对于BERT来说，正确的object被排在top10的有60%，排在top100的有80%。</p><p><img src="https://img-blog.csdnimg.cn/9f448e6b181144b09d1ef5a5b7bd78b1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p> 此外，BERT-large即使预测不对object，但也能预测出object的正确类型。(这个性质有益于使用prompt预测实体的类型)</p><p> 为了研究预训练语言模型对同一个事实的不同询问方式的变化(prompt的模板)。论文分析了每个关系中至多100个事实，并从T-REx中随机挑选出10个对齐的句子。每个句子中，遮盖掉object并使用模型进行预测。这可以测试一些语言模型从训练数据中记忆和召回的能力，因为这些模型已经在Wikipedia上训练过。下图展示了每个事实在10个不同查询上排序的平均分布。BERT和ELMo 5.5B的变化程度最低，正确的object接近平均的顶部。令人惊讶的是，ELMo original的表现也与BERT相差不大，但其并没有在训练时见过Wikipedia数据。Fairseq-fconv和Transformer-XL的变化程度高，因为其在训练时没有见过很多的Wikipedia数据。</p><p><img src="https://img-blog.csdnimg.cn/772c90faf7784e5f87fd45c285d9a8c2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQlFXXw==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h4 id="6-3-ConceptNet"><a href="#6-3-ConceptNet" class="headerlink" title="6.3 ConceptNet"></a>6.3 ConceptNet</h4><p> 在ConceptNet上检索事实的结果与Google-RE、T-REx一致，BERT-large的模型表现的最好。</p><h4 id="6-4-SQuAD"><a href="#6-4-SQuAD" class="headerlink" title="6.4 SQuAD"></a>6.4 SQuAD</h4><p> 在开发域问答上BERT-large和DrQA还是有一定的差距(也就是有改进的空间)。但是，预训练语言模型是完全无监督的，且没有专门的信息检索系统。此外，还比较了DrQA和BERT-large的P@10，发现差距十分的小。BERT-large为57.1，而DrQA为63.5。(如果top1更准的话，BERT可以直接作为问答系统)</p><h2 id="讨论与结论"><a href="#讨论与结论" class="headerlink" title="讨论与结论"></a>讨论与结论</h2><p>作者通过事实和常识问题的系统性分析，发现 BERT-large 能够比其竞争对手更好地回忆起这些知识，并且在非神经和有监督的替代品方面具有明显的竞争力。 请注意，作者没有比较相应的架构和目标在给定的正文中捕获知识的能力，而是将重点放在现有的预训练模型权重中所存在的知识上，这些模型已被许多研究人员用作研究的起点。了解我们常用的模型和学习算法正在捕获哪些数据方面是至关重要的研究领域，并且本文对许多专注于所学习数据的语言特性的研究进行了补充。</p><p>作者发现从与标准性能相当的文本中提取知识库，直接使用预训练的 BERT-large 并非难事。尽管为关系提取基线仅提供了可能表达目标事实的数据，从而减少了假阴性的可能性，以及使用了慷慨的实体链接预言。作者怀疑 BERT 可能由于其处理的数据量较大而具有优势，因此将 Wikitext-103 作为附加数据添加到关系提取系统，并且观察到性能没有明显变化。这表明尽管可能无法通过更多数据来提高关系提取性能，但将来在不断增长的语料库上训练的语言模型可能会成为将来从文本中提取的传统知识库的可行替代方案。</p><p>除了使用 LAMA 探针测试未来的预训练语言模型外，我们还希望量化关于各种自然语言模板的回忆事实知识的方差。此外，评估多记号答案仍然是作者评估设置面临的开放挑战。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>英文论文写作表达积累</title>
    <link href="/2022/07/13/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E8%A1%A8%E8%BE%BE%E7%A7%AF%E7%B4%AF/"/>
    <url>/2022/07/13/%E8%8B%B1%E6%96%87%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E8%A1%A8%E8%BE%BE%E7%A7%AF%E7%B4%AF/</url>
    
    <content type="html"><![CDATA[<p>开一篇文用于记录日常读paper时好的英文表达。</p><h2 id="Sentence"><a href="#Sentence" class="headerlink" title="Sentence"></a>Sentence</h2><ol><li>The target task can be performed <strong>conditioning</strong> the LLM <strong>with</strong> task-specific prompts, a small portion of parameters, or features. 在…有着…的条件下</li><li>The optimization can be highly efficient since it does not require backpropagation, <strong>where</strong> （修饰backpropagation） the computation complexity <strong>is proportional</strong> <strong>to</strong> the model size and therefore can be expensive or even infeasible for LLMs.  与…成正比</li><li>It has been demonstrated that LLMs can <strong>achieve competitive performance</strong> <strong>on a broad range of tasks</strong> with limited or even zero labeled data. </li><li>Most works in LMaaS also <strong>focus on few-shot or zero-shot settings.</strong></li><li>…lead to a surge of improvements for downstream NLP tasks 导致推进了下游NLP任务的快速发展</li><li><strong>Whilst</strong> learning linguistic knowledge,  在…的同时</li><li>We present an in-depth analysis of the ….  in a wide range of stste-of-the-art pretrained language models 我们做了一个更深层次的分析</li><li>BERT <strong>does remarkably well</strong> on … against …</li><li>certain types of factual knowledge <strong>are learned much more readily</strong> than…</li><li>the <strong>surprisingly strong ability</strong> of … demonstrates their potential as …</li><li><strong>they are optimised to</strong> either predict the next word in a sequence or some masked word anywhere in a given sequence.</li><li>…is <strong>crucial</strong> for current state-of-art results</li><li>Moreover, errors can easily <strong>propagate and accumulate</strong> throughout the pipeline</li><li>language models <strong>come with various attractive properties</strong></li><li>beyond gathering a better… 除了…</li><li>we discuss each step in detail next and provide considerations on the probe below</li><li>we <strong>cover</strong> a variety of sources</li><li><strong>to what extent</strong> aligned texts （对应的文本）exist 在某种程度上</li><li>one can expect that …, and this is indeed the case:</li><li>with respect to 关于…</li><li>…will become a viable alternative to …将会成为一个可行的替代方案</li></ol><h2 id="Word"><a href="#Word" class="headerlink" title="Word"></a>Word</h2><p>大模型的另一种表达：pretrained high-capacity language models</p><p>extract relational data from <strong>text or other modalities</strong>  从文本数据或者一些其它模态的数据</p><p><strong>populate</strong>：populate knowledge bases 填充数据库</p><p>off-the-shelf 现成的 in pretrained off-the-shelf language models</p><p>knowledge base completion literature</p><p>canonical ways 权威的</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Writing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>研究方向之语言模型及服务(LMaaS)</title>
    <link href="/2022/07/11/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B9%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%9C%8D%E5%8A%A1-LMaaS/"/>
    <url>/2022/07/11/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E4%B9%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%9C%8D%E5%8A%A1-LMaaS/</url>
    
    <content type="html"><![CDATA[<p>在知乎上看到一篇文章，在这里附上链接：<a href="https://zhuanlan.zhihu.com/p/538857729?utm_source=wechat_timeline&utm_medium=social&utm_oi=27925374042112&utm_campaign=shareopn">”语言模型即服务“必读论文</a> ，解决了我很多天以来的疑惑，因为包括自己的毕业论文，自己刚刚投完的EMNLP2022，都是我导给了一个大致的方向，然后自己看了些论文，做了些实验，但其实一直都不知道自己的大方向的可以如何用专业名词来概括，以及自己以后是不是要一直研究这个领域，以及这个领域还能研究些什么，这篇文章都解决了我的这些疑问。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><img src="https://pic4.zhimg.com/80/v2-bb3ef100e1ceedb20ca9aa1d80b5234f_1440w.jpg" alt="img"></p><p>以上这张很精美，在未来可能会成为大趋势的图，就来自知乎的那篇文章。LMaaS，即<strong>Language-Model-as-a-Service</strong> 。一直以来，”无法规模化“，”没有通用的解决方案“是让很多AI初创公司广受吐槽的一点。所以现在更适用于工业界的大厂的方法，是把大规模语言模型的推理功能包装成一个API来给用户提供服务。从商业应用层面上来说，仅需部署一个通用的语言模型，即可支持用户适配很多目标任务，即上图所示。</p><p>然而，在我阅读了一小部分论文后，我也知道这在目前来说还仅仅是一种美好的理想，在众多实验中，仅调用模型推理API的方式却通常很难超过在本地微调一个小模型。这篇文章的作者抱着让NLP狠狠出圈发财致富的愿景（当然也是我的最大愿景），最近调研了适用于LMaaS场景的几个方向并维护了一个论文列表，<a href="https://github.com/txsun1997/LMaaS-Papers">https://github.com/txsun1997/LMaaS-Papers</a> ，目前已经收集了40篇相关论文，未来一定会补充更多新工作加进来。</p><h2 id="细分方向"><a href="#细分方向" class="headerlink" title="细分方向"></a>细分方向</h2><p>经过调研，作者将LMaaS的方向细分为以下五个方面：</p><ol><li><strong>Text prompt.</strong> 手工或自动地构造提示语来诱导大模型说出想要的答案</li><li><strong>In-context learning.</strong> 将少量带标签样本放到输入的上下文中，帮助大模型适配到目标任务</li><li><strong>Black-box optimization.</strong> 仅访问大模型的输出概率来使用黑箱优化的手段优化一小部分任务特定的参数</li><li><strong>Feature-based learning.</strong> 将大模型作为特征抽取器，得到样本特征后在本地训练小模型完成任务</li><li><strong>Data generation.</strong> 使用生成式大模型生成特定任务的训练集，利用生成的训练集在本地训练小模型来完成任务</li></ol><p>目前来说，在这五个方向上所做的工作都可以促进LMaaS的发展，我之前的工作就是在text prompt上一直在进行探索，从作者整理的论文列表可以看出来，text prompt和in-context learning是目前研究最多的方向，有不少有意思的工作，但是现阶段想要突破其实我觉得不容易。“相比之下，其余三个方向方兴未艾，大有可为”。我大致看了一下，大概这四十篇文章中有十篇是我看过的文章，很多都是2022年的新工作，最新有到2022年6月份的，但是涉及到多语言的工作还是很少很少，那么接下来暑假的时间就给了我非常好的一些阅读方向，除了text prompt，其余四个方向我也需要了解，或许能为自己的多语言方面的工作提供一些新的思路。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Reading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>EMNLP2022投稿总结</title>
    <link href="/2022/07/10/EMNLP2022%E6%8A%95%E7%A8%BF%E6%80%BB%E7%BB%93/"/>
    <url>/2022/07/10/EMNLP2022%E6%8A%95%E7%A8%BF%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>EMNLP2022的论文投稿已经过去两周了，这是第一次完全自己进行投稿的经历，还是值得总结与反思，算是给自己研究生的一个开始，自己的工作其实还是缺少创新性的，所以看看审稿人的意见，再继续好好做实验与推进下一步工作。</p><h3 id="How-to-sumbit-paper-to-EMNLP2022"><a href="#How-to-sumbit-paper-to-EMNLP2022" class="headerlink" title="How to sumbit paper to EMNLP2022"></a>How to sumbit paper to EMNLP2022</h3><p>因为对投稿完全一点都没有经验，流程都不了解，然后又是因为自己的事情拖了几天才开始真的写初稿，所以其实整个时间线上还是很紧的，所以以后投稿一定要先了解会议投稿流程！</p><p>EMNLP 2022：<a href="https://link.zhihu.com/?target=https://2022.emnlp.org/">https://2022.emnlp.org/</a></p><h4 id="01-重要时间"><a href="#01-重要时间" class="headerlink" title="01 重要时间"></a>01 重要时间</h4><p>匿名期开始时间：2022年5月24日</p><p>通过softconf投稿的摘要截止时间：<strong>2022年6月17日</strong></p><p>通过softconf投稿的全文截止时间：<strong>2022年6月24日</strong></p><p>通过ARR投稿的截止时间：2022年7月24日</p><p>作者反馈时间：2022年8月23日-29日</p><p>录用通知时间：2022年10月6日</p><p>终稿提交时间：2022年10月21日</p><p>研讨会&amp;讲习班：2022年12月7日-8日</p><p>大会时间：2022年12月9日-11日</p><p>1）所有截止时间是11:59PM UTC-12h（即地球上的任何地方）</p><p>2）长文与短文的时间轴一致</p><p>3）<strong>如果要直接投稿，则必须在摘要截止时间之前完成摘要提交，否则不允许提交全文</strong></p><h4 id="02-主要变动"><a href="#02-主要变动" class="headerlink" title="02 主要变动"></a>02 主要变动</h4><p>EMNLP 2022延续EMNLP 2021的做法，使用混合投稿模式，即EMNLP 2022同时接收两种投稿方式：</p><ul><li>通过ARR投稿并完成审稿，后续提交到EMNLP</li><li>直接通过softconf系统投稿到EMNLP</li></ul><p>为了保证整个研究社区审稿量的平衡度，我们会提前询问作者是想被ARR还是EMNLP审稿。</p><p><strong>审稿流程</strong>：</p><ul><li><strong>通过EMNLP直接投稿</strong>：与传统会议相同，论文将由3位审稿人审稿，有author response环节，并且在camera-ready前可以完善他们的论文（论文录用的情况下）。</li><li><strong>通过ARR投稿</strong>：论文将由高级领域主席（SAC）处理。作者可以提供author response，但不允许现有的论文。</li></ul><p><strong>混合投稿政策</strong>：</p><ul><li><p>在2022年7月24日前，如果ARR论文得到所有审稿意见和综合审稿意见（meta-review），即可提交（commit）到EMNLP。</p></li><li><ul><li>论文不可修改，但可以附加author response。</li><li>EMNLP会考虑在2022年7月24日前完成审稿的ARR论文。但与ACL和NAACL不同之处在于，ARR并不能保证在EMNLP截稿时间之前完成所有审稿。所以作者需要抉择是通过ARR还是直接投稿到EMNLP。</li><li>非ARR投稿论文的截稿时间是2022年6月24日。</li></ul></li><li><p>在2022年5月24日以前投稿到ARR的论文可以撤回并投稿到EMNLP 2022。</p></li><li><ul><li>如果要直接投稿到EMNLP 2022，论文必须从ARR系统中撤回，或者论文在5月24日前完成上一轮审稿并且没有提交到下一轮ARR。</li><li>作者可以在2022年5月24日前从ARR撤稿（不论已经收到几份审稿意见）。</li></ul></li><li><p>2022年5月24日之后，论文仍然在ARR系统中（不论是新提交还是未能及时撤稿）将不能直接投稿到EMNLP 2022。</p></li><li><p>在审稿期间，提交到EMNLP 2022的论文不能再次投到其他刊物上（包括ARR）。</p></li></ul><h4 id="03-投稿类型和要求"><a href="#03-投稿类型和要求" class="headerlink" title="03 投稿类型和要求"></a>03 投稿类型和要求</h4><p><strong>3.1 论文篇幅及提交</strong></p><p>和往届会议一样，EMNLP 2022将接收两种投稿类型：长文和短文。长文最多8页正文，<strong>短文最多4页正文</strong>，参考文献不计入页数限制。我这次投的就是short paper，刚开始没有很注意这个要求，后来还花了很多时间去删改，这是很不应该的。而且最终的文章把参考文献，图表等都放在了从第五页开始的地方，排版还是不够好看，这都是以后要注意的地方。录用后的论文可增加不超过一页的正文内容。</p><p><strong>3.2 论文署名</strong></p><p>论文列表中应列举所有并且只列举那些对论文工作作出重要贡献的个人。每位作者都将收到EMNLP 2022的通知（投稿、修改、录用结果）<strong>。在EMNLP 2022的摘要截稿（2022年6月17日）之后，将无法增减作者，也不能够更改作者顺序。</strong></p><p><strong>3.3 （新增）论文局限性****（非常重要！）</strong></p><p>我们认为探讨论文工作的不足之处也是非常重要的。EMNLP 2022要求所有论文添加一个“Limitations”章节来探讨论文局限性。这一章将会放在discussion&#x2F;conclusion之后，参考文献之前。需要注意的是，这部分内容不计入页数限制之内。<strong>如果论文中没有添加这一章节，将会自动被拒稿。</strong>如果通过ARR审稿的论文不包含这一章节，可以在提交到EMNLP 2022时附带一个PDF来讨论论文局限性。</p><p><strong>3.4 论文的模板</strong></p><p>更新在EMNLP 2022的官方网站上。另外，<strong>请不要修改样式文件</strong>，也不要使用其他会议的模板。如果论文不满足样式要求将在审稿期前直接被拒稿，这包括纸张大小、边距、字体等限制。</p><p><strong>3.5 引用和对比</strong></p><p>作者应在论文中对比所有已发表的相关文献，但可以因为不了解未发表的文献而没有进行对比（尤其是近期才公开的文献或没有被广泛引用的文献）。如果相关论文的预印本被正式出版，作者需要引用已出版的版本而不是预印本版本。在截稿时间之前3个月内的论文（不论发表与否）可以被认为是同期论文，你不需要进行深度对比，例如进行额外的实验或深度分析等。</p><h3 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h3><p><strong>写论文工具</strong>—overleaf</p><p><strong>画图工具</strong>—draw.io（放入文章里面的图一定是高清矢量图，所以一定要用draw.io画好了把pdf进行导出！）</p><p><strong>文献整理工具</strong>—一定要使用google scholar（或者是国内的一些镜像网站，都是最权威的引用方法）；将arxiv上面的论文变成bibtex形式 链接：<a href="https://arxiv2bibtex.org/?q=2203.12277&amp;format=bibtex">https://arxiv2bibtex.org/?q=2203.12277&amp;format=bibtex</a></p><p><strong>翻译工具</strong>—DeepL翻译器（最优雅的翻译！翻译的是真的好诶）</p><h3 id="Notice—To-do"><a href="#Notice—To-do" class="headerlink" title="Notice—To do"></a>Notice—To do</h3><h4 id="01-论文结构"><a href="#01-论文结构" class="headerlink" title="01 论文结构"></a>01 论文结构</h4><p>论文前前后后改了差不多有五版，第一次大改就是结构基本上都不太对，然后控制的篇幅也不太对，首先论文结构就是一篇好的论文的基础，所以从现在开始看文章不能是只看内容了，还需要看文章的结构，篇幅布局，要多看些文章，多学习文章结构的写法，好的文章都是从模仿开始。</p><h4 id="02-英文表达"><a href="#02-英文表达" class="headerlink" title="02 英文表达"></a>02 英文表达</h4><p>英文表达对于一篇好文章也至关重要，之后还是要继续学习英语，平时注意积累英文表达，读paper的时候尽量先去看英文，而不是依赖于翻译，并且同样注意积累好的表达，很多时候好的句子表达可以让一个段落都活起来。还有一些小的细节需要注意，同一个出现的单词，写法一定要保证一致，比如多任务，全部都改为multitask，不能存在有multi-task；还有比如文章中如果第一次出现缩写的词，需要全拼进行解释，这些都是约定俗成的规矩，需要注意。</p><h4 id="03-其它"><a href="#03-其它" class="headerlink" title="03 其它"></a>03 其它</h4><p>注意引用！别的论文的观点一定要引用，做到严谨！</p><p>实验部分是重点，一定要详细叙述，做实验的时候记录非常重要，对比实验很重要，一定要做全面</p><p>result部分一定要进行重点分析，为什么会出现这些实验结果的原因，是更加有价值的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Paper Submission</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/07/09/hello-world/"/>
    <url>/2022/07/09/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
